<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=light data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><title>Advanced Self-supervised Pre-training model &#183; Blog of gigio1023</title>
<meta name=title content="Advanced Self-supervised Pre-training model &#183; Blog of gigio1023"><meta name=description content="GPT-1과 기본적인 구조는 같다. Transformer layer를 보다 더 많이 쌓았다.다음 단어를 예측하는 task로 학습을 진행.더 많은 학습 데이터 사용보다 양질의 데이터 사용zero-shot setting으로 다뤄질 수 있는 잠재적인 능력을 보여줬다ref: "><meta name=keywords content="NLP,"><link rel=canonical href=https://gigio1023.github.io/posts/ml/2021-09-18-advanced-self-supervised-pre-training-model/><link type=text/css rel=stylesheet href=/css/main.bundle.min.e96772c5cdc14f53dc8f2d49a7f91bd1dc21484251faa988567a4cb5313e24ea6befb15bbad2f816721b267c196c83c5c688647f3f59faf0e25b638d0e602880.css integrity="sha512-6Wdyxc3BT1Pcjy1Jp/kb0dwhSEJR+qmIVnpMtTE+JOpr77FbutL4FnIbJnwZbIPFxohkfz9Z+vDiW2ONDmAogA=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script><script defer type=text/javascript id=script-bundle src=/js/main.bundle.min.b6411b5d4cd56c0068d34c4acbce043846adad56b824e3d486a06d3459aed2eb7f7413874b7871cc2c822c8c8834cbed944022918bcc8cca710a962167c36d32.js integrity="sha512-tkEbXUzVbABo00xKy84EOEatrVa4JOPUhqBtNFmu0ut/dBOHS3hxzCyCLIyINMvtlEAikYvMjMpxCpYhZ8NtMg==" data-copy data-copied></script><script src=/lib/zoom/zoom.min.37d2094687372da3f7343a221a470f6b8806f7891aa46a5a03966af7f0ebd38b9fe536cb154e6ad28f006d184b294525a7c4054b6bbb4be62d8b453b42db99bd.js integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ=="></script><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:url" content="https://gigio1023.github.io/posts/ml/2021-09-18-advanced-self-supervised-pre-training-model/"><meta property="og:site_name" content="Blog of gigio1023"><meta property="og:title" content="Advanced Self-supervised Pre-training model"><meta property="og:description" content="GPT-1과 기본적인 구조는 같다. Transformer layer를 보다 더 많이 쌓았다.다음 단어를 예측하는 task로 학습을 진행.더 많은 학습 데이터 사용보다 양질의 데이터 사용zero-shot setting으로 다뤄질 수 있는 잠재적인 능력을 보여줬다ref:"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-09-18T11:08:52+00:00"><meta property="article:modified_time" content="2021-09-18T11:08:52+00:00"><meta property="article:tag" content="NLP"><meta name=twitter:card content="summary"><meta name=twitter:title content="Advanced Self-supervised Pre-training model"><meta name=twitter:description content="GPT-1과 기본적인 구조는 같다. Transformer layer를 보다 더 많이 쌓았다.다음 단어를 예측하는 task로 학습을 진행.더 많은 학습 데이터 사용보다 양질의 데이터 사용zero-shot setting으로 다뤄질 수 있는 잠재적인 능력을 보여줬다ref:"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"Advanced Self-supervised Pre-training model","headline":"Advanced Self-supervised Pre-training model","description":"GPT-1과 기본적인 구조는 같다. Transformer layer를 보다 더 많이 쌓았다.다음 단어를 예측하는 task로 학습을 진행.더 많은 학습 데이터 사용보다 양질의 데이터 사용zero-shot setting으로 다뤄질 수 있는 잠재적인 능력을 보여줬다ref: ","inLanguage":"en","url":"https:\/\/gigio1023.github.io\/posts\/ml\/2021-09-18-advanced-self-supervised-pre-training-model\/","author":{"@type":"Person","name":"Sungho Park (gigio1023)"},"copyrightYear":"2021","dateCreated":"2021-09-18T11:08:52\u002b00:00","datePublished":"2021-09-18T11:08:52\u002b00:00","dateModified":"2021-09-18T11:08:52\u002b00:00","keywords":["NLP"],"mainEntityOfPage":"true","wordCount":"1487"}]</script><meta name=author content="Sungho Park (gigio1023)"><link href=mailto:relilau00@gmail.com rel=me><link href=https://gigio1023.github.io/ rel=me><link href=https://www.linkedin.com/in/gigio1023/ rel=me><script src=/lib/jquery/jquery.slim.min.b0dca576e87d7eaa5850ae4e61759c065786cdb6489d68fcc82240539eebd5da522bdb4fda085ffd245808c8fe2acb2516408eb774ef26b5f6015fc6737c0ea8.js integrity="sha512-sNylduh9fqpYUK5OYXWcBleGzbZInWj8yCJAU57r1dpSK9tP2ghf/SRYCMj+KsslFkCOt3TvJrX2AV/Gc3wOqA=="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-6R7457QHQ1"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-6R7457QHQ1")</script><meta name=theme-color><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css integrity=sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js integrity=sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js integrity=sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh crossorigin=anonymous onload=renderMathInElement(document.body)></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32 scrollbar-thin scrollbar-track-neutral-200 scrollbar-thumb-neutral-400 dark:scrollbar-track-neutral-800 dark:scrollbar-thumb-neutral-600"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start gap-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Blog of gigio1023</a></nav><nav class="hidden md:flex items-center gap-x-5 md:ml-12 h-12"><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Posts>Blog</p></a><a href=/categories/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Categories>Categories</p></a><a href=https://github.com/gigio1023 target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><span class=mr-1><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span><p class="text-base font-medium" title>GitHub</p></a><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-base font-medium" title=Tags>Tags</p></a><button id=search-button aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></button><div class="flex items-center"><button id=appearance-switcher aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></nav><div class="flex md:hidden items-center gap-x-5 md:ml-12 h-12"><span></span>
<button id=search-button-mobile aria-label=Search class="text-base hover:text-primary-600 dark:hover:text-primary-400" title>
<span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg>
</span></button>
<button id=appearance-switcher-mobile aria-label="Dark mode switcher" type=button class="text-base hover:text-primary-600 dark:hover:text-primary-400 ltr:mr-1 rtl:ml-1"><div class="flex items-center justify-center dark:hidden"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M32 256C32 132.2 132.3 32 255.8 32c11.36.0 29.7 1.668 40.9 3.746 9.616 1.777 11.75 14.63 3.279 19.44C245 86.5 211.2 144.6 211.2 207.8c0 109.7 99.71 193 208.3 172.3 9.561-1.805 16.28 9.324 10.11 16.95C387.9 448.6 324.8 480 255.8 480 132.1 480 32 379.6 32 256z"/></svg></span></div><div class="items-center justify-center hidden dark:flex"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M256 159.1c-53.02.0-95.1 42.98-95.1 95.1s41.2 96.9 95.1 96.9 95.1-42.98 95.1-95.1S309 159.1 256 159.1zM509.3 347l-63.2-91.9 63.15-91.01c6.332-9.125 1.104-21.74-9.826-23.72l-109-19.7-19.7-109c-1.975-10.93-14.59-16.16-23.72-9.824L256 65.89 164.1 2.736c-9.125-6.332-21.74-1.107-23.72 9.824L121.6 121.6 12.56 141.3C1.633 143.2-3.596 155.9 2.736 164.1L65.89 256 2.74 347.01c-6.332 9.125-1.105 21.74 9.824 23.72l109 19.7 19.7 109c1.975 10.93 14.59 16.16 23.72 9.824L256 446.1l91.01 63.15c9.127 6.334 21.75 1.107 23.72-9.822l19.7-109 109-19.7C510.4 368.8 515.6 356.1 509.3 347zM256 383.1c-70.69.0-127.1-57.31-127.1-127.1.0-70.69 57.31-127.1 127.1-127.1S383.1 186.2 383.1 256c0 70.7-56.4 127.1-127.1 127.1z"/></svg></span></div></button></div></div><div class="-my-2 md:hidden"><label id=menu-button class=block><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-0 overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li id=menu-close-button><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Posts>Blog</p></a></li><li class=mt-1><a href=/categories/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Categories>Categories</p></a></li><li class=mt-1><a href=https://github.com/gigio1023 target=_blank class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><div class=mr-2><span class="relative block icon"><svg viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></div><p class="text-bg font-bg" title>GitHub</p></a></li><li class=mt-1><a href=/tags/ class="flex items-center text-gray-500 hover:text-primary-600 dark:hover:text-primary-400"><p class="text-bg font-bg" title=Tags>Tags</p></a></li></ul></div></label></div></div><div class="relative flex flex-col grow"><main id=main-content class=grow><article><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">Advanced Self-supervised Pre-training model</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime=2021-09-18T11:08:52+00:00>18 September 2021</time><span class="px-2 text-primary-500">&#183;</span><span>1487 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">7 mins</span></div></div><div class="flex author"><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Sungho Park (gigio1023)" src=/img/profile_hu_bc5ec457e77a1933.png><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Sungho Park (gigio1023)</div><div class="text-sm text-neutral-700 dark:text-neutral-400">To build a genuinely useful product</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=mailto:relilau00@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://gigio1023.github.io/ target=_blank aria-label=Link rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 640 512"><path fill="currentcolor" d="M172.5 131.1c55.6-55.59 148-55.59 203.6.0 50 50 57.4 129.7 16.3 187.2L391.3 319.9C381 334.2 361 337.6 346.7 327.3c-14.4-10.3-17.8-30.3-7.5-44.6L340.3 281.1C363.2 249 359.6 205.1 331.7 177.2c-31.4-31.4-82.5-31.4-114 0L105.5 289.5c-31.51 30.6-31.51 82.5.0 114C133.3 431.4 177.3 435 209.3 412.1L210.9 410.1C225.3 400.7 245.3 404 255.5 418.4 265.8 432.8 262.5 452.8 248.1 463.1L246.5 464.2c-58.4 41.1-136.3 34.5-186.29-15.4-56.469-56.5-56.469-148.1.0-204.5L172.5 131.1zM467.5 380c-56.5 56.5-148 56.5-204.5.0-50-50-56.5-128.8-15.4-186.3L248.7 192.1C258.1 177.8 278.1 174.4 293.3 184.7 307.7 194.1 311.1 214.1 300.8 229.3L299.7 230.9C276.8 262.1 280.4 306.9 308.3 334.8c31.4 31.4 82.5 31.4 114 0L534.5 222.5c31.5-31.5 31.5-83.4.0-114C506.7 80.63 462.7 76.99 430.7 99.9L429.1 101C414.7 111.3 394.7 107.1 384.5 93.58 374.2 79.2 377.5 59.21 391.9 48.94L393.5 47.82C451 6.731 529.8 13.25 579.8 63.24c56.5 56.46 56.5 148.06.0 204.46L467.5 380z"/></svg>
</span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://www.linkedin.com/in/gigio1023/ target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a></div></div></div></div><div class=mb-5></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="min-w-0 min-h-0 max-w-fit"><div class="article-content max-w-prose mb-20"><h1 class="relative group">GPT-2<div id=gpt-2 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#gpt-2 aria-label=Anchor>#</a></span></h1><p>GPT-1과 기본적인 구조는 같다.</p><ul><li>Transformer layer를 보다 더 많이 쌓았다.</li><li>다음 단어를 예측하는 task로 학습을 진행.</li><li>더 많은 학습 데이터 사용<ul><li>보다 양질의 데이터 사용</li></ul></li><li>zero-shot setting으로 다뤄질 수 있는 잠재적인 능력을 보여줬다
ref: <a href=https://velog.io/@stapers/%EB%85%BC%EB%AC%B8%EC%8A%A4%ED%84%B0%EB%94%94-Week9-10-Zero-shot-Learning-Through-Cross-Modal-Transfer target=_blank>zero shot learning</a>
한 번도 보지 못한 데이터를 분류 가능하도록 학습하는 것.</li></ul><p>![](/assets/images/Advanced Self-supervised Pre-training model/fb607f1a-4fe0-47cc-9754-4f58b7450794-image.png)</p><p>GPT-2의 기본적인 task는 위와 같이 지문이 주어지면 순차적으로 다음에 올 단어들을 에측하는 language model이다. 소설과도 같은 지문이 주어졌을 때, 실제 사람이 쓸 법한 공상적인 이야기를 이어서 썻다.</p><h2 class="relative group">decaNLP, motivation of GPT-2<div id=decanlp-motivation-of-gpt-2 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#decanlp-motivation-of-gpt-2 aria-label=Anchor>#</a></span></h2><p>기존의 nlp task는 task별로 다른 모델 구조와 해결 방법을 고안해야했다. 가령, 문장의 긍/부정을 판단하기 위해서는 CLS token을 output layer에 통과시킨 후 이를 binary classification에서 사용한다. 혹은 QnA를 진행하기 위해서 QnA만을 위한 모델 구조를 별도로 고안해야 했다.</p><p>GPT-2로부터 3~4년 전의 decaNLP에서는 모든 NLP task를 질의응답으로 통합해서 구성할 수 있다는 것이 요지다. 즉, 모든 task를 자연어 생성 task로 간주하는 것이다.</p><p>** e.g., 문장의 긍/부정 판단 task를 다음과 같이 구성한다.**</p><ol><li>임의의 문장을 입력 후</li><li>&lsquo;What do you think about this document in terms of positive or negative?&lsquo;라는 질문을 추가로 입력</li><li>모델이 1번 문장에 대한 긍/부정 판단을 기대한다.</li></ol><p>2번의 질의 문장은 자유롭게 바꿀 수 있다. &lsquo;Do you think wheter this sentence is postivie or negative?&lsquo;처럼 말이다.</p><p>문단의 요약을 원한다면 &lsquo;What is the summarization of the above paragraph?&lsquo;를 1번 문장 다음에 추가하면 된다.</p><h2 class="relative group">Dataset of GPT-2<div id=dataset-of-gpt-2 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dataset-of-gpt-2 aria-label=Anchor>#</a></span></h2><p>양질의 글으 얻기 위해 아래의 웹 사이트들을 dataset으로 사용했다고 한다.</p><ul><li>Reddit<ul><li>3개 이상의 karam(up-vote)를 받은 discussion이 외부 링크를 포함하고 있다면, 해당 링크의 document가 양질의 데이터를 포함하고 있다고 가정</li><li>이러한 방식으로 Reddit의 데이터와 Reddit에서 참조하는 외부 링크들의 document를 dataset으로 활용</li><li>45M links 수집</li></ul></li><li>8M removed Wikipedia documents</li><li>Dragnet(경찰 시리즈물인 것 같다)</li><li>newspaper</li></ul><h2 class="relative group">Preprocess<div id=preprocess class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#preprocess aria-label=Anchor>#</a></span></h2><ul><li>BPE(Byte pair encoding)</li><li>Minimal fragementation of words across multiple vocab tokens</li></ul><h2 class="relative group">Modification of models<div id=modification-of-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#modification-of-models aria-label=Anchor>#</a></span></h2><ul><li>Layer normalization<ul><li>sub-block 단위로 수행하거나 기존의 normalization 위치를 변경</li></ul></li><li>Initialization of weight in residual Layer<ul><li>Residual Layer의 index가 커질수록 weight 초기화 값을 $\sqrt{n}$에 반비례하게 만들었다.</li><li>Output에 가까워질수록 Layer의 선형변환의 출력들이 0에 가까워지도록 하기 위함이다.</li><li>뒤쪽 layer의 역할을 줄여주는 것이 목적.</li><li>멘토님 답변: 모델의 뒤로 갈수록 high dimentional한 특징을 학습하는데, 이러한 feature에 영향을 덜 받기 위해서?</li></ul></li></ul><h2 class="relative group">Question Answering<div id=question-answering class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#question-answering aria-label=Anchor>#</a></span></h2><p>CoAQ(Conversationi question answering dataset)을 사용.</p><ul><li>해당 데이터셋을 전혀 사용하지 않은채로 테스트할 경우<ul><li>F1 score = 55%</li></ul></li><li>Fine tuned 후<ul><li>F1 score = 89%</li></ul></li></ul><p>zero-shot leraning에 대한 가능성을 GPT-1보다 더욱 크게 보여줬다.</p><h2 class="relative group">Summarization<div id=summarization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#summarization aria-label=Anchor>#</a></span></h2><p>![](/assets/images/Advanced Self-supervised Pre-training model/54b0c314-4525-41b1-9167-74b8e9a876e5-image.png)</p><p>이 부분은 교수님의 설명이 부족한 것 같아 좀 더 찾아봐야 될 것 같다. 모든 학습 데이터에 대해서 TL;DR; token이 존재할리가 없는데..</p><p>GPT-2의 학습 데이터에는 TL;DR; token의 등장 이후에 한 줄 요약을 하는 데이터가 많다. 따라서 zero-shot learning처럼 별도의 fine tuning 없이 원하는 지문의 끝에 Tl;DR; token을 붙이는 것만으로도 summarization task를 수행할 수 있다고 한다.</p><h2 class="relative group">Translation<div id=translation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#translation aria-label=Anchor>#</a></span></h2><p>Summarization의 TL;DR; token처럼, 번역하고자 하는 문장의 뒤에 &lsquo;in French&rsquo;와 같은 문구를 추가하면 translation task를 수행한다.</p><h1 class="relative group">GPT-3<div id=gpt-3 class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#gpt-3 aria-label=Anchor>#</a></span></h1><p>GPT-2보다 굉장히 많은 parameters, transformer layer, batch size로 학습해보니 GPT-2보다 좋은 모델이 나왔다고 한다.</p><h2 class="relative group">Few-shot learning<div id=few-shot-learning class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#few-shot-learning aria-label=Anchor>#</a></span></h2><p>GPT-2에서는 zero-shot, few-shot learning에 대한 가능성을 보여줬다면 GPT-3는 이에 대한 매우 좋은 성능을 보여줬다.</p><p>![](/assets/images/Advanced Self-supervised Pre-training model/2668f8f5-e35d-47c3-b548-5d41228b28be-image.png)</p><p>모델을 전혀 변경하지 않고 아래의 inference task를 수행했다.</p><ul><li>Zero-shot: GPT-3는 번역 데이터를 학습하지 않았지만, 별도의 fine tuning을 전혀 하지 않고도 translation이 가능했다.</li><li>One-shot: 학습 데이터에 대한 예시를 한 쌍만 보여준다.</li><li>Few-shot: 학습 데이터에 대한 예시를 여러 쌍을 보여준다.</li></ul><p>![](/assets/images/Advanced Self-supervised Pre-training model/68579874-7c27-4d3e-8938-85890f933e74-image.png)</p><p>모델의 크기(parameter의 개수)를 늘릴수록 zero-shot, one-shot, few-shot에 대한 성능은 계속 증가한다. 모델이 클수록 모델의 동적인 학습 능력이 올라갔음을 알 수 있다.</p><h1 class="relative group">ALBERT<div id=albert class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#albert aria-label=Anchor>#</a></span></h1><p>A Lite BERT.
BERT, GPT와 같은 거대 모델을 self-supervised learning으로 학습하기 위해서는 많은 메모리, parameters, batch size가 필요하다. 하지만 이러한 자원은 한정적이다. 이를 경량화하면서 오히려 BERT보다 더 좋은 성능을 가진 모델이 ALBERT다.</p><ul><li>Obstacles<ul><li>Memory Limitation</li><li>Training speed</li></ul></li><li>Solutions<ul><li>Factoried Embedding Parameterization</li><li>Cross-layer Parameter Sharing</li><li>(For performance) Senetence Order Prediction</li></ul></li></ul><h2 class="relative group">Factorized Embedding Parameterization<div id=factorized-embedding-parameterization class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#factorized-embedding-parameterization aria-label=Anchor>#</a></span></h2><p>![](/assets/images/Advanced Self-supervised Pre-training model/9c19439d-673b-4846-8cc6-a58de9926c6b-image.png)</p><ul><li>기존 transformer: transformer 내의 residual block 때문에 Embedding의 차원과 출력 차원이 동일해야하기 때문에, 모든 Layer의 입출력 차원들은 같다.</li><li>ALBERT: Layer별 출력 차원들을 축소시키자.</li></ul><h3 class="relative group">Motivation<div id=motivation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#motivation aria-label=Anchor>#</a></span></h3><ul><li>Embedding layer: 문맥을 전혀 고려하지 않고 Word만이 가진 정보를 상수로써 표현하는 vector</li><li>hidden state vector: 문맥이 고려된 semantic한 정보들을 포함하고 있는 vector</li></ul><p>Embedding layer는 hidden state vector에 비해 담고 있는 정보가 적다. 따라서 Embedding layer를 쪼개서 보다 작은 차원으로 표현해보자.</p><h3 class="relative group">Implementation<div id=implementation class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implementation aria-label=Anchor>#</a></span></h3><p>![](/assets/images/Advanced Self-supervised Pre-training model/0dee7a07-9347-4262-be6c-1ce1985b6109-image.png)
본래 4차원으로 word들이 Embedding 된다고하자. 그러면 위 도식과 같이 Word embedding layer 또한 4차원이다. 이러한 Embedding layer를 2차원으로 줄이여서 모델에 입력하고자 한다.</p><p>위 도식에서 V x E에 해당하는 layer를 transformer의 입력으로 준다면 transformer의 paramters 수는 이전보다 줄어들 것이다.</p><p>residual 연산을 위해서는 입력 차원과 동일해야하는데, transformer는 2차원을 출력할 것이다. 따라서 residual 이전에 위 도식의 E x H와 같이 본래 차원으로 되돌려주는 layer를 추가한다. 결과적으로 transformer의 parameter는 줄고 출력은 입력과 동일해진다. 이러한 방식을 Low rank matrix factorization이라고 한다.</p><p>Factorized Embedding Parameterization은 기존의 Word Embedding 결과를 그대로 사용하는 것에 근사하는 결과를 보여주는 것으로 알려져 있다.</p><h2 class="relative group">Cross-layer Parameter Sharing<div id=cross-layer-parameter-sharing class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#cross-layer-parameter-sharing aria-label=Anchor>#</a></span></h2><p>![](/assets/images/Advanced Self-supervised Pre-training model/b90f1049-accb-40d3-9c85-c7cd9e623a3f-image.png)</p><p>transformer에서 학습이 진행되는 parameters는 self-attention layer별로 가지고 있는 $W_n^Q, W_n^K, W_n^V$와 concatenate된 matrix의 차원을 줄여주는 $W_O$이다. transformer를 쌓을수록 parameter의 수가 늘어나는데, ALBERT에서는 이 parameter들을 self-attetion들이 공유하도록 해봤다.</p><p>![](/assets/images/Advanced Self-supervised Pre-training model/fc8c39c3-3ef7-4f74-ab2a-3801921c84cf-image.png)</p><ul><li>Shared-FFN: Only sharing feed-forward network parmeters</li><li>Shared-Attnetion: Only sharing attention parameters</li><li>All-shared: Both of them</li></ul><p>All-shared를 해도 not-shared에 비해서 성능이 나쁘지 않다.</p><h2 class="relative group">Sentence Order Prediction<div id=sentence-order-prediction class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#sentence-order-prediction aria-label=Anchor>#</a></span></h2><p>BERT는 두 가지 형태로 학습이 진행된다.</p><ul><li>Maksed language modeling: k%를 mask token으로 치환 후 학습</li><li>Next sentence prediction: 두 문장을 sep token으로 concat후 학습</li></ul><p>BERT의 next sentence prediction 실효성이 없다는 연구결과들이 나았다. next sentence prediction을 학습 과정에서 제외하고 masked language modeling만을 수행해도 모델의 성능이 좋기 때문이다.</p><p>왜냐하면 next sentence prediction에서 negative samples를 판단하는 것은 유사한 단어가 나타나는지 파악만 해도 가능하기 때문이다.</p><blockquote><p>e.g., 사회면의 기사와 스포츠면의 기사는 내용과 사용되는 단어들이 매우 상이하기에 두 분야에서 sampling한 단어들은 next sentence가 아님을 쉽게 예측 가능하다.</p></blockquote><p>next sentence 관계에 있는 문장들은 동일하거나 유사한 단어들이 자주 등장할 것이다. next sentence 판별이 또한 예측이 매우 쉬워진다.</p><p>고차원적인 판단이 아니라 단어의 출현을 기반으로 next sentence prediction을 수행하기 때문에, 이렇게 학습된 모델은 얻게 next sentence prediction으로부터 얻은 정보가 많지 않을 것이다</p><p>ALBERT는 next sentence prediction을 변형해서 sentence order prediction으로 학습을 진행했다.
이 방법론은 올바른 순서의 두 문장을 concat해서 모델에 입력하면 올바른 순서라고 인지해야 한다. 반대로 올바르지 않은 순서의 두 문장(Negative smaples)을 concat해서 모델에 입력하면 올바르지 않은 순서라고 인지해야 한다.</p><p>overlapped 되는 단어들이 거의 없도록 두 문장을 뽑기 위해서 동일 문서에서 sampling을 한다. 단어의 출현을 기반으로 학습하지 않고 문맥을 고려하여 Sentence order prediction을 학습하도록 하기 위함이다.</p><p>![](/assets/images/Advanced Self-supervised Pre-training model/0c88484e-052c-4874-a174-560c90c82f97-image.png)</p><p>NSP(Next sentence prediction)과 SOP(Sentence order prediction)의 결과를 나타낸 표이다. NSP는 성능 향상이 미미하고 오히려 떨어지는 case도 있다. SOP는 큰 성능 향상을 이뤘다.</p><p>![](/assets/images/Advanced Self-supervised Pre-training model/f344c656-a105-476d-b5da-0d6bde95cffe-image.png)
NLP task를 평가하기 위한 데이터셋인 GLUE에서도 ALBERT가 기존 모델들보다 더 좋은 성능을 보여주는 것을 알 수 있다.</p><h1 class="relative group">ELECTRA<div id=electra class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#electra aria-label=Anchor>#</a></span></h1><p>Efficiently Learning an Encoder that Classifies Token Replacements Accurately. ICLR 2020에서 구글 리서치 팀이 발표한 논문이다.</p><p>![](/assets/images/Advanced Self-supervised Pre-training model/634ddd5e-ec24-487b-ba83-2027e083684c-image.png)
두 모델은 Adversarial(적대적) learning 형태로 학습이 진행된다.
관련 링크: <a href=https://green-late7.tistory.com/100 target=_blank>Adversarial learning</a>, <a href=https://medium.com/@jongdae.lim/%EA%B8%B0%EA%B3%84-%ED%95%99%EC%8A%B5-machine-learning-%EB%A8%B8%EC%8B%A0-%EB%9F%AC%EB%8B%9D-%EC%9D%80-%EC%A6%90%EA%B2%81%EB%8B%A4-part-8-d9507cf20352 target=_blank>Adversarial learning 긴 설명</a></p><ul><li>Generator: Maksed language model<ul><li>BERT와 같은 원리로 동작</li><li>Maksed 문장을 복원</li></ul></li><li>Discriminator: mask token에 위치한 단어가 원본 단어인지, replaced된 단어인지 추론.<ul><li>Transformer 기반으로 Binary classification 진행.</li><li>GAN(Generative adversarial network)에서 착안했다.</li><li>학습 데이터의 Ground truth와 Discriminator의 결과를 비교하면서 학습한다.</li></ul></li></ul><p>** Pre-trained model로는 Generator가 아니라 Discriminator를 사용한다. **</p><h2 class="relative group">Performance<div id=performance class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#performance aria-label=Anchor>#</a></span></h2><p>![](/assets/images/Advanced Self-supervised Pre-training model/1770209f-2f5e-4ad0-9fd8-4192bf586d95-image.png)</p><p>동일 연산량에서 기존 모델에 비해 더 높은 GLUE score를 기록했다. ALBERT와 함께 많은 downstream task에서 활용되고 있다.</p><p><a href=https://velog.io/@nawnoes/Downstream-Task%EB%9E%80 target=_blank>Downstream Task의 의미</a></p><h1 class="relative group">Light-weight models<div id=light-weight-models class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#light-weight-models aria-label=Anchor>#</a></span></h1><p>BERT, GPT, ELECTRA들은 self-attention block을 매우 많이 쌓아서 성능 향상을 이뤘기 때문에 parameters가 매우 많다. 이러한 parameters를 줄여서 모델의 크기와 학습 속도를 줄이는 경량화 모델이 연구되고 있다.</p><p>클라우드, 고성능컴퓨팅 자원이 아닌 모바일 기기에서 빠르고 저전력을 소모해서 모델을 돌리기 위해 사용된다.</p><h2 class="relative group">DistillBERT<div id=distillbert class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#distillbert aria-label=Anchor>#</a></span></h2><p>HuggingFace가 NeuralPS(뉴립스) 2019에서 발표한 논문.</p><p>teacher, student model로 구성된다.</p><ul><li>teacher: 기존의 거대한 구조를 유지하면서 feature들 학습하고 student model을 학습 시킨다.</li><li>student: teacher보다 layer, parameters가 적음에도 teacher의 feature를 모사하려고 노력하는 모델.</li></ul><h3 class="relative group">동작 원리<div id=%EB%8F%99%EC%9E%91-%EC%9B%90%EB%A6%AC class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#%EB%8F%99%EC%9E%91-%EC%9B%90%EB%A6%AC aria-label=Anchor>#</a></span></h3><p>** Teacher: BERT의 Seq2Seq와 동일하다. **</p><ol><li>&lsquo;I go home&rsquo;을 입력으로 했을 때, Seq2Seq의 teacher는 &lsquo;I&rsquo;에 대해서 &lsquo;go&rsquo;를 예측하고자 한다.</li><li>&lsquo;I&rsquo;에 대한 입력에 대해서 vocabulary size만큼의 vector 생성되고 여기에 softmax를 취한다.</li><li>2번의 결과에 대해서 확률값이 가장 큰 값은 &lsquo;go&rsquo;에 해당하는 index일 것이다.</li></ol><p>** Studnet: Teacher의 출력인 target distribution을 ground truth로 하여 학습한다. ** 단순히 Teacher의 출력을 모사하도록 한다.</p><h2 class="relative group">TinyBERT<div id=tinybert class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#tinybert aria-label=Anchor>#</a></span></h2><p>Teacher, student로 이루어지는 구조는 DistillBERT와 동일하다. TinyBERT는 teacher의 출력 distribution만을 모사하는 것이 아니라, 중간 생성물인 query, key, value, hidden state들까지도 모사하도록 한다.</p><p>Teacher에 비해서 student는 경량화모델이기 때문에 layer의 dimension들이 teacher에 비해 작다. 모사할 때 이것이 문제가 되는데, teacher의 dimension을 축소시키는 fully connected layer를 추가해서 해결했다고 한다.</p><h1 class="relative group">Fusing Knowledge Graph into Language model<div id=fusing-knowledge-graph-into-language-model class=anchor></div><span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#fusing-knowledge-graph-into-language-model aria-label=Anchor>#</a></span></h1><p>BERT는 문맥을 파악하거나 단어들간의 유사도를 구하는 task에서는 뛰어났지만 주어진 데이터셋 외의 정보들에 대해서 효과적으로 처리하지 못하기도 한다.</p><p><strong>e.g., 땅을 파는 행위에 대한 데이터로 &lsquo;땅을 팠다.&lsquo;라는 문장만이 존재하는 데이터셋을 가정하자.</strong>
Question Answering task에서 이 문장에 대해서 &lsquo;어떤 도구를 사용했는가?&lsquo;를 묻는다면 기존의 모델은 답하기 어려울 수 있다. 정보가 없기 때문이다.
사람의 경우 &lsquo;상식&rsquo;이라는 외부 정보를 활용해 다양한 상황에 대한 사용 도구를 추론하고 답할 수 있을 것이다.</p><p>이와 같이 외부 정보를 모델에서 활용하고자 하는 분야다.</p><p>** Knowledge graph: 정보들을 쳬게화해서 정리한 것 **</p><p>대표적인 모델</p><ul><li>ERNIE<ul><li>Information fusion layer takes the concatenation of the token embedding and
entity embedding</li></ul></li><li>KagNET<ul><li>For each pair of question and answer candidate, it retrieves a sub-graph from
an external knowledge graph to capture relevant knowledge</li></ul></li></ul></div></div><script>var oid="views_posts/ML/2021-09-18-Advanced Self-supervised Pre-training model.md",oid_likes="likes_posts/ML/2021-09-18-Advanced Self-supervised Pre-training model.md"</script><script type=text/javascript src=/js/page.min.0860cf4e04fa2d72cc33ddba263083464d48f67de06114529043cb4623319efed4f484fd7f1730df5abea0e2da6f3538855634081d02f2d6e920b956f063e823.js integrity="sha512-CGDPTgT6LXLMM926JjCDRk1I9n3gYRRSkEPLRiMxnv7U9IT9fxcw31q+oOLabzU4hVY0CB0C8tbpILlW8GPoIw=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/ml/2021-09-18-self-supervised-pre-training-models/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Self-supervised Pre-training models</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2021-09-18T10:51:48+00:00>18 September 2021</time>
</span></span></a></span><span><a class="flex text-right group ml-3" href=/posts/ml/2021-09-19-recent-trends-of-nlp/><span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">Recent trends of NLP</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime=2021-09-19T13:41:16+00:00>19 September 2021</time>
</span></span><span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&larr;</span></a></span></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><nav class="flex flex-row pb-4 text-base font-medium text-neutral-500 dark:text-neutral-400"><ul class="flex flex-col list-none sm:flex-row"><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/tags/ title=Tags>Tags</a></li><li class="flex mb-1 ltr:text-right rtl:text-left sm:mb-0 ltr:sm:mr-7 ltr:sm:last:mr-0 rtl:sm:ml-7 rtl:sm:last:ml-0"><a class="decoration-primary-500 hover:underline hover:decoration-2 hover:underline-offset-2 flex items-center" href=/categories/ title=Categories>Categories</a></li></ul></nav><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">&copy;
2025
Sungho Park (gigio1023)</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.ee03488f19c93c2efb199e2e3014ea5f3cb2ce7d45154adb3399a158cac27ca52831db249ede5bb602700ef87eb02434139de0858af1818ab0fb4182472204a4.js integrity="sha512-7gNIjxnJPC77GZ4uMBTqXzyyzn1FFUrbM5mhWMrCfKUoMdsknt5btgJwDvh+sCQ0E53ghYrxgYqw+0GCRyIEpA=="></script></footer><div id=search-wrapper class="invisible fixed inset-0 flex h-screen w-screen cursor-default flex-col bg-neutral-500/50 p-4 backdrop-blur-sm dark:bg-neutral-900/50 sm:p-6 md:p-[10vh] lg:p-[12vh]" data-url=https://gigio1023.github.io/ style=z-index:500><div id=search-modal class="flex flex-col w-full max-w-3xl min-h-0 mx-auto border rounded-md shadow-lg top-20 border-neutral-200 bg-neutral dark:border-neutral-700 dark:bg-neutral-800"><header class="relative z-10 flex items-center justify-between flex-none px-2"><form class="flex items-center flex-auto min-w-0"><div class="flex items-center justify-center w-8 h-8 text-neutral-400"><span class="relative block icon"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="search" class="svg-inline--fa fa-search fa-w-16" role="img" viewBox="0 0 512 512"><path fill="currentcolor" d="M505 442.7 405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9.0 208 0S0 93.1.0 208s93.1 208 208 208c48.3.0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9.0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7.0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7.0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></span></div><input type=search id=search-query class="flex flex-auto h-12 mx-1 bg-transparent appearance-none focus:outline-dotted focus:outline-2 focus:outline-transparent" placeholder=Search tabindex=0></form><button id=close-search-button class="flex items-center justify-center w-8 h-8 text-neutral-700 hover:text-primary-600 dark:text-neutral dark:hover:text-primary-400" title="Close (Esc)">
<span class="relative block icon"><svg viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></button></header><section class="flex-auto px-2 overflow-auto"><ul id=search-results></ul></section></div></div></div></body></html>