[{"content":"","date":"13 March 2025","externalUrl":null,"permalink":"/","section":"","summary":"","title":"","type":"page"},{"content":"","date":"13 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"20 May 2022","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"20 May 2022","externalUrl":null,"permalink":"/tags/gpu/","section":"Tags","summary":"","title":"Gpu","type":"tags"},{"content":"","date":"20 May 2022","externalUrl":null,"permalink":"/categories/gpu/","section":"Categories","summary":"","title":"GPU","type":"categories"},{"content":"Apple silicon에서도 pytorch에서의 gpu acceleartion이 적용됐다. 이참에 사용하는 프레임워크에서 gpu acceleartion을 사용하는 방법들을 정리하고자 한다.\nPytorch # 2022-05-20 기준.\nPytorch 1.12를 설치하면 된다. Nightly build에서만 작동한다.\nimport torch import torchvision.models as models from torchsummary import summary print(torch.__version__) mps_device = torch.device(\u0026#34;mps\u0026#34;) print(mps_device) # Create a Tensor directly on the mps device x = torch.ones((1, 3, 224, 224), device=mps_device) print(x.shape) # Move your model to mps just like any other device model = models.resnet18() summary(model, (3, 244, 244)) model.to(mps_device) # Now every call runs on the GPU pred = model(x) print(pred, pred.shape) HuggingFace # pip, conda로 설치가 안되므로 직접 빌드해야 한다. rust tokenizer를 사용했다.\n# install rust on arm terminal curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh # intsall tokenizer git clone https://github.com/huggingface/tokenizers cd tokenizers/bindings/python pip install setuptools_rust python setup.py install # install transformers pip install git+https://github.com/huggingface/transformers # install datasets pip install git+https://github.com/huggingface/datasets from transformers import AutoTokenizer, BertModel device = \u0026#34;mps\u0026#34; sentence = \u0026#39;Hello World!\u0026#39; tokenizer = AutoTokenizer.from_pretrained(\u0026#39;bert-large-uncased\u0026#39;, use_fast=True) model = BertModel.from_pretrained(\u0026#39;bert-large-uncased\u0026#39;) inputs = tokenizer(sentence, return_tensors=\u0026#34;pt\u0026#34;).to(device) model = model.to(device) outputs = model(**inputs) print(outputs) Ref # https://discuss.pytorch.kr/t/apple-m1-pytorch-gpu/276?fbclid=IwAR2noGGOMnCVSqfKF2WQ9fHajerTkBWdB4TPkwwMCt16CJrAwi9sCHmInoc https://towardsdatascience.com/hugging-face-transformers-on-apple-m1-26f0705874d7 https://discuss.huggingface.co/t/is-transformers-using-gpu-by-default/8500 ","date":"20 May 2022","externalUrl":null,"permalink":"/posts/mac/2022-05-20-m1-gpu-acceleration/","section":"Posts","summary":"","title":"m1 gpu acceleration","type":"posts"},{"content":"","date":"20 May 2022","externalUrl":null,"permalink":"/categories/mac/","section":"Categories","summary":"","title":"Mac","type":"categories"},{"content":"","date":"20 May 2022","externalUrl":null,"permalink":"/tags/mac/","section":"Tags","summary":"","title":"Mac","type":"tags"},{"content":"","date":"20 May 2022","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"산개해있던 정보들을 취합했다.\nRosetta2 # /usr/sbin/softwareupdate --install-rosetta agree-to-license brew # terminal이 arm이냐, intel이냐에 따라서 brew가 알아서 설치된다. 이원화를 위해서 경로만 따로 파주면 된다.\nARM brew installation # /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; Intel brew installation # arch -x86_64 /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install.sh)\u0026#34; # 위 명령어가 안되면 아래 명령어로 실행 arch -x86_64 zsh cd /usr/local mkdir homebrew curl -L https://github.com/Homebrew/brew/tarball/master | tar xz --strip 1 -C homebrew .zshrc에 아래 내용 기입\nalias ibrew=\u0026#39;arch -x86_64 /usr/local/homebrew/bin/brew\u0026#39; terminal # # install oh-my-zsh sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\u0026#34; # install syntax-highlight brew install zsh-syntax-highlighting source /usr/local/share/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh # install auto-suggestion brew install zsh-autosuggestions Language toggle key # Karabiner-elements 설치 Setting \u0026gt; Keyboard \u0026gt; Shortcuts \u0026gt; Input Sources에서 뭐든 1개만 옵션 키고 F18로 binding key 변경 Karabiner-elements \u0026gt; Simple modifications에서 아래와 같이 변경\nFrom key: right_command\nTo key: f18 Menu bar settings # Runcat # 무작위 runner 쓰도록 변경\nHidden bar # Command 키 눌러서 \u0026lsquo;|\u0026rsquo;, \u0026lsquo;\u0026gt;\u0026rsquo; 모양의 아이콘을 옮긴다. 해당 경계선을 기준으로 menu icon 숨길 수 있음.\nUnsplash Wallpaper # Daily하게 바뀌게 하고 맘대로 바꾸자.\n","date":"19 May 2022","externalUrl":null,"permalink":"/posts/mac/2022-05-19-mac-settings/","section":"Posts","summary":"","title":"Mac basic settings","type":"posts"},{"content":"","date":"18 May 2022","externalUrl":null,"permalink":"/categories/generative-model/","section":"Categories","summary":"","title":"Generative-Model","type":"categories"},{"content":"","date":"18 May 2022","externalUrl":null,"permalink":"/tags/generative-model/","section":"Tags","summary":"","title":"Generative-Model","type":"tags"},{"content":"","date":"18 May 2022","externalUrl":null,"permalink":"/categories/nlp/","section":"Categories","summary":"","title":"NLP","type":"categories"},{"content":"","date":"18 May 2022","externalUrl":null,"permalink":"/tags/nlp/","section":"Tags","summary":"","title":"NLP","type":"tags"},{"content":"잘못된 내용에 대한 피드백은 언제나 감사합니다. 간단하게 요약하고자 하는 것을 추구하는데, 몰랐던 내용들이 많아서 논문의 대부분 내용을 포함시켰습니다.\nAbstract # 논문은 NLP에서 성공적이었던 retrieval-based approach를 사용하여 diffusion model을 보완했다. 학습 시에, CLPI과 training instance에 이웃한 요소들과 같은 visual features를 retrieve하여 학습한다.\n논문의 모델은 CLIP의 image-text embedding space를 사용하는데, class-conditional이나 text-image synthesis처럼 explicity train을 하지 않는 task에 대해서 높은 성능을 보여준다. 또한 text와 image embedding에 모두 영향을 받을 수도 있다.\n논문의 모델은 SOTA를 찍은 unconditinal generation에도 적용할 수 있다.\n논문의 접근법은 적은 컴퓨터 자원을 소모하고 구현하기 쉽다고 한다.\nIntroduction # Language modeling과 high-fidelity images / other data types에 대한 generative synthesis는 엄청난 도약을 이뤘다. 특히 image synthesis에서 큰 충격을 준 결과들이 나왔다. [Ref 1, 2, 3, 4] 이에 대한 3가지 main factor들은 아래와 같다.\nVision task에서 transformer의 성공. Image synthesis에서는 이를 autoregressive modeing으로 수행했다. Diffusion model이 high-resolution image generation에 성공적으로 적용됐고, generative image modeling의 표준이 됐다. 1, 2번의 방법들은 scale이 잘 된다. 특히, 모델과 배치 사이즈를 고려할 때 scalability가 성능에 핵심적이라는 증거가 있다. 하지만 대부분의 성능 개선 방법은 단순히 computational power와 parameter를 늘림으로써 이뤄진다. 이 논문은 이러한 방법을 사용하지 않고 성능을 향상시키고자 한다.\n반대로, retrieval-augmented generative language model의 성공에 영감을 얻어서, 논문은 visual examples의 memory를 위해 trainable parameters를 trade-off?한다. 또한 image database에 의하여 제안된 모델의 일부를 명시적으로 정의한다. (무슨 말인지 모르겠다..)\nTraining하는 동안, retrieval-augmented semi-parametric model은 nearest neighbor lookup을 통해 데이터베이스에 접근하고 retrieved visual building blocks를 기반으로 synthesize images를 학습한다.\n이러한 retrieval-enhanced 전략을 diffusion model과 결합하는 것으로 multi-modal datasets에서 fully parametric conterparts를 능가하는 경량화 모델을 얻을 수 있다.\nFig2에서는 CLIP을 사용하여 text-image space에서 nearst neighbors를 retrieve할 때, text prompts를 exemplar-based synthesis의 쿼리로 사용함을 보여준다. CLIP의 text encoding인 $\\phi_{\\tiny CLIP}(c_{\\tiny text})$에 직접적으로 영향을 받아 retrieved neighbors를 구성할 때, 논문의 ImageNet 모델이 unseen/fictional한 text prompt를 일반화할 수 있음을 관찰했다. 반면 $\\phi_{\\tiny CLIP}(c_{\\tiny text})$를 retrieval database에서 가져온 $\\phi_{\\tiny CLIP}(c_{\\tiny text})$와 관련 있는 k - 1개의 nearest neighbors와 함께 사용하거나, 혹은 text representation 없이 k개의 nearst neighbors와 사용한다면 모델은 generalization capabilities를 보여주지 않았다.\n즉, Fig2는 \u0026lsquo;NNs only\u0026rsquo;와 \u0026lsquo;Text repr. and NNs\u0026rsquo;는 \u0026lsquo;Text repr. only\u0026rsquo;에 비해서 generalization capailities가 떨어지는 것을 보여주는 것이다.\nAbstract summary # 따라서 논문은 retrieval-augmnented generative modeling with diffuson model이라는 간단한 framework를 보여준다. CLIP의 latent space에서 seraching, conditioning을 진행함으로써 아주 작은 컴퓨팅 연산만으로 nearest neighbo representation을 만들 수 있는 것이다. 또한 retrieval 속도는 매우 빠르고 CLIP embedding은 매우 작은 storage만을 요구한다. semi-parametric approach는 high fidelity와 diversity를 모두 충족시킬 수 있다고도 한다.\nCLIP의 image-text features를 사용하는 것으로 Fig2에서 보여줬던 text-to-image, class-conditional synthesis와 같은 다양한 conditional application을 만들 수 있다.\n마지막으로 test time에서 retrieval database를 변화시키는 것이 synthesis process control에 추가적인 flexibility를 부여하는지 demonstrate 할 것이다. 또한 이것이 기존의 classifier-free diffusion model과 어떻게 결합되는지도 보여줄 것이다.\nRelated work # Diffusion Models for Image Synthesis # 일반적인 diffusion model의 성과와 한계에 대해서 짚는다. ImageNet과 같은 복잡한 Dataset에 대해서 unconditional image generation을 할 때, 모델의 크기와 compute resources가 많이 요구된다고 한다.\n논문은 이러한 한계를 극복하기 위해 trainable parameters를 external memroy와 교환하는 것을 제안한다. 이를 통해 작은 모델이라 할지라도 지속적으로 발전하는 모델과 동등한 수준의 high fidelity image generation을 수행할 수 있도록 한다.\nRetrieval-Agumented Generative Models # External memory를 활용하여 기존 모델의 성능을 향상시키는 것은 NLP 분야에서 많이 사용하는 기법이다. RETRO[5]의 경우 더 적은 parameter와 compute resources를 사용해서 SOTA를 찍은 retrieaval-enhanced transformer를 제안했다. External memory를 사용한 retireval-augmented model이 parametric deep learning model을 semi-parametric model로 변환했다.\n초기의 retireval-augmented visual model들은 external memory를 사용하지 않고 retrieval를 위해 training data를 사용했다. IC-GAN의 경우, GAN을 학습시키기 위해 training images의 neighborhood를 활용하고 training data의 single instances에 제한을 받아 samples를 생성한다.\n하지만 training data를 retireval 대상으로 삼았기 때문에 generalization capacity가 떨어질 수 밖에 없고, 우리는 exteranl memory를 통해 이를 해소하고자 한다.\nConcurrent Work # 최근에 본 논문과 비슷한 연구들로 unCLIP[6]과 kNN-Diffusion[7]이 제안됐다.\nunCLIP은 CLIP의 representation으로 diffusion model을 conditioning하고 large-scale computation을 사용해서 high quality text-image results를 만든다. 하지만 본 논문의 연구와 다르게, 이 모델은 training data에 대한 CLIP representation에 제한을 받기 때문에, geneartive text-image를 나중에 학습한다.\nkNN-Diffusion은 unCLIP의 위와 같은 문제를 neighborhood를 통해 conditioning함으로써 회피하는데, 이것은 본 논문의 연구와 매우 유사하다. 또한 서로 다른 형태의 neighborhood representation을 분석하기 위해 discrete diffusion formulation보다 continuous formulation을 사용했고 text-image synthesis에 제한되지 않는다는 점 또한 매우 유사하다.\n\u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; Updated upstream\nImage Synthesis with Retrieval-Augmented Diffusion Models # Retrieval-Enhanced Generative Models of Images # Semi-parametric generative model by introducing a tuple of trainable and non-trainable model components.\nA non-trainable samping strategy obtains a subset of the database based on a query.\nSemi-Parametric Diffusion Models # 논문의 연구진들이 이전에 연구했던 LDM(Latent diffusion model)을 활용했다. VQ-GAN의 latent space에 대해서 Diffusion model을 태운다고 생각하면 된다고 한다. 예를 들어서, VQ-GAN에서 256x256 이미지를 64x64로 donwsampling하고 여기서 reverse diffusion process를 학습한다.\nunCLIP의 경우, 64x64의 이미지를 직접 학습하기 때문에 이릘 256x256으로 변환시켜주는 super resolution을 따로 학습해야 한다. 반면, LDM은 VQ-GAN의 latent space를 사용했기 때문에 VQ-GAN에 다시 64x64 이미지를 넣어줘서 256x256 이미지를 얻는다. Object function은 DDPM에서 제공된 formula를 사용한다.\nInference for Retrieval-Augmented Diffusion Models # Training 과정과 다르게 Inference에서는 Query image가 존재하기 않기 때문에 별도의 process를 추가해야 한다. text query를 만들어서 CLIP으로 text-conditional generation을 해볼 수 있을 것이다. query image가 없다고 할지라도, CLIP은 text-image의 shared representaion을 가지고 있기 때문에 이처럼 활용할 수 있다.\nImage 대신 text query를 간단하게 만든다. CLIP embedding을 통해 text query의 text embedding을 구한다. text embedidng과 datset image들간의 유사도를 통해서 neighborhood를 얻는다. Inference 가능한 내용을 정리하면 아래와 같다.\ntext-to-image text query의 embedding을 CLIP을 통해 구한다. 이를 활용해 neighborhood searching에 사용 class-condition text query로 \u0026ldquo;A photo of { }\u0026ldquo;를 사용 unconditional Fixed dataset에서 random하게 sampling하고 이에 대한 neighborhood searching을 할 수 있다. 하지만 이렇게 시도한 결과 단순한 이미지이거나 reference와 유사한 이미지만 나왔다고 한다. 따라서 아래와 같이 proposal distribution을 만들어서 실제로 학습에 많이 기여한 sample 위주로 고를 수 있도록 했다고 한다. proposal distribution을 통해 pseudo query를 생성하고 이를 통해 neighborhood seraching해서 unconditional generation 수행. Trading Quality for Diversity # Truncation distribution of datasets # Unconditoinal generation 수행 시 너무 다양하게 이미지가 생성되기 때문에, density가 높은 top m개만 사용했다고 한다.\nClassifier-free guidance # conditional image generation에서 generation 품질 향상을 위해 사용했다. condition, uncondition 상황에서 모두 generation 해보고 두 상황의 차이만큼 denoising을 하도록 하는 것이다.\nMethods Overview # Experiments # Experiments settings # Decoding head(generative model): LDM with 400M parametes. 기존 LDM이 500M이기 때문에 약간 작은 모델이다. Retrieval model: CLIP-ViT-B/32 Fixed Database: 20M examples of OpenImages(9M). OpenImages의 resolution의 한 차원이 보통 1200보다 크기 때문에, 하나의 이미지에서 2 ~ 3개의 patch를 뽑아서 사용. Training dataset: ImageNet NN search: ScaNN search algorithm Image encoder # VQ-GAN도 pooling을 통해서 구현할 수 있지만, 역시 CLIP embedding을 사용하는 것이 제일 좋았다고 한다.\nK-NN # K에 따른 evaluation이다. K가 증가할수록 recall, precision, FID, IS가 떨어진다. classifier-free guidance 등을 통해서 image의 품질을 높일 수는 있기 때문에 recall이 높아지는 방향으로 K를 설정하지 않았나 생각한다고 하신다.[8]\nPatch size # OpenImages의 resolution이 매우 높기 때문에 patch를 어떻게 하느냐도 중요했다고 한다. 이를 patchify이라고 하는데, size가 클 수록 성능이 좋았다고 한다.\n엄격한 evaluation은 아니라고 할 수 있는데, CLIP이 64x64로 학습되어 있지 않기 때문에 64x64에 대한 성능이 나쁘게 나왔다고도 볼 수 있기 때문이다.[8]\nTraining data complexity # Generation하고자 하는 target을 변경하면서 실험한 것이다. Dogs, Mamals, Animals로 범위를 늘릴수록 precision, FID는 baseline, SP-LDM 모두 지표가 나빠진다. 하지만 recall의 경우 SP-LDM은 지표가 오히려 좋아진다.\ngeneration 성능이 하락하더라도 external dataset에서 관련된 이미지를 가져오고 이것과 유사한 이미지를 generation하고자 노력할 것이다. 따라서 recall 수치가 올라간다고 논문에서는 주장한다.\nUnconditiona generation result # Conditional generation result # class에 대한 CLIP의 text embedding과 k - 1개의 nearest neighborhood를 통해 generation해도 잘 만든다고 한다. 학습할 때 class-condition을 사용하지 않았기 때문에 의미가 있다고 한다.\nText-to-image # 사용 가능한 경우의 수는 아래와 같다.\ntext embedding condition text embedding + nearest neighborhood condition nearest neighborhood condition ImageNet으로만 학습했음에도 CLIP의 text embedding으로 condition해도 결과가 잘 나왔다고 한다. 오히려 image에 대한 많은 정보를 줄 수록 성능이 하락함을 볼 수 있다. text embedding과 nearest neighborhood를 함께 사용하면 text embedding만 사용했을 때보다 좋지가 않은데, # Image Synthesis with Retrievl-Augmented Diffusion model # 본 논문의 모델은 data points를 explicit part of the model로 고려한다. 기존의 neural generative approaches와 다른 점은, 내부 데이터 데이터로부터 적절한 data representations를 retrieve하는 data representation과 non-learnable retireval function 또한 parameterized했다는 것이다. [5]에서 사용된 방법을 NLP modeling을 토대로, 본 논문은 nearest neighbor lookup을 retrieval pipeline으로 구현했다.\nFig3에서 본 논문의 접근법을 보여주고 있다.\nOverview(Fig3) # 본 논문의 retrieval-augmented, semi-parametric diffusion model은 아래와 같이 구성돼있다.\n학습 가능한 conditional generative decoding head인 $p(x \\vert \\cdot)$ database $\\mathcal{D}$의 구성 요소는 아래와 같다. visual examples $p_\\theta$에 대한 conditioning을 제공하는 subset $\\mathcal{M}_{\\mathcal{D}}^{(k)} \\subseteq \\mathcal{D}$를 얻을 수 있는 sampling strategy인 $\\xi_k$ 좌측의 $\\xi^{train}_k$를 학습하는 동안 $\\mathcal{D}$에서 nearest neighbors를 retrieve한다.\nStashed changes\nReference # Paper: https://arxiv.org/abs/2204.11824 1: Taming Transformers for High-Resolution Image Synthesis 2: Diffusion model beat GANs on image synthesis 3: Glide: Towards photorealistic image generation and editing with text-guided diffusion models 4: Hierarchical text-conditional image generation with clip latents 5: Improving language models by retrieving from trillions of tokens 6: Hierarchical Text-Conditional Image Generation with CLIP Latents 7: Knn-diffusion: Image generation via large-scale retrieval 8: https://www.youtube.com/watch?v=Ktgt7bcXLYI ","date":"18 May 2022","externalUrl":null,"permalink":"/posts/ml/2022-05-18-retrieval-augmented-diffusion-model/","section":"Posts","summary":"","title":"Retrieval-Augmented Diffusion model","type":"posts"},{"content":" Primary conrtributions # 논문에서 직접 제시한 contriubution들은 다음과 같다.\n실제 세계의 modeling task에서 structured EHR로 학습한 BERT-style model이 얼마나 의미있는지 PoC한 첫번째 연구 EHR data에서 일반적이고 contextual semantics를 capture 할 수 있는 domain-specific한 cross-visit pretrainig task를 디자인 Phenotyped cohorts에서 multiped clinical task를 수행하는 SOTA methods보다 더 좋은 성능을 보여준 첫번째 demonstration Trainig dataset(Cerner)이 아닌 dataset(Truven)을 사용하여 EHR BERT 모델의 일반화 Med-BERT의 성능 향상이 모든 sample size에서 관촬됨. Training data가 제한된 상황에서도 Pretraning model이 잘 작동. EHR의 dependency semantic 시각화 툴 제공. Pretrained model, code 공개. Abstract # Structured EHR(Electronic Health Record)를 transfer learning으로 모델링하고자 하는 기존 연구들로 BEHRT와 G-BERT가 있었다.\nBEHRT는 방문 기록에 대한 medical codes에 대한 prediction을 통해 pretrain했다. 이는 AUC와 같이 non-standard한 지표를 사용했기에 기존 연구와 비교하기 어려웠다.\nG-BERT는 clinical code를 통해 GNN과 BERT embedding을 학습했다. 이 모델은 MLM의 pretrainig task를 기존 clinical code와 존재하지 않는 clinical code 간의 차이를 극대화하고 서로 다른 clinical code를 예측할 수 있는 domain specific task로 변경했다. 하지만 G-BERT의 input data는 single-visit sample인데 이는 EHR의 long-term contextual information을 식별하기에 부족했다.\n앞선 이슈들과 disease predction에 특화된 모델을 예측할 수 있도록 논문은 Med-BERT를 디자인했다고 한다. 기존의 BERT는 free text로부터 학습했다면 Med-BERT는 IDC(Internatinal Classification of Disease) codes를 사용한 구조화된 진단 데이터를 사용하여 학습했다.\nCompare with relevant study # Med-BERT는 BEHRT, G-BERT보다 큰 vocab size를 가지고 더 큰 pretraining cohort를 가지고 있다. 논문은 larger cohort size와 longer visit sequence로 pretraining 했기 때문에 contextual semantic을 이해하는데 더 도움이 될 것이라고 주장한다.\n또한 ICD-9, ICD-10과 같은 large/publicly accesiible vocabulary와 여러 기관의 데이터로 pretrainig했기 때문에 각기 다른 기관과 clinical scenario에도 적합할 것이라고 주장한다.\nBEHRT, G-BERT와 유사하게 Med-BERT는 clinical code에는 code embedding을, 서로 다른 visit에 대해서는 visit embedding을, code간의 상호관계를 식별하기 위해서는 transformer를 사용했다. BEHRT, G-BERT는 visit에 code ordering을 사용하지 않았지만, Med-BERT는 serialization embedding을 통해 code의 상대적인 순서를 나타낸다.\n논문에서는 Prolonged legth of stay in hospital(Prolonged LOS, 장기 입원 기간)을 에측하는 domain-specific한 pretraining task를 디자인했다. 이는 질병 진행 상태에 따라 환자의 건강 정보에 대한 심각성을 평가하기 위한 모델링잉 필요하고 human annotation을 요구하지 않는 유명한 clinical probelm이라고 한다. 이러한 task를 학습하는 것은 모델이 clinical하고 contextualized features를 더 잘 학습하도록 도움이 될 것이다.\nFine-tuning # Pretrained Med-BERT의 유용성은 두 개의 다른 EHR datasets의 3개의 patient cohorts로부터 다음 두 가지 prediction task에 대해 fine-tuning하여 평가한다.\nHeart failure among patients with diabets (DHF, 당뇨병과 관련된 심부전) Oneset of pancreatic cancer(췌장암 발생) 해당 task들은 pretraing의 prediction task인 MLM과 Prolonged LOS와 다르기 때문에 모델이 얼마나 일반화되어 있는지 평가하기 좋을 것이라고 논문은 주장한다. 위 2개 task들이 fine-tuning taks로 선택된 이유는 다음과 같다.\n특정 diagnosis codes보다 복잡한 정보를 내포하고 있다. 해당 task들은 시간대 제약, 진단 발생 시점, 약물, 연구실 실험 데이터와 같은 정보를 내포하는 diagnosis code를 넘어서 여러 정보를 통합하는 phenotyping algorithms를 기반으로 한다. Fine-tuning은 다음과 같은 목적으로 진행했다.\n3개의 SOTA model에 Med-BERT를 추가하여 성능 향상을 test Med-BERT와 pretrained non-contextualized embedding인 word2vec-style embedding의 비교 서로 다른 fine-tuning training size에 대한 Med-BERT의 disease prediction performance Med-BERT architecture # Input data modality # Original BERT paper와 동일한 architecture(multi-level embedding, bidirectinal transformer)와 유사한 pretraining techniques(loss funcion on masking, classification pretraining task)을 사용했다. EHR과 text간의 semantic한 차이가 있기 때문에 BERT의 방법론을 structured EHR에 사용하는 것은 중요하다. ?\noriginal BERT의 input data는 1 dimension이지만, structured EHR은 multilayer, multi-relation style의 데이터다. 따라서 structured EHR 데이터를 1 dimension으로 flatten하고 BERT에 맞게 encoding을 어떻게 할지가 중요하다.\nTable3에서 이러한 차이점을 명시했다.\nModel architecture # Med-BERT에는 3가지 형태의 input이 존재한다.\ncode embedding Low-dimensional representations of each diagnosis code serialization embedding Relative order of each visit. 이 논문의 데이터에 대해서는 priority order, of each code in each visit를 의미. visit embedding sequecne에서 각각의 visit를 구분 BERT와 다르게 [CLS], [SEP]를 쓰지 않았다. [CLS]에 정보 요약을 하기에 sequence의 길이가 매우 길기 때문에, 별도의 feed-forward layer로 output token들의 내용을 압축한다. 또한 visit embedding만으로도 각각의 visit 정보들을 잘 분리할 수 있기 때문에 [SEP]가 불필요하다고 논문은 주장한다.\nPretraining Med-BERT # Original BERT paper의 recommended hyperparamter와 알고리즘으로 pretraining했다.\nMasked LM # Orignial BERT paper의 masking 알고리즘을 사용했다. 임의의 code에 대해서 80%의 확률로 [MASK] 토큰으로 변환되고, 10%의 확률로 random code로 변환되고, 10%의 확률로 바뀌지 않는다.\nPrediction of prolonged length of stay (Prolonged LOS) in hospital # Pretrained model의 generalizability를 위해 disease-specific하지 않고 pretrainig dataset과 유사한 clinical problem을 설정했다고 한다. 병원에서 일반적으로 사용되는 quality-of-care indicators(치료 품질 지표), mortality(사망률), earlyf readmission(입원), Prolonged LOS가 Pretrainig task 후보였다. 이 중, mortality와 early readmission은 99%를 초과하는 정확도를 보이며 비교적으로 매우 쉬운 task라고 판명됐다고 한다. 따라서 입원 날짜가 7일을 넘기는지 평가하는 task를 pretraining task로 정했다.\n데이터 구조적으로 prolonged LOS는 Med-BERT의 bidirectional한 구조를 활용한다. 왜냐하면 과거 방문에서 기록된 환자의 건강 정보와 다음 방문에 대해서 LOS는 영향력을 가지기 때문이다. 반면, disease onest이나 mortality는 항상 환자 정보 sequence 중 마지막 방문에서 끝날 수 밖에 없기 때문에 one directional한 구조를 가진다.\nDownstream prediction task by fine-tuning # Pretrainig model은 input data에 대해서 general purpose embedding을 출력할 뿐, prediction labael을 출력할 수 없다.\nEHR predicitive model에서는 prediction head로 RNN을 사용했다.\nEvaluation # 2개의 Database에서 가져온 3개의 cohort로부터 two disease prediction task를 수행하여 평가했다.\ntwo task: DHF, PaCa prediction 3 cohort: DHF-Cerner and PaCa-Cerner cohort for both task; Truven for only the pancreatic cancer prediction BEHRT, G-BERT와 다르게 Med-BERT는 pretrainig task와 evalutation task가 보다 복잡하고 phenotyping from multiple perspectives가 요구된다. 따라서 논문에서는 논문의 방법론이 보다 현실적이고 generalizability를 확립하는데 도움이 된다고 주장한다.\n비교에 사용된 method들은 다음과 같다.\nGRU, Bi-GRU RETAIN: a popular disease prediction model with double GRUs with attention model L2LR: L2-regulariezd logistic regression RF: random forest Reference # Med-BERT: https://www.nature.com/articles/s41746-021-00455-y Med-BERT Github: https://github.com/ZhiGroup/Med-BERT BEHRT: https://www.nature.com/articles/s41598-020-62922-y G-BERT: https://arxiv.org/abs/1906.00346 ","date":"17 May 2022","externalUrl":null,"permalink":"/posts/ml/2022-05-17-med-bert/","section":"Posts","summary":"","title":"Med-BERT","type":"posts"},{"content":"","date":"17 May 2022","externalUrl":null,"permalink":"/tags/medical-ai/","section":"Tags","summary":"","title":"Medical Ai","type":"tags"},{"content":"스터디에서 진행했던 논문 리뷰 발표 정리.\nhttps://github.com/luyug/Condenser\nCondensor # Abstract # PLM은 text comparison과 retrieval에서 좋은 성능을 보여줬다. 하지만 dense encoder를 학습시키기 위해서는 많은 데이터와 복잡한 기술이 요구된다. 이 논문은 표준적인 LM의 내부 구조가 dense encoder로 사용되기에 충분하지 않은 이유를 밝힌다. 또한 Condenser가 text retrieval와 이와 유사한 task에서 표준적인 LM보다 더 좋은 성능을 보임을 보여준다.\nIssues with Transformers Encoder # Transformers에서 CLS token을 포함한 모든 token들은 한번의 attention으로 sequence 상의 다른 token들에 대한 정보를 받는다. CLS token에 대한 분석 논문에서는 다음과 같이 CLS token에 대해 분석한다.\n대부부의 middle layers에서 CLS token은 다른 text token들과 유사한 attention pattern을 가지게 되고, 다른 token들에 의해서 attention되지 않는다. last layer에서 CLS는 NSP task를 위해 uniqe한 broad attention을 가진다. 이러한 분석들을 종합하면 CLS token은 많은 middle layers에서 활성화되지 않다가, 마지막 attention round에서만 활성화된다고 생각할 수 있다. 논문에서 정의하고 싶은 효과적인 bi-encoder는 all layers를 통해 서로 다른 수준의 정보를 집계할 수 있어야 한다고 말한다. 이러한 관점에서 표준적인 PLM은 fine-tuning을 위한 준비가 되지 않았다고 주장하는 것이 논문의 요지이다.\nMethod # Pre-training # 논문은 아래와 같은 model design을 통해 이를 해결하고자 한다.\n하나의 attention만으로 pre-trainig하지 않고, early encoder, late encoder, condenser header를 통해 pre-training을 하고자 주장한다. 이를 early encoder, late encoder를 수식으로 표현하면 아래와 같다.\n$$[h^{0}_{cls};h^{0}] = Embed([CLS;x])$$\n$$[h^{early}{cls};h^{early}] = Encoder{early}([h^{0}_{cls};h^{0}])$$\n$$[h^{late}{cls};h^{late}] = Encoder{late}([h^{early}_{cls};h^{early}])$$\nearly encoder의 hidden state가 skip connection으로 condenser head에 들어간다. (논문에서는 short circuit이라고 표현했다) late encoder의 CLS가 condenser head에 투입되면서 late-early representation이 condenser head가 들어가도록 유도한다.\nMLM Loss는 아래와 같이 구한다.\n$$\\mathcal{L}\\text{mlm} = \\sum{i \\in \\text{masked}} \\text{CrossEntropy}(W h^{cd}_i, x_i)$$\n이러한 구조에서 late encoder이 token representation을 refine(정비?, 정제?)할 순 있지만 오직 $h^{late}_{cls}$를 통해서만 새로운 정보를 통과시킨다. 따라서 late encoder는 새롭게 생성되는 정보들을 CLS representation에 집계하려고 노력할 것이고, heads는 late CLS에 의존해서 prediction을 하게 된다.\nearly layer의 hidden state를 skip connecting함으로써 encoding 결과의 local information과 input text의 문법적인 구조를 제거했다. 논문에서는 이를 통해 CLS가 input text의 global meaning에 집중하도록 했다고 주장한다.\nFine-tuning # Condenser의 fine-tuning에서 head는 drop된다. Fine-tuning을 통해 $CLS\\ h^{late}_{cls}$를 학습하고 backprogate하며 backbone의 graident를 update한다. head가 pre-training의 guide만을 수행하면서 Condenser는 encoder backbone의 크기를 줄일 수 있고 효율적으로 변한다. 실제로, Condenser는 경량화된 BERT의 대체품이 될 수 있다.\nWeight Initialization # Condenser의 head는 random하게 초기화하고 early, late encdoer는 기존 PLM의 weight를 사용했다. 개인적으로 이러면 연구가 너무 쉬운게 아닌가?. .싶긴했다.\nHead에 대해서 gradient backpropgation을 수행할 때 backbone weight가 이를 방해하는 것을 막아야 한다. 따라서 late output에 대한 MLM을 문맥적인 제약사항으로 아래와 같이 사용하여 Loss에 추가했다.\n$$\\mathcal{L}\\text{mlm}^c = \\sum{i \\in \\text{masked}} \\text{CrossEntropy}(W h^{late}_i, x_i)$$\n$$\\mathcal{L} = \\mathcal{L}\\text{mlm} + \\mathcal{L}\\text{mlm}^c$$\nResult # Sentence Similarity # Retrieval for Open QA # Open-domain에서의 retrieval 성능을 말하는 듯 하다. Retrieval for Web Search # Open-domain이되 Web Search에 대한 지표.\ncoCondenser # 이 논문에서는 기존의 dense retrieval가 가지는 두 가지 문제점에 대해서 밝힌다.\n학습 데이터의 노이즈 large batch size에 대한 요구 논문은 Condenser를 pre-training architecture로 사용했다. 또한 passage embedding space를 학습하기 위해서 unsupervised corpus-level contrastive loss로 학습하는 coCondenser를 제안한다.\ncoCondenser는 larget batch trainig뿐만 아니라 augmentation, synthesis, filtering과 같은 heavy data engineering의 필요성을 제거해준다.\n","date":"16 May 2022","externalUrl":null,"permalink":"/posts/ml/2022-05-16-condenser-cocondenser/","section":"Posts","summary":"","title":"Condenser, coCondenser","type":"posts"},{"content":"","date":"16 May 2022","externalUrl":null,"permalink":"/tags/dense-retrieval/","section":"Tags","summary":"","title":"Dense-Retrieval","type":"tags"},{"content":"","date":"16 May 2022","externalUrl":null,"permalink":"/tags/paper-review/","section":"Tags","summary":"","title":"Paper-Review","type":"tags"},{"content":"추가 예정..\nGrad Cache # Overview # In-batch negative 방식의 contrastive learning에서 마치 gradient accumulation처럼 large batch를 사용하게 해주는 방식이다.\n일반적인 학습 방법의 경우, batch-wise하게 loss 계산을 하지 않기 때문에 loss update를 한꺼번에 모아서 하는 것에 문제가 없다. 하지만, DRP, MRC model과 같이 contrastive leraning을 시도할 때 in-batch negative를 사용할 경우, batch-wise하게 loss가 계산되기 때문에 batch 내 data들간에 종속성이 발생한다. 따라서 contrastive learning에서는 gradient accumulation이 사용 불가능하다.\nGrad Cache에서는 grad accumulation과 유사한 방법을 contrastive learning에서 구현해, single gpu로도 많은 batch size를 확보할 수 있도록 해준다.\nText and Code Embeddings by Contrastive Pre-Training에서 batch size를 12288까지 늘린다. 하드웨어적으로 불가능에 가까운 영역이기 때문에 contrastive learning에서 큰 batch size를 확보하기 위해 Grad Cache를 사용한다.\nMethod # Reference # Arxiv: Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup https://seopbo.github.io/gradCache/ https://github.com/luyug/GradCache ","date":"12 April 2022","externalUrl":null,"permalink":"/posts/ml/2022-04-12-grad-cache/","section":"Posts","summary":"","title":"Grad Cache","type":"posts"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/categories/ml/","section":"Categories","summary":"","title":"ML","type":"categories"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/tags/ml/","section":"Tags","summary":"","title":"ML","type":"tags"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/tags/ml-engineering/","section":"Tags","summary":"","title":"Ml-Engineering","type":"tags"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/categories/ml-engineering/","section":"Categories","summary":"","title":"ML-Engineering","type":"categories"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/tags/multi-gpu/","section":"Tags","summary":"","title":"Multi-Gpu","type":"tags"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/tags/nvt/","section":"Tags","summary":"","title":"Nvt","type":"tags"},{"content":" NVTabular # https://nvidia-merlin.github.io/NVTabular/v0.6.1/Introduction.html\nNvidia에서 제공해주는 tabular data에 대한 Feature Engineering, Preprocessing 라이브러리이다. Transformers4Rec에서 해당 라이브러리를 사용하기에 정리했다.\nGPU 연산도 지원한다고 한다.\nInstallation # 되도록 nvidia docker를 사용하자. pip는 의존성 문제가, conda는 system library에 대한 의존성 문제가 발생했었다. (Ubuntu 18.04 기준)\n# 1. Run Nvidia Merlin container docker run --gpus all --rm -it -p 8888:8888 -p 8797:8787 -p 8796:8786 --ipc=host \\ -v /$(pwd)/data:/workspace/data \\ nvcr.io/nvidia/merlin/merlin-pytorch-training:22.03 /bin/bash # 2. Run Jupyter Lab cd /transformers4rec/examples jupyter-lab --allow-root --ip=\u0026#39;0.0.0.0\u0026#39; --port 8888 Workflow # 일조으이 pipeline이다. NVT를 통해 수행할 작업을 workflow에 정의한 후, workflow.fit을 통해 pipeline을 실행한다. GPU를 통해 연산하기 때문에 GPU VRAM이 자꾸 터졌다..\nCategorify # Tabular data에서 text 형태의 categorical data를 unique integer value로 변환해준다.\n# Define pipeline cat_features = CATEGORICAL_COLUMNS \u0026gt;\u0026gt; nvt.ops.Categorify(freq_threshold=10) # Initialize the workflow and execute it proc = nvt.Workflow(cat_features) proc.fit(dataset) proc.transform(dataset).to_parquet(\u0026#39;./test/\u0026#39;) ","date":"12 April 2022","externalUrl":null,"permalink":"/posts/ml/2022-04-12-nv-tabular/","section":"Posts","summary":"","title":"NVTabular","type":"posts"},{"content":"","date":"12 April 2022","externalUrl":null,"permalink":"/tags/preprocessing/","section":"Tags","summary":"","title":"Preprocessing","type":"tags"},{"content":"ref: Line Engineering Blog - Airflow Kubernetes - 1, Line Engineering Blog - Airflow Kubernetes - 2\nAirflow와 Kubernetes를 함께 사용하는 방법은 두 가지가 있다. 각자 장단점이 있고, 서비스와 자원량에 적합한 방법을 쓰면 된다.\nAirflow on Kubernetes # Kubernets 위에 Airflow를 구성. Airflow scheduler, Airflow Worker 등 Airflow의 component들이 본래 프로세스나 하드웨어 장비 형태였다면, 여기서는 POD 형태로 구성 된다. Airflow on Kubernetes의 장점 # 모든 것이 Kubernetes 상에 있기 때문에 템플릿화가 용이하다. 따라서 관리형 Airflow 서비스 개발에 좋다.\ne.g., GCP의 Cloud Composer.\nKubernetes의 orchestration을 사용할 수 있다.\nAirflow on Kubernetes의 단점 # POD만으로 변환되기 때문에 Celery Executor를 사용한다면 master, message, broker, worker 등이 모두 Kubernetes 환경에 지속적으로 상주해야 한다.\n또한 확장성의 문제가 있다. Airflow container 내에서 여러 확장이 발생할 경우, docker 이미지도 커질 뿐더러 유지 보수/관리가 힘들어진다.\ne.g., Airflow container 내에 Hadoop client가 1개 있었는데, n개로 늘어나면 n개에 대한 환경 설정과 테스팅을 해줘야한다.\nKubernetesExecutor \u0026amp; KubernetesPodOperator # KubernetesExecutor는 Airflow가 필요할 때만 Kubernetes 환경을 사용하도록 해준다. 또한 KubernetesPodExectuor는 필요한 Docker 컨테이너만을 골라서 POD로 실행할 수 있다.\n둘은 서로 종속되지 않는 기능들이다.\nKubernetes Executor # Kubernetes Executor의 동작은 일반적인 Operator와 KubernetesPodOperator로 나뉜다.\n일반적인 Operator # PythonOperator, BashOperator, ExteranlTaskSensor 등\u0026hellip; 수행할 task를 scheduler가 찾는다. Executor가 동적으로 Airflow worker를 POD 형태로 실행 해당 Worker POD에서 개발자가 정의한 task를 수행 Pod Operator # KubernetesPodOperator의 실행 순서는 아래와 같다. 수행할 task를 scheduler가 찾는다. Executor가 동적으로 Airflow worker를 POD 형태로 실행 해당 Worker POD에서 개발자가 정의한 컨테이너 이미지를 POD 형태로 또다시 실행. -\u0026gt; 하나의 Airflow 환경에서 다양한 클라우드에 접근 가능 장점 # 가볍다 라이브러리 의존성이 없는 가벼운 이미지로도 실행 가능 기존에는 Airflow 장비 혹은 컨테이너에 Hadoop 클라이언트, Spark 클라이언트, Hive 클라이언트, Sqoop 클라이언트, Kerberos 설정 등을 모두 구성해야 했다. 하지만 KubernetesExecutor로 KubernetesPodOperator를 사용한다면 그렇지 않아도 된다. 유지보수 비용 절감 라이브러리 간 의존성 검사 불필요 다양한 데이터 플랫폼 환경에 한번에 접근 가능 -\u0026gt; 하나의 Airflow 환경만 있어도 됨. 효율적인 자원 관리 기존 Celery Executor를 Kubernetes에서 사용할 경우, master, worker가 자원을 지속적으로 점유. Kubernetes Executor의 경우 task가 실행될 경우에만 worker가 생성되고 자원이 반납. 개발 효율성 DAG들이 KubernetesPodOperator라면 Workflow DAG 코드를 템플릿화 가능 단점 # 부족한 레퍼런스 라인 데이터 엔지니어링팀이 2019년도에 작업할 때는 레퍼런스가 부족했다고 한다. 요즘도 부족해보인다. 까다로운 구성 로깅\nWorker POD가 휘발성이기 때문에 별도의 로깅 시스템을 구축해야 한다. 라인 데이터 엔지니어링팀은 GCS, S3에 저장했다고 한다. Kubernetes 자체가 러닝커브가 높다. 사용에 난이도가 있다. ","date":"8 April 2022","externalUrl":null,"permalink":"/posts/infra/2022-04-08-airflow-and-kubernetes/","section":"Posts","summary":"","title":"Airflow and Kubernetes","type":"posts"},{"content":"","date":"8 April 2022","externalUrl":null,"permalink":"/tags/mlops/","section":"Tags","summary":"","title":"Mlops","type":"tags"},{"content":"","date":"8 April 2022","externalUrl":null,"permalink":"/categories/mlops/","section":"Categories","summary":"","title":"MLOps","type":"categories"},{"content":" Set spec of service and experiment # 서비스와 자원에 대한 spec이 명확해지면 MLOps 구성에 대한 결정이 가능하다.\n절대적인 솔루션이 없기 때문에 여러 솔루션을 혼용해도 된다. e.g., 서비스 스펙 상 kubernetes를 통한 pod 생성과 airflow를 통한 batch process가 병행되어야 할 수도 있다. 그렇다면 Airflow on Kubeflow 혹은 Airflow DAG를 통해 Kubeflow Pipeline을 trigger 구현하여 아키텍쳐를 구성해보자.\nMLOps frameworks # MLflow model tracking: commit, 실험지표, 실험 결과 artifact 관리 DVC(data version control): S3 Bucket과 호환되는 minio를 사용. git commit 기준으로 versioning한다. Airflow ETL(Extraction, Transform, Load)이나 범용적인 task ochestration이 필요할 때. shell script나 py파일을 사용하는 것만으로도 충분할 때. Kubeflow 다양하고 scaling이 가능한 ML/DL pipeline이 요구될 때 노트북(ipynb) 위주의 개발, 배포가 필요하다면 유용. 예시 # CPU를 활용하는 작업이 많고 scaling이 필요하다면 Airflow + Kubernetes CPU를 활용하는 작업이 적다면 Airflow pipeline이 복잡하고 단순 scheduling만으로 불가능하다면 Kubeflow or Airflow + Kubeflow ","date":"8 April 2022","externalUrl":null,"permalink":"/posts/infra/2022-04-08-mlops-summary/","section":"Posts","summary":"","title":"MLOps Summary","type":"posts"},{"content":" Overview # 남는 VRAM 자원에 대한 자유로운 할당은 Kubernetes에서는 GPU 단위로만 가능할까.\n잉여 자원을 다른 작업에 어떻게 할당 가능한지에 대한 task에 대해 정리했다.\nExamples # GPU spec: 32GB Workstation spec: GPU 2개 상시 Inference spec: GPU 1개, VRAM 12GB 여유 VRAM이 32GB, 20GB인 GPU가 발생.\n잉여 자원을 다른 task에 할당해서 GPU를 최대한으로 활용하고 싶다. HOWTO # Replica # pod 수를 제한하고 미리 요구 자원을 계산이 된다면 replica를 통해서 전체 자원 할당량 제한은 가능. e.g., 최소 CPU 1개, RAM 2GB, 최대 CPU 2개 RAM 4GB의 pod이 최대 5개만 생성 가능\n단, CPU와 RAM에 대해서만 가능하고, GPU에 대해서는 불가능하다.\nExtended Resources # ref: Apply Extended Resources\nKubernetes의 Extended Resources를 사용해 pod당 할당될 수 있는 VRAM을 조절해서 train, serving pod에 대한 자원을 조절할 수 있다.\n하지만, runtime 도중에 해당 컨테이너가 전체 vram을 다 잡아먹어도 알 수가 없다. 해당 컨테이너를 감시하는 또 다른 컨테이너를 생성하면 되지만 불필요한 자원 점유가 늘어난다.\nGPU Virtualization # 본래는 GPU Virtualization을 VMWare으로 실현해 하나의 GPU에 대한 자원 할당을 한다. 하지만 해당 가상화를 data center에 들어가는 gpu급에서만 지원하고 VMWare도 유료다..\n결론 # Kubernetes에서 VRAM 쪼개기를 지원하지 않는다. 따라서 Extended Reousrces를 써서 POD 생성 시점에서 VRAM을 쪼개서 할당 가능하다. 하지만 이것이 해당 POD의 VRAM 할당량을 조절해주는 것은 아니다.\n","date":"8 April 2022","externalUrl":null,"permalink":"/posts/ml/2022-04-08-split-vram-of-gpu-on-kubernetes/","section":"Posts","summary":"","title":"Split VRAM of GPU on Kubernetes","type":"posts"},{"content":"","date":"6 April 2022","externalUrl":null,"permalink":"/tags/blog/","section":"Tags","summary":"","title":"Blog","type":"tags"},{"content":"","date":"6 April 2022","externalUrl":null,"permalink":"/categories/blog-building/","section":"Categories","summary":"","title":"Blog-Building","type":"categories"},{"content":"ref: github.io setting reference\nvelog.io/@naem1023에서 naem1023.github.io로 옮기는 중.\n커스터마이징, 로컬 포스팅, 필요하다면 개인 서버에서 deploy가 가능하다는 장점이 velog의 간편성보다 좋다고 생각했다.\nInstallation # # install ruby, jekyll, bundler # Ubunu sudo apt install ruby ruby-dev build-essential # m1 mac brew install ruby rbenv rbenv install 3.1.2 rbenv global 3.1.2 # install jekyll, bundler gem install jekyll bundler bundle update --bundler bundle add webrick bundle install --redownload # set zshrc echo \u0026#39;eval \u0026#34;$(rbenv init - zsh)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc echo \u0026#39;export PATH=\u0026#34;/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.zshrc # setup jekyll theme and deploy git clone {git address of jekyll theme} mv ./minimal-mistakes {name of my github.io repository} cd {{name of my github.io repository}} git remote remove origin git remote add origin {link of my github.io repository} git push -u origin master Testing # # Serve in local environmnet bundle exec jekyll serve YFM(YAML Front Matter) # markdown 형식 포스트의 정보를 지정. YAML 형태로 작성. markdown 상단에 아래와 같이 표시. --- title: \u0026#34;velog to github.io\u0026#34; excerpt: \u0026#34;excerpt\u0026#34; toc: true toc_sticky: true toc_label: \u0026#34;페이지 주요 목차\u0026#34; # default: On this page header: teaser: /assets/images/bio-photo-keyboard-teaser.jpg categories: - Blog building tags: - Blog last_modified_at: 2022-04-06T08:06:00-05:00 --- 이중 괄호를 통해 YFM 정보에 접근 가능 e.g., {{ page.title }}, {{ page.last_modified_at }} toc(table of contents)를 통해 markdown의 H1~H6 사이의 헤더 목록을 오른쪽에 위치시킬 수 있다.\n_config.yml # Jekyll 동작 설정에 대한 내용을 담고 있다. 다른 파일들은 jekyll 서비스 중에도 변경사항이 발생하면 자동 반영되지만, _config.yml은 재빌드 시에만 새로운 내용이 반영된다.\n_config.yml의 내용을 아래처럼 활용 가능하다.\n\u0026lt;{{ site.url }}{{ site.baseurl }}/blog/\u0026gt; 위 구문을 jekyll로 빌드하면 \u0026lt;{{ site.url }}{{ site.baseurl }}/blog/\u0026gt;로 표시된다.\nComments # _config.yml의 comments를 수정해서 댓글 서비스 추가 가능.\nutterances 사용 예정.\nref: utterances setting blog\nOpen Graph Image # Open Graph Protocol을 지정할 수 있다.\nog_image : \u0026#34;path\u0026#34; og_description : \u0026#34;\u0026#34; og_title : \u0026#34;\u0026#34; Open Graph Protocol란\nSite Author # site 좌측 사이드바에 표시되는 정보들이다. 유의할점은 url을 적을 때 쌍따옴표 없이 써야한다.\n# Site Author author: name : \u0026#34;취미코딩하는 개발자\u0026#34; avatar : \u0026#34;/assets/images/bio-photo-keyboard.jpg\u0026#34; bio : \u0026#34;회사에서는 월급 받고 집에서는 취미로 코딩하는 개발자\u0026#34; location : \u0026#34;South Korea\u0026#34; email : links: - label: \u0026#34;Email\u0026#34; icon: \u0026#34;fas fa-fw fa-envelope-square\u0026#34; url: mailto:devinlifeidea@gmail.com - label: \u0026#34;Website\u0026#34; icon: \u0026#34;fas fa-fw fa-link\u0026#34; url: \u0026#34;https://devinlife.com\u0026#34; Outputting # 블로그 표시 방법들에 대한 정보들이다. paginate를 통해 첫 페이지에 보여줄 최근 게시물의 수를 지정 가능하다. 해당 개수를 넘으면 다음 페이지 번호들이 표기된다.\n_posts, _pages # _posts는 일반적인 날짜 기반의 포스팅들이다.\n날짜와 관련 없는 포스팅을 하기 위해서 _pages를 사용한다. 사이트 내 특정 주소에 보여줄 포스팅을 작성할 수 있다.\n_config.yml에서 설정한 _posts, _pages 설정이 markdown 포스팅들의 기본 설정이 된다. markdown 포스팅들에서 YFM을 재정의한다면 재정의한 설정을 지키게 된다.\nCategory, tag # category, tag에 대한 url과 type 설정 가능.\nMenu bar # _data/navigation.yml을 통해 수정 가능.\nCategories, tag # Pages를 통해 categories, tag에 대한 별도의 페이지를 생성해야 한다.\n\u0026lsquo;/categories\u0026rsquo;와 \u0026lsquo;/tag\u0026rsquo;를 permalink로 하여 해당 page들을 만들자. 해당 url 아니어도 무관하다. _config.yml에 설정된 base url이기만 하면 된다.\ncategories, tag에서는 author profile이 기본적으로 false이기 때문에 true로 변경했다.\nCategory # Categories에 블로그의 모든 category가 담겨있다면, category에는 하나의 category에 대한 포스팅들이 존재해야 한다. categories의 하위 url로 permalink 설정해 page를 만들자.\n--- title: \u0026#34;About building blog\u0026#34; permalink: /categories/blog layout: category author_profile: true taxonomy: blog-building --- taxonomy를 통해 표기할 category를 설정한다.\nComments # _config.yml에서 comments를 꼭 true로 해야한다. 이것때문에 엄청 해맸다..\ndefaults: # _posts - scope: path: \u0026#34;\u0026#34; type: posts values: comments: true uterances를 사용했다. github issue를 사용하기 때문에 fork repo라면 반드시 issue 작성이 가능한지 확인해야한다.\npermalink와는 다르게 큰 따옴표로 묶어서 repo이름을 적어야 한다.\nrepository: # GitHub username/repo-name e.g. \u0026#34;mmistakes/minimal-mistakes\u0026#34; comments: provider: \u0026#34;utterances\u0026#34; utterances: theme: \u0026#34;github-light\u0026#34; # \u0026#34;github-dark\u0026#34; issue_term: \u0026#34;pathname\u0026#34; issue_term이 pathname이라면 말 그대로 path name에 의존해서 github issue comment와 mapping이 된다. 즉, 포스팅의 제목이 달라져서 url path가 수정된다면 댓글이 삭제된다.\nFont # scss 파일들을 수정해줘서 font를 바꿀 수 있다.\n# _sass/minimal-mistakes/_variables.scss 에서 User Font 수정 $sans-serif: -apple-system, BlinkMacSystemFont, {User Font} # assets/css/main.scss에 다음 구문 추가 @import url(\u0026#39;https://fonts.googleapis.com/css?family=Noto+Sans+KR\u0026#39;); # _sass/minimal-mistakes/_reset.scss 에서 font size 변경 html { box-sizing: border-box; background-color: $background-color; font-size: 14px; @include breakpoint($medium) { font-size: 14px; } @include breakpoint($large) { font-size: 16px; } @include breakpoint($x-large) { font-size: 18px; } -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; } Date 표시 # _config.yml를 아래와 같이 수정하면 읽은 시간이 사라지고 포스팅 날짜가 표시된다. _config.yml의 아래 항목에 설정을 해두면 전체 posts의 YFM에 해당 내용들이 자동반영된다. ref: Reference github comment\n# Defaults defaults: # _posts - scope: path: \u0026#34;\u0026#34; type: posts values: layout: single author_profile: true read_time: false comments: true share: true related: true show_date: true ","date":"6 April 2022","externalUrl":null,"permalink":"/posts/old_postings/2022-04-06-github.io-basic-setting/","section":"Posts","summary":"","title":"github.io basic","type":"posts"},{"content":" Jekyll Page # 날짜 기반 포스팅 외의 포스팅을 Page로 한다.\nPage의 YFM은 아래와 같다.\n--- title: \u0026#34;About me\u0026#34; permalink: /about/ layout: single --- permalink: page의 base url. 날짜 기반 포스팅이 아니기 때문에 base url이 필요하다. layout: 사전에 jekyll에서 지정된 layout 중 어떤 것을 사용할지 고를 수 있다. _layouts 디렉터리에 여러 포맷들이 있다. page의 layout은 single이 기본 설정이다. 404 page # 이를 활용해서 github.io만의 404 페이지를 만들 수 있다. Github Pages에서 기본적으로 404 페이지를 지원해주니 필수는 아니다.\n--- title: \u0026#34;Page Not Found\u0026#34; excerpt: \u0026#34;Page not found. :(\u0026#34; permalink: /404.html --- Page not Found. :( \u0026lt;script\u0026gt; var GOOG_FIXURL_LANG = \u0026#39;en\u0026#39;; var GOOG_FIXURL_SITE = \u0026#39;https://naem1023.github.io\u0026#39; \u0026lt;/script\u0026gt; \u0026lt;script src=\u0026#34;https://linkhelp.clients.google.com/tbproxy/lh/wm/fixurl.js\u0026#34;\u0026gt; \u0026lt;/script\u0026gt; ","date":"6 April 2022","externalUrl":null,"permalink":"/posts/old_postings/2022-04-06-jekyll_page/","section":"Posts","summary":"","title":"Jekyll Page","type":"posts"},{"content":"Facebook에서 개발한 meta data protocol. HTML의 meta tag를 통해서 meta data를 전달해도 되지만, 직접 tag 수정을 해야하는 번거로움이 있다. Open Graph Protocol에 정의된 표준화된 meta data 표기법을 사용하면 Jekyll에서 알아서 meta data를 블로그에 넣어준다.\nref: Open Graph Protocol blog\nOpen Graph Protocol에 대한 사용여부와 정보를 확인하려면 Facebook에서 제공하는 Sharing Debugger를 사용하면 된다. Cachce에도 TTL이 있는데, TTL이 남아서 cache를 지우고 싶다면 페이스북은 sharing deubugger은 서비스 업체(카카오스토리 등)에서 cache reload 기능을 제공해준다.\n","date":"6 April 2022","externalUrl":null,"permalink":"/posts/infra/2022-04-06-open-graph-protocol/","section":"Posts","summary":"","title":"Open Graph Protocol","type":"posts"},{"content":" Google # Google Search Console # 본인의 블로그를 검색 색인에 등록할 수 있다. 시간이 지나면 Google이 자동으로 내 웹사이트를 찾지만, Google Search Console을 사용하면 능동적으로 색인 생성 요청을 하고 개선할 수 있다.\nGoogle 검색에 사이트가 표시되는 빈도, 사이트를 표시하는 검색어, 검색 사용자가 검색어를 클릭하여 연결하는 빈도 등의 검색 트래픽 정보 확인이 가능하다.\nhttps://search.google.com/search-console\nHTTP page로 인증 URL Inspection(색인 생성 범위)에서 indexing request Sitemaps(사이트맵 제출)에서 sitemap.xml 제출 Google Analytics # 사람들이 나의 웹 사이트를 어떻게 사용하는지 파악할 수 있게 해주는 도구다. Search Console이 Google 검색을 통한 유입 방문자 정보를 확인한다면, Google Analytics는 모든 유입 경로에 대한 방문자 정보 확인이 가능하다.\nGoogle Analytics 계정 생성 property 생성 _config.yml에서 아래의 정보 기입 # Analytics analytics: provider : \u0026#34;google-gtag\u0026#34; # false (default), \u0026#34;google\u0026#34;, \u0026#34;google-universal\u0026#34;, \u0026#34;custom\u0026#34; google: tracking_id : \u0026#34;your tracking id\u0026#34; anonymize_ip : # true, false (default) Naver # https://searchadvisor.naver.com/에 github.io 주소 등록 웹 페이지 수집 요청 sitemap.xml 제출 ","date":"6 April 2022","externalUrl":null,"permalink":"/posts/infra/2022-04-06-search-engine-registration/","section":"Posts","summary":"","title":"Search Engine Registration","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/64065 ref: https://hazung.tistory.com/103\n풀이 # import re def solution(s): answer = [] s = s.split(\u0026#39;},{\u0026#39;) s = [re.sub(\u0026#39;[{}]\u0026#39;, \u0026#39;\u0026#39;, c) for c in s] s = [list(map(int, c.split(\u0026#39;,\u0026#39;))) for c in s] s = list(sorted(s, key = len)) for i in range(len(s)): target = s[i][0] answer.append(target) for j in range(i, len(s)): del s[j][s[j].index(target)] return answer 내가 푼 방식. 머릿 속에 떠오른 방법을 그대로 구현한거라 비효율적이다.\n풀이 2 # def solution(s): answer = [] s = s[2:-2] s = s.split(\u0026#34;},{\u0026#34;) s.sort(key = len) for i in s: ii = i.split(\u0026#39;,\u0026#39;) for j in ii: if int(j) not in answer: answer.append(int(j)) return answer 앞뒤로 규칙적이지 않았던 문자들을 제거해주면 split 결과 자체를 그대로 사용할 수 있다. 굳이 숫자로 변환할 필요가 없으니 문자를 그대로 사용. 나중에만 숫자로 변환 길이가 작은 s의 요소들 부터 가져온다. 따라서 s의 첫번째 요소부터 가져와서 answer에 없다면 추가해주는 것만으로도 문제에서 정의한 \u0026lsquo;튜플\u0026rsquo;의 정의를 충족시킬 수 있다. 풀이 3 # import re def solution(s): answer = [] a = s.split(\u0026#39;,{\u0026#39;) a.sort(key = len) for j in a: numbers = re.findall(\u0026#34;\\d+\u0026#34;, j) for k in numbers: if int(k) not in answer: answer.append(int(k)) return answer 알고리즘은 풀이 2와 같다. 다른 점은 정규표현식을 사용해서 s의 요소별로 리스트를 만드는 것이다. 해당 정규식을 사용하면 하나 이상의 숫자를 찾을 때마다 리스트에 넣어준 후 반환해준다.\n","date":"24 February 2022","externalUrl":null,"permalink":"/posts/algorithms/2022-02-24-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%ED%8A%9C%ED%94%8C/","section":"Posts","summary":"","title":"[프로그래머스] 튜플","type":"posts"},{"content":"","date":"24 February 2022","externalUrl":null,"permalink":"/categories/algorithm/","section":"Categories","summary":"","title":"Algorithm","type":"categories"},{"content":"","date":"24 February 2022","externalUrl":null,"permalink":"/tags/algorithm/","section":"Tags","summary":"","title":"Algorithm","type":"tags"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/77484?language=cpp\nC++ 재활 # find() iterator의 첫번째와 마지막을 parameter로 하여 검색. 못 찾으면 iterator의 마지막을, 찾았으면 값을 반환. #include \u0026lt;algorithm\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;iostream\u0026gt; vector\u0026lt;int\u0026gt; target; int i = 0; auto res = find(target.begind(), target.end(), i); if (res != target.end()) cout \u0026lt;\u0026lt; \u0026#34;find\u0026#34; \u0026lt;\u0026lt; endl; vector method // 복사 dest.assign(source.begin(), source.end()); // 삭제 dest.erase(idx); 풀이 # lottos, win_nums를 비교해 일치하는 개수를 센다. 0의 개수를 센다. 1번을 통해 최저 순위를, 1번과 2번을 더한 값을 통해 최고 순위를 알 수 있다. 코드 # #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;iostream\u0026gt; using namespace std; int get_rank(int correct) { int rank; if (correct \u0026gt;= 2) { rank = 7 - correct; } else { rank = 6; } return rank; } void print_vector(vector\u0026lt;int\u0026gt; target) { for (auto i: target) { cout \u0026lt;\u0026lt; i \u0026lt;\u0026lt; \u0026#39; \u0026#39;; } cout \u0026lt;\u0026lt; endl; } vector\u0026lt;int\u0026gt; solution(vector\u0026lt;int\u0026gt; lottos, vector\u0026lt;int\u0026gt; win_nums) { vector\u0026lt;int\u0026gt; answer; // worst: lottos와 win_nums를 비교 // best: worst + 0의 개수 int zero_cnt = 0, cnt = 0; for (auto\u0026amp; i: lottos) { // count zero if (i == 0) { zero_cnt++; continue; } // check auto res = find(win_nums.begin(), win_nums.end(), i); if (res != win_nums.end()) { cnt++; } } int best = get_rank(cnt + zero_cnt), worst = get_rank(cnt); answer.push_back(best); answer.push_back(worst); return answer; } ","date":"31 January 2022","externalUrl":null,"permalink":"/posts/algorithms/2022-01-31-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%A1%9C%EB%98%90%EC%9D%98-%EC%B5%9C%EA%B3%A0-%EC%88%9C%EC%9C%84%EC%99%80-%EC%B5%9C%EC%A0%80-%EC%88%9C%EC%9C%84/","section":"Posts","summary":"","title":"[프로그래머스] 로또의 최고 순위와 최저 순위","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/92334?language=cpp ref: https://wadekang.tistory.com/6\nC++ 재활 # unordered_map\u0026lt;T, T\u0026gt; map과 동일한 사용 방법 map은 insert, erase시에 정렬이 발생하는데, unorderded_map은 안한다. 따라서 빠름. map의 탐색시간이 O(log n) unordered_map은 O(1), 최악의 경우 O(n) ref: Blog1, Blog2 stringstream str(): 공백과 \u0026lsquo;\\n\u0026rsquo;을 제외하고 문자열에서 적절한 자료형의 정보를 buffer에서 추출 ref: Blog1, HowToUse Ranged-based for loops, auto ref: Auto 타입추론, Ranged-based for for (auto it: v) cout \u0026lt;\u0026lt; it; for (auto\u0026amp; it: v) it *= 2; //포인터 사용 가능 for (const auto\u0026amp; it: v) // 복사 비용이 크다면, const pointer 사용 풀이 # 이용자를 구분할 수 있도록 이용자와 인덱스를 mapping \u0026ldquo;key: 신고당한 사람, value: 신고한 사람의 집합\u0026quot;으로 mapping 2번의 정보를 활용해 k명 이상이 신고했다면, 1번의 인덱스를 기준으로 정지당한 이용자의 정보를 업데이트 코드 # #include \u0026lt;string\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;unordered_map\u0026gt; #include \u0026lt;set\u0026gt; #include \u0026lt;sstream\u0026gt; using namespace std; vector\u0026lt;int\u0026gt; solution(vector\u0026lt;string\u0026gt; id_list, vector\u0026lt;string\u0026gt; report, int k) { vector\u0026lt;int\u0026gt; answer(id_list.size(), 0); unordered_map\u0026lt;string, int\u0026gt; idx_map; // 이용자에게 index 부여 for (int i = 0; i \u0026lt; id_list.size(); i++) idx_map[id_list[i]] = i; unordered_map\u0026lt;string, set\u0026lt;string\u0026gt;\u0026gt; report_map; // 신고당한 이용자를 기준으로 신고한 사용자 집합 저장 stringstream ss; for (auto rep: report) { ss.str(rep); string first, second; ss \u0026gt;\u0026gt; first \u0026gt;\u0026gt; second; // stringstream으로 stream output을 진행하면서 공백 분리 report_map[second].insert(first); // second를 신고한 사람인 first를 second의 set에 저장 ss.clear(); } for (auto it: report_map) { if (it.second.size() \u0026gt;= k) { // 신고한 사람이 k명 이상인 이용자에 대해서 for (auto set_it: it.second) { // 신고한 사람들을 인덱스 기준으로 찾아서, 정지당했다고 알려준다. int idx = idx_map[set_it]; answer[idx]++; } } } return answer; } ","date":"30 January 2022","externalUrl":null,"permalink":"/posts/algorithms/2022-01-30-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%8B%A0%EA%B3%A0-%EA%B2%B0%EA%B3%BC-%EB%B0%9B%EA%B8%B0/","section":"Posts","summary":"","title":"[프로그래머스] 신고 결과 받기","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/42587\n풀이 # 몇 번째에 인쇄가 발생하고 발생하는 인쇄 건수마다 count를 증가시켜야 함을 명확히 이해해야지 풀 수 있다.\n코드1 # 내가 푼 방법이다. 조건분기점은 문제를 코드 형태로 바꾸기만 했다. 주의할 점은 solution의 가장 마지막 줄에서는 count + 1을 return해야 하는 것이다. 마지막 줄까지 간 것은 priorities의 모든 item들이 pop됐기 때문에 count가 아직 증가되지 않은 상태이기 때문이다.\ndef solution(priorities, location): count = 0 while priorities: front = priorities.pop(0) if priorities: if front \u0026lt; max(priorities): priorities.append(front) if location == 0: location = len(priorities) - 1 else: location -= 1 else: count += 1 if location == 0: return count else: location -= 1 return count + 1 코드2 # ref: https://programmers.co.kr/learn/courses/30/lessons/42587/solution_groups?language=python3\nany: iteration 내에 하나라도 True가 있다면 True. 아니라면 False.\n나는 location의 위치를 추적해줬지만, 이 답안은 미리 원본일 때의 위치를 기억하기 위해 튜플을 활용했다. 덕분에 코드가 매우 간결해졌다.\n또한 any를 사용해서 중요도를 비교했다. any를 쓰는것보다 max를 쓰는 것이 더 깔끔해보인다. 다만단순 max, min 값이 아니라 복잡한 조건문을 사용해야 할 때는 any가 코드를 매우 간결하게 해준다.\ndef solution(priorities, location): queue = [(i,p) for i,p in enumerate(priorities)] answer = 0 while True: cur = queue.pop(0) if any(cur[1] \u0026lt; q[1] for q in queue): queue.append(cur) else: answer += 1 if cur[0] == location: return answer ","date":"20 January 2022","externalUrl":null,"permalink":"/posts/algorithms/2022-01-20-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%ED%94%84%EB%A6%B0%ED%84%B0/","section":"Posts","summary":"","title":"[프로그래머스] 프린터","type":"posts"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/categories/automl/","section":"Categories","summary":"","title":"AutoML","type":"categories"},{"content":" Data Engineering # Data Cleansing, Preprocessing Feature Engineering Select ML Algorithm DL: Select Backbone Model Set Hyperparameters DL: Loss, Optimzier, Learning rate, batch size model architecture와 hyperparameter 선정은 train/evaluate에 대한 피드백을 받고 재선정하게 된다. 이러한 과정을 사람이 직접 수행하는게 일반적이다. 이러한 일련의 과정에서 사람을 제거하고 자동화하는 것이 AutoML의 목표이다.\nDefinition # AutoML의 목표를 설명하면서 서술했던 말을 수식으로 명시화한 것이다. hyperparameter, ML algorithm, data들이 주어져있을 때 loss를 minimize하는 hyperparmeter configuration을 찾는 것이 HPO(Hyperparameter Optimization = AutoML)의 목표이다.\n경량화의 다른 관점 # 기존 모델을 경량화 Pruning, Tensor decomposition Searching을 통해 경량 모델을 찾는 기법 NAS(Neural Architecture Search), AutoML AutoML은 경량모델을 찾는 기법이다!\nDL Model Configuration # Type # Categorical Optimizer: Adam, SGD, AdamW \u0026hellip; Module: Conv, BottleNeck, InvertedResidual Continuous learning rate, regularizer param, \u0026hellip; Integert Batch size, epochs Conditional configuration # configuration에 따라서 search spaec가 달라진다.\nOptimizer에 따라서 optimizer parameter의 종류와 search space가 달라진다. Module sample(Vanilla conv, BottleNeck, InvertedResidual)에 따라서 모듈별 parameters와 search space가 달라진다. AutoML Pipeline # 앞서 서술했던 HPO의 정의와 유사하다. 추가된 점은 Evaluate Objective function인 $f$이다. $f$의 정의는 다양할 수 있다. 모델의 사이즈만이 작아지는 것을 원할 수도 있고, 모델의 성능만이 향상되는 것을 원할 수도 있고 여러 목표치가 혼합된 형태일 수도 있다.\n이러한 목표들을 maximize하도록 Blackbox optimization을 진행해서 새로운 configuration $\\lambda$를 찾는다.\nBayesian Optimization(BO) # Blackbox 형태의 optimization을 위와 같이 구성한 것이다.\nSurrogate function: $f(\\lambda)$를 예측하는 regression model. 정확히 예측이 가능해진다면 다음에 시도할 $\\lambda$ 더 잘 결정할 수 있을 것이다. Acquisition function: 다음에 시도할 $\\lambda$를 결정해준다. 도식도의 과정을 순서대로 나열하면 아래와 같다.\n$\\lambda$(x)를 sample(observation) 해당 configuration으로 DL 모델 학습 objective 계산. 위 그림에서의 observation(x)가 이것에 해당한다. Surrogate model update. 위 그림에서 실선과 보라색 영역으로 표현되는 것들이다. e.g., GP(Gaussian Process) model, posterior mean, jposterior varicance(uncertainty) Acquisition function update. 위 그림에서 초록색으로 표현되는 영역이다. surrogate model의 추세를 보고 가장 좋은 다음 $\\lambda$를 예측한다. BO with GPR # Gaussian Process Regression # 불확실성(uncertainty)을 모델링할 수 있는 방법이다. BO에서 사용한 그래프에서 Surrogate model의 두 지점만을 알고 있고 이외의 지점에 대해서는 불확실하다. 이 때, GP를 사용해서 값을 알고 있는 두 지점 외의 값들에 대해서, 범위를 얻을 수 있다.\n일반적인 Regression taks Set of train data: $(X,Y)$ Set of test data: $(X_,Y_)$ $Y\\approx f(X) + e$\nGP의 아이디어\n알고자 하는 특정 위치의 $Y_$ 값은 이미 알고 있는 $X,Y,X_$와 연관이 있지 않을까? positive, negative 관계 무관 $X,Y,X_$값으로부터 $Y_$를 추정하는 표현을 Kernel 함수 $K$로 표현해보자! GP의 엄밀하지 않은 정의\n$f(x)$: input x에 대한 Random variable로 정의 = input x에 대해서 가능한 함수들의 분포 Random variable의 distribution: Multivariate Gaussian distribution 정의만 서술하면 위와 같이 되고, $f(x)$에 대한 정의를 GP에서 어떻게 생각했는지 풀어쓰면 아래와 같다.\n함수들의 분포를 정의. 이 분포가 Multivariate Gaussian distribution을 따른다고 가정. = 함수 $f$가 Gaussian process를 따른다. 지금 설명된 내용을 수식으로 정리하면 위 수식이 된다.\n해당 수식에서는 Gaussian Identities가 적용된다고 한다. Gaussian의 margianl과 conditional도 Gaussian을 따른다는 정의다.\n이를 그림으로 위 처럼 그림으로 생각할 수 있다. conditional의 어떤 쪽에서 본래 Gaussian을 바라본다고 해도, 해당 conditional 또한 Gaussian을 따른다.\n수식으로 설명한 GP를 통해 알 수 있는 사실은 $X_, X, f$가 주어졌을 때 $f_$의 평균과 분포를 알 수 있다는 것이다.\nSurrogate Model # 앞서 정리한 내용들을 활용해 Surrogate model에 대한 보다 자세한 정리를 해보자.\ndef: Objective $f(\\lambda)$를 예측하는 모델 관측된 $f(\\lambda)$를 활용해, 새로운 $\\lambda_$에 대한 objective $f(\\lambda_)$f를 예측 Surrogate model을 학습해서, 다음 step의 좋은 $\\lambda$를 선택하는 기준으로 사용 대표적 Surrogate model GPR(Gaussian Process Regression) model mean: 예측 $f$값, var: uncertainty Observation data가 늘어날수록 uncertainty가 줄어들면서 true function에 prediction이 fitting된다.\nAcquisition Function # def: Surrogate model의 ouput을 활용해 다음에 시도하면 좋은 $\\lambda$를 결정하는 함수 Exploration, Exploitation 사이의 적절히 balancing 할 수 있도록 수식이 구성됐다. 둘 사이의 분배는 heuristic하게 결정한다. Exploration: 불확실한 지점을 탐색 Exploitation: 알고 있는 가장 좋은 곳을 탐색 갱신된 Acquistion function의 max 지점을 다음 iteration에서 시도 위쪽 그래프는 surrogate model이고 아래 그래프가 acquisition function 그래프이다. acquisition function의 값들이 특정 지점에서 굉장히 작아지고 해당 값 부근에서는 값이 커진다. Exploitation 관점에서 이미 알고 있는 지점은 탐색할 필요가 없고 이미 알고 있는 값 부근이 가장 좋은 탐색 포인트이기 때문이다. 이러한 방식으로 Acquisition function이 구성된다.\ne.g., Upper Confidence Bound(UCB) $\\mu$: posterior mena(=Exploitation) $\\sigma$: posterior variance(=Exploration) $\\kappa$: balancing parameter BO with TPE # GP의 문제점\nComplexity: $O(N^3)$ Conditional, continuous/discrete parameter들이 혼합될 때 적용이 어려움 대부분 두번째에서 어려움이 많이 발생하고 요즘은 TPE를 많이 사용한다고 한다. TPE(Tree-structured Parzen Estimator)와 GPR의 차이\nGPR: $p(f|\\lambda)$(posterior distribution)을 계산 TPE: $p(\\lambda|f)$(likelihood), $p(\\lambda)$(prior)를 계산 ","date":"28 November 2021","externalUrl":null,"permalink":"/posts/ml/2021-11-28-automl/","section":"Posts","summary":"","title":"AutoML","type":"posts"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/tags/ligthweight/","section":"Tags","summary":"","title":"Ligthweight","type":"tags"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/categories/ml-lightweight/","section":"Categories","summary":"","title":"ML-Lightweight","type":"categories"},{"content":" 경량화의 관점 # 모델 크기(=파라미터 수)를 줄이자 속도를 빠르게만 해보자 연산횟수를 작게 바꿔보자 FLOPs # 연산횟수를 나타내는 factor 연산속도를 결정하는 간접적인 factor ShuffleNetv2라는 논문에서 FLOPs외에 속도에 영향을 끼치는 요소를 고려하여 가이드라인을 다음과 같이 제시했다.\n입출력의 크기가 동일할 때 memory access cost 큰 Group convolution은 memory cost를 증가 여러 분기의 path가 나오는 구조. 즉, 모델이 병렬적으로 구성되면 속도 저하 발생. Element-wise operation은 무시하지 못할 비중을 가지고 있으니 주의하자. ","date":"28 November 2021","externalUrl":null,"permalink":"/posts/ml/2021-11-28-%EA%B2%BD%EB%9F%89%ED%99%94-%EB%8C%80%ED%9A%8C/","section":"Posts","summary":"","title":"경량화 대회","type":"posts"},{"content":"","date":"28 November 2021","externalUrl":null,"permalink":"/tags/lightweight/","section":"Tags","summary":"","title":"Lightweight","type":"tags"},{"content":" Goal # On device AI # 사용자 개인 기기에 탑재되는 모델들이 가지는 제한사항을 극복 power usage RAM Storage Computing power AI on cloud # 많은 사용자가 사용해야 하기 때문에 latency, throughput이 중요한 이슈 e.g., 요청당 소요시간, 단위 시간당 처리 가능한 요청 수 동일 자원으로 더 적은 latency, 더 큰 throughput을 구현/구축해야한다. Computation # ![](/assets/images/경량화 overview/cbb682dc-946e-40d1-848a-97f392602caa-image.png)\n모델 자체의 연산 수행 횟수를 줄여야한다. 2012년 이후로 모델 학습에 소요되는 연산은 3, 4개월마다 두 배씩 증가 Efficient Architecture Design # ![](/assets/images/경량화 overview/29953b37-5063-42ad-aabd-88d69139ab02-image.png) 출시된 CNN 모델들의 파라미터 수와 성능에 대한 도표다. 위 도표에서 출시된 모델들은 파라미터수를 효율적으로 줄이면서 성능을 높이려고 했다. 이러한 것들의 대표적인 Efficient Architecture Design이다. 즉, 모델 자체를 효율적으로 설계하고자 하는 것이다.\nAutoML;Neural Architecture Search(NAS) # 알아두면 써먹을 곳이 많은 유용한 기법! 사람이 아니라 알고리즘을 통해 효율적인 모델을 설계하거나 찾아보자. ![](/assets/images/경량화 overview/f2092147-94ed-4f74-a111-f7f7f5adac13-image.png)\ncontroller는 모델 아키텍쳐를 제시하는 모델이다. controller에서 제시된 모델로 accuracy를 구해보고 이 수치를 활용해 controller를 재학습한다. 이러한 과정을 반복해서 효율적인 모델을 찾아볼 수 있을 것이다.\n![](/assets/images/경량화 overview/f22cbdf0-df1c-44d2-9b86-7a17e44f8f85-image.png)\nAutoML, NAS를 통해 얻은 모델은 사람의 직관으로 이해할 수 없는 모델일 가능성이 높다. 그럼에도 이러한 모델의 성능이 기존 모델보다 높을 수도 있다.\nNetwork Pruning # 중요도가 낮은 모델의 파라미터를 제거 topic: 좋은 중요도를 정의, 찾기 e.g., 임의의 파라미터의 L2 norm이나 loss gradient를 계산해서 중요도를 계산한다. structured/unstructeured pruning으로 나뉜다. Structured pruning # 파라미터를 그룹 단위로 pruning하는 기법들을 총칭 그룹: channel, filter, layer 등 한꺼번에 pruning하기 때문에 Dense computation에 최적화된 SW/HW에 적합한 기법 ![](/assets/images/경량화 overview/08644e1d-2c18-4a68-9d84-3428b4349fb8-image.png) 기존 네트워크에 대해서 channel별로 그룹을 지어서 layer별 factor를 계산한다. 낮은 중요도를 가지는 layer를 지워버리면 경량화된 모델을 만들 수 있다. Unstructured prunig # 파라미터 각각을 독립적으로 pruning 개별적으로 적용되기 때문에 pruning을 수행할수록 네트워크 내부 행렬이 희소(sparse)해진다. sparse computation에 최적화된 SW/HW에 적합 Knowledge Distillation # Pre-trained된 큰 모델을 작은 네트워크의 학습 보조로 사용 ![](/assets/images/경량화 overview/6625704e-7465-4577-a5e4-e1bd03202a18-image.png) student loss 부분은 기존의 네트워크 학습 방식과 동일하다. ground truth와 prediction 결과를 활용해 loss를 구한다. distillation loss 부분이 knowledge distillation이 발생하는 부분이다. ground truth가 아니라 teacher model을 통해 얻은 soft label을 활용해서 prediction과의 loss를 구한다.\n![](/assets/images/경량화 overview/bf88e088-4e28-42f6-94af-23570be67091-image.png) soft targets(soft outputs)에는 ground truth보다 더 많은 정보를 담고 있다. 위 그림은 row별로 예측된 label을 표시할 때 probability를 색깔로 표현한 것이다. ground truth와 다르게 classification의 예측값 1개만을 사용하지 않고 모든 label에 대한 probability를 활용할 수 있게 된 것이다. 따라서 더 많은 정보를 활용해 작은 네트워크를 학습 가능하다.\n수식 # ![](/assets/images/경량화 overview/ac84ee51-83e6-496a-a155-a70ad5a9ee2e-image.png)\n왼쪽 항: student network, ground truth의 cross-entropy 오른쪽 항: teacher network와 student network의 KLD loss $T$: temperature hyperparameter. softmax의 결과가 작다면 크게, 크다면 작게 만들어준다.ref $\\alpha$: 두 loss에 대한 가중치 Matrix/Tensor Decompostion # 수학적으로 복잡하지만 간단한 기법만 적용하더라도 유효한 성능을 낼 수 있는 기법\n하나의 tensor를 작은 tensor들의 합/곱으로 표현 Cp decomposition # rank 1 vector들의 output product의 합으로 tensor를 approximation.\nNetwork Quantization # fp32를 fp16, int8로 mapping. ![](/assets/images/경량화 overview/0b944f17-cfba-48b3-a945-c051fd5ab4b3-image.png)\nQuantization을 적용 후, 연산 결과에 대해서 Dequantization을 수행하면 fp32의 결과와 비교해서 error가 발생할 것이다. 하지만 경험적으로 이러한 error에 대해서 모델이 robust하게 잘 동작한다고 알려져 있다.\nmodel size: 감소 acc: 약간 하락 time: HW에 따라서 다르다. 전반적으로 HW와 무관하게 향상되는 추세 e.g., 특정 HW에서는 int8 quantization이 더 느릴수도 있다. Network compiling # target system이 고정됐을 때, 효율적인 연산을 하도록 네트워크 자체를 컴파일. 속도에 가장 큰 영향을 미치는 기법 TensorRT(NVIDIA), Tflite(Tensorflow), TVM(apache) compile library, HW system, model 조합별로 성능이 상이하다. ![](/assets/images/경량화 overview/38f1e42c-a732-4458-b684-016e12199162-image.png) rule-based로 compiling을 수행하면 위와 같이, 정의된 rule에 따라서 graph를 간소화시킨다. 간소화시킨 결과물을 fusion이라고 부른다.\n![](/assets/images/경량화 overview/b9c15471-01c3-4392-b670-6e8f8c1b30be-image.png)\nAutoML로 graph의 좋은 fusion을 만들고자 하는 시도도 있다. framework, HW, system을 모두 고려하면 굉장히 많은 조합이 발생할 수 있다. 따라서 target system에 대해 최적화된 fusion을 찾기위해 AutoML을 사용하는 것이다. Apahce에서 출시한 AutoTVM이 그러한 역할을 한다고 한다.\n","date":"28 November 2021","externalUrl":null,"permalink":"/posts/ml/2021-11-28-%EA%B2%BD%EB%9F%89%ED%99%94-overview/","section":"Posts","summary":"","title":"경량화 overview","type":"posts"},{"content":"https://www.hackerrank.com/challenges/torque-and-development/problem\n풀이 # cost를 최소화하면서 모든 도시의 시민들이 도서관을 이용할 수 있게 하는 문제이다. 도서관이 있는 도시에 거주하거나 도서관이 있는 도시와 연결되어 있다면 해당 도시의 시민들이 도서관을 이용할 수 있다.\ncost의 정의는 아래와 같다. $c_lib=cost\\ of\\ libarary$ $c_road=cost\\ of\\ road$\n도서관과 도로를 짓는 방법은 두 가지로 나눌 수 있다.\n모든 도시에 도서관을 짓는다. 모든 도시가 연결될 수 있도록 도로를 만든다. 1번 방법 # c_lib가 c_road보다 작다면 1번 방법이 최선의 방법이다.\n2번 방법 # c_lib가 c_road보다 큰 상식적인 상황이다. 이 문제는 도시와 도시 사이의 도로를 맘대로 연결할 수 없다. 문제에서 주어진 relation만을 활용해서 도로를 지을 수 있다.\n따라서 2번 방법을 수행하되 연결할 수 없는 도시 그룹들은 해당 도시 그룹만의 도서관이 필요하다.\n문제에서 주어진 relation을 이미 존재하는 그래프라고 간주하고 dfs를 수행한다. dfs를 수행하면서 통과하는 edge의 수를 센다. 1번에서 얻은 edge의 수를 $total_path$라고 하자. $total_path - num_of_city$를 통해 도시 그룹의 수를 구한다. 최종적인 cost는 아래와 같이 구할 수 있다. $total_path \\times c_road+(total_path - num_of_city)\\times c_city$\n코드 # https://github.com/naem1023/codingTest/blob/master/graph/hackerrank-roads-and-libraries.py\n","date":"19 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-19-hakerrank-roads-and-libraries/","section":"Posts","summary":"","title":"[HakerRank] Roads and Libraries","type":"posts"},{"content":"https://www.hackerrank.com/challenges/sherlock-and-cost/problem\n풀이 # ref: Blog\n![](/assets/images/[HackerRank] Sherlock and Cost/49a6baec-e8f0-4f53-b8a1-7318512ddec5-image.png)\n위 수식을 최대화하는 $$A$$를 구하면 된다. diff의 차이를 극대화하는 것이기 때문에 A[i]는 B[i]나 1이 되면 된다. 왜냐하면 $1 \u0026lt;= A[i] \u0026lt;= B[i]$이기 때문이다.\nbfs, dfs를 통해 A[i]의 모든 가능성을 brute-force하게 검색할 수도 있지만 $n = 10^5$이기 때문에 time limit에 걸린다. Dynamic programming을 통해 S[i]를 계속 갱신하면서 최대 S[i]를 찾는다.\ncost함수에서는 가장 큰 S의 요소를 반환하면 된다.\nS 갱신 # A[i]가 선택할 수 있는 값의 경우는 다음과 같다.\n1 B[i] 이러한 경우의 수는 A[i - 1] 또한 마찬가지이다.\nS[i]를 일반화하면 다음과 같다. $S[i] = S[i - 1] + A[i]$\n이 때, A[i]의 경우의 수가 2가지이므로 A[i - 1]의 경우의 수도 2가지이다. 따라서 A[i]가 고정된 경우, S[i]에 대한 경우의 수는 2가지이다. S[i]에서 선택할 수 있는 경우의 수는 다음과 같다.\n$A[i] = 1$, $S[i][0]$ $A[i - 1] = 1, S[i - 1][0] + (1 - 1)$ $A[i - 1] = B[i - 1], S[i - 1][1] + abs(B[i - 1] - 1)$ $A[i] = B[i]$, $S[i][1]$ $A[i - 1] = 1, S[i - 1][0] + abs(B[i] - 1)$ $A[i - 1] = B[i - 1], S[i - 1][1] + abs(B[i] - B[i - 1])$ 위와 같은 점화식을 구성하면 S[i]에는 순차적으로 이전 정보들의 누적되면서 새로운 S[i]가 갱신된다.\n코드 # https://github.com/naem1023/codingTest/blob/master/dp/hackerrank-sherlock-and-cost.py\n","date":"19 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-19-hackerrank-sherlock-and-cost/","section":"Posts","summary":"","title":"[HackerRank] Sherlock and Cost","type":"posts"},{"content":"https://www.hackerrank.com/challenges/climbing-the-leaderboard/problem\n풀이 # ref: https://inspirit941.tistory.com/199 python list의 sort로 해결하려하니까 time limit에 결렸다. bs로 검색해도 마찬가지였다.\ndef climbingLeaderboard(ranked, player): # Write your code here result = [] from collections import defaultdict rank_dict = defaultdict(int) for r in ranked: rank_dict[r] += 1 for p in player: score = list(rank_dict.keys()) score.append(p) score.sort(reverse=True) for idx, s in enumerate(score): if s == p: break result.append(idx + 1) return result time limit 해결을 위해서 p를 찾을 때마다 검색을 시도하면 안됐다. player는 descending order이고 ranked는 ascending order인 점을 활용해서 검색하지 않아도 되는 영역을 지나쳐야 하는 것이 핵심이다.\ndef climbingLeaderboard(ranked, player): queue = sorted(set(ranked), reverse=True) idx = len(queue) - 1 result = [] for p in player: while queue[idx] \u0026lt;= p and idx \u0026gt;= 0: idx -= 1 if idx \u0026lt; 0: result.append(1) continue result.append(idx + 2) return result plyaer와 ranked의 정렬순서 때문에 player의 앞쪽에 위치한 값들은 ranked의 뒤쪽에 나올 것이다. 따라서, player에 대한 iteration을 돌 때 ranked의 뒤쪽부터 체크한다. 또한 한번 지나친 queue의 index에 대해서는 다시 검색할 필요가 없어진다. 이 방법을 사용하면 ranked의 길이가 아무리 커져도 ranked를 한번만 순회하면 된다.\n음수 index # idx가 음수가 되는 경우가 존재할 수 있다. 이러한 경우는 ranked의 모든 요소들에 대해서 순회했을 때 발생할 것이다. 따라서 p는 1등일 것이므로 result에 1을 append 해준다.\n코드 # https://github.com/naem1023/codingTest/blob/master/implementation/hackerrank-climbing-the-leaderboard.py\n","date":"18 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-18-hackerrank-climbing-the-leaderboard/","section":"Posts","summary":"","title":"[HackerRank] Climbing the Leaderboard","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/60057\n풀이 # N-gram Language Model처럼 문자열에 접근하면 된다.\ndef get_len(n): for i in range(cut, length of string, cut): 문자열 체크, 카운팅 마지막 카운팅 처리 for i in range(0, len(s) // 2 + 1): get_len(i) 문자열 체크, 카운팅\ns[i : i + cut]이 기존 임시 저장소와 비교 같다면 카운팅 다르면 출력 결과 갱신 pseudo code대로 처리하면 마지막 문자 조합에 대해서는 for문에서 처리 못한다. for 문 밖에서 임의의로 마지막 문자열 조합에 대한 카운팅 처리를 해주자.\n코드 # https://github.com/naem1023/codingTest/blob/master/implementation/pg-60057.py\n","date":"17 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-17-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%AC%B8%EC%9E%90%EC%97%B4-%EC%95%95%EC%B6%95/","section":"Posts","summary":"","title":"[프로그래머스] 문자열 압축","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/49189\n풀이 # ref: Blog 그래프 탐색 문제이기 때문에 dfs, bfs에서 편한걸 선택하면 된다. bfs를 사용해서 풀었다.\nedge 관계만이 주어졌기 때문에 임의의 node와 인접한 node들을 저장하는 새로운 graph dictionary를 만든다. graph dictionary에서 1번 node를 시작점으로 하여 bfs 수행. Undirected graph이고 edge들의 distance가 없다. 따라서 bfs를 수행하면서 최초로 만난 node들의 거리값을 갱신해주면 1번 노드로부터의 최소 거리를 알 수 있다. 코드 # https://github.com/naem1023/codingTest/blob/master/graph/pg-49189.py\n","date":"12 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-12-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EA%B0%80%EC%9E%A5-%EB%A8%BC-%EB%85%B8%EB%93%9C/","section":"Posts","summary":"","title":"[프로그래머스] 가장 먼 노드","type":"posts"},{"content":"https://www.acmicpc.net/problem/1074\n풀이1 # 최소 단위 배열을 만들어서 풀려고 했는데 너무 어려웠다. 구현 문제로 접근하면 안된다. 이전에 방문됐다고 간주되는 요소들을 한꺼번에 더하는 아이디어를 활용해 재귀로 풀어야 한다.\n| 1 | 2 | | 3 | 4 |\n배열 영역을 위와 같은 순서로 분리해서 본다.\n영역을 구분하는 기준 좌표: 좌측 상단 size: 현재 탐색하고자 하는 영역의 한변의 길이 이전에 방문됐다고 간주되는 요소들의 개수: size*size 배열의 요소들이 순서대로 정렬됐기 때문에 개수들을 누적시켜주면서 counting한다. 누적 요소 개수 counting 여부\n현재 탐색하고자 하는 영역에 r, c가 있다면 영역을 4분할해서 다시 탐색 r, c가 없다면 영역 내에 존재하는 요소들의 개수를 counting해서 누적 counting 여부를 재귀로 구현해서 풀었다.\nimport sys input = sys.stdin.readline n, r, c = list(map(int, input().split())) ans = 0 def Z(y, x, size): \u0026#34;\u0026#34;\u0026#34; 영역을 4분할하는 단위 함수. 영역의 기준점은 해당 영역의 가장 왼쪽 위 구석. y: r x: c \u0026#34;\u0026#34;\u0026#34; global ans # 검색 영역이 정확하게 (r, c)와 일치한다면 반환 if y == r and x == c: print(ans) return # r, c가 현재 사분면에 존재한다면 if r \u0026lt; y + size and r \u0026gt;= y and c \u0026lt; x + size and c \u0026gt;= x: # 1사분면 탐색 Z(y, x, size // 2) # 2사분면 탐색 Z(y, x + size // 2, size // 2) # 3사분면 탐색 Z(y + size // 2, x, size // 2) # 4사분면 탐색 Z(y + size // 2, x + size // 2, size // 2) # 지나왔다고 간주되는 현재 영역들을 모두 ans에 누적시킨다. else: ans += size * size # (0, 0), size=N^2부터 시작해서 검색 영역을 좁혀간다. Z(0, 0, (1 \u0026lt;\u0026lt; n)) 풀이2 # 누적되는 요소들의 개수를 누적시킨다는 아이디어는 같다. 다른 점은 n을 감소시켜면서 (r, c)를 찾기 때문에 재귀보다 좀 더 빠르다.\nimport sys input = sys.stdin.readline n, r, c = map(int, input().split()) cnt = 0 while n \u0026gt; 0: n -= 1 # 첫번째 if r \u0026lt; 2**n and c \u0026lt; 2**n: continue elif r \u0026lt; 2**n and c \u0026gt;= 2**n: cnt += (2**n)*(2**n)*1 c -= 2**n elif r \u0026gt;= 2**n and c \u0026lt; 2**n: cnt += (2**n)*(2**n)*2 r -= 2**n else: cnt += (2**n)*(2**n)*3 r -= 2**n c -= 2**n print(cnt) ","date":"11 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-11-%EB%B0%B1%EC%A4%80-z/","section":"Posts","summary":"","title":"[백준] Z","type":"posts"},{"content":"","date":"10 November 2021","externalUrl":null,"permalink":"/categories/competition/","section":"Categories","summary":"","title":"Competition","type":"categories"},{"content":"","date":"10 November 2021","externalUrl":null,"permalink":"/tags/kaggle/","section":"Tags","summary":"","title":"Kaggle","type":"tags"},{"content":"kaggle notebook을 안쓰고 개인 서버에서 train 해보려면 kaggle dataset을 전부 서버에 받아야한다. 대회에서 제공해주는 train/test 파일들만 쓴다면 kaggle api를 쓸 필요까지 없다.\n하지만 discussion에 올라온 여러 code들을 돌려보려면 정말 많은 dataset들을 받아야한다 귀찮고 시간도 오래 걸린다. kaggle api로 한꺼번에 받는 쉘 스크립트를 만들어서 쓰니 편했다.\nkaggle datasets download -d kishalmandal/extra-data kaggle competitions download -c chaii-hindi-and-tamil-question-answering kaggle datasets download -d kishalmandal/cleaned-data-for-chaii kaggle datasets download -d kishalmandal/input kaggle datasets download -d msafi04/squad-translated-to-tamil-for-chaii files=(\u0026#34;extra-data\u0026#34; \u0026#34;cleaned-data-for-chaii\u0026#34; \u0026#34;input\u0026#34; \u0026#34;squad-translated-to-tamil-for-chaii\u0026#34; \u0026#34;chaii-hindi-and-tamil-question-answering\u0026#34;) for i in \u0026#34;${files[@]}\u0026#34;; do unzip $i\u0026#34;.zip\u0026#34; -d \u0026#34;$i;done ","date":"10 November 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-11-10-kaggle-dataset/","section":"Posts","summary":"","title":"Kaggle dataset","type":"posts"},{"content":"https://www.acmicpc.net/problem/2026\n풀이 # ref: https://westmino.tistory.com/84\ndfs, bfs로 친구 관계 그래프를 탐색한다고 생각했다. 실제로 그래프는 아니지만 입력으로 주어진 친구 관계를 활용해서 dfs, bfs를 그래프에서 적용하는 것과 같이 구현할 수 있다고 한다.\nadj_mat: Adjacency matirx로 친구 관계 여부를 저장한다. adj_list: i번째 배열에 i의 친구들이 담기도록 배열을 준비한다. DFS # 친구 관계인 사람들의 정의는 \u0026lsquo;모두 각자의 친구이어야한다.\u0026lsquo;이다. 건너건너 친구이면 친구로 성립되지 않는다.\n임의의 지점을 시작점으로 설정 path: 함께 갈 수 있는 사람들이 저장될 배열 시작점과 친구인 사람들을 adj_list에서 가져온다. adj_mat을 사용해 2번 사람들이 path에 들어갈 수 있는 체크한다. path의 길이가 K가 되면 path를 반환 모든 탐색을 끝내도 path의 길이가 K가 안되면 None을 반환 DFS 결과 처리 # DFS를 통해 None 혹은 path를 얻을 수 있다.\nNone: 다른 사람을 DFS의 시작점으로 설정해서 DFS 재수행 path: path 정렬 후 반환 코드 # https://github.com/naem1023/codingTest/blob/master/graph/acmicpc-2026.py\n","date":"10 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-10-%EB%B0%B1%EC%A4%80-%EC%86%8C%ED%92%8D/","section":"Posts","summary":"","title":"[백준] 소풍","type":"posts"},{"content":"https://www.acmicpc.net/problem/1759\n풀이 # combination과 조건 체크로 풀 수 있다.\n암호는 정렬된 알파벳에서 순서대로 추출해야한다. combination 사용 최소 한 개의 모음, 최소 두 개의 자음 모음 리스트 구성 후 조건 체크 코드 # import sys L, C = list(map(int, sys.stdin.readline().split())) char_list = sys.stdin.readline().split() from itertools import combinations char_list.sort() answer = list(combinations(char_list, L)) answer = list(map(lambda x: \u0026#39;\u0026#39;.join(x), answer)) m = [\u0026#39;a\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;i\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;u\u0026#39;] for a in answer: m_count = 0 j_count = 0 for c in a: if c in m: m_count += 1 else: j_count += 1 if m_count \u0026gt;= 1 and j_count \u0026gt;= 2: print(a) ","date":"8 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-08-%EB%B0%B1%EC%A4%80-%EC%95%94%ED%98%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0/","section":"Posts","summary":"","title":"[백준] 암호 만들기","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/17686\n풀이1 # 숫자를 기준으로 파일명을 split해야 한다. 이 때 다음과 같이 직접 구현해도 무방하다.\nnumber_list = [str(i) for i in range(10)] for idx in range(len(files)): head, number, tail = \u0026#34;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;\u0026#34; number_idx, tail_idx = -1, -1 # number start index 찾기 for j in range(len(files[idx])): if files[idx][j] in number_list: head = files[idx][:j] # print(head) number_idx = j break # tail start index 찾기 for j in range(number_idx, len(files[idx])): if files[idx][j] not in number_list: number = int(files[idx][number_idx:j]) break tail = files[idx][j:] files[idx] = [head, number, tail] 구현하다가 regex가 떠올랐다. \\d로 숫자만 구분할 수 있어서 다음과 같이 구상해볼 수 있다.\nre.compile(r\u0026#39;(\\d+)\u0026#39;) 코드에서는 split만 하면 되기 때문에 re.split을 사용했다.\n정렬은 우선순위를 sort 함수에 넘겨주면 알아서 계산해준다. head와 tail에 대해서만 정렬을 수행하면 되기 때문에 sort 함수의 key에 다음의 람다 함수를 넘겨준다.\nlambda x: (x[0].lower(), int(x[1])) 풀이2 # java의 Comparable inreface를 사용해서 비교할 수도 있다고 한다. python에서는 __cmp__를 통해서 구현할 수 있는데 이 문제에서는 사용할 이유가 없을 것 같다. 좀 더 복잡한 형식의 정렬 알고리즘이 요구된다면 사용할 법하다.\npython _cmp_ 풀이2 코드 코드 # https://github.com/naem1023/codingTest/blob/master/sort/pg-30-17686.py\n","date":"2 November 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-11-02-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%ED%8C%8C%EC%9D%BC%EB%AA%85-%EC%A0%95%EB%A0%AC/","section":"Posts","summary":"","title":"[프로그래머스] 파일명 정렬","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/12981\n풀이 # 문제의 제안사항 중에 다음과 같은 항목이 있다.\nwords는 끝말잇기에 사용한 단어들이 순서대로 들어있는 배열\n제안사항을 통해 words에서 순서대로 원소를 추출한다면 끝말잇기를 재현할 수 있음을 알 수 있다.\n이를 통해 다음과 같은 알고리즘을 고안할 수 있다.\nwords를 순서대로 추출하면서 끝말잇기가 이어지는지 확인 끝말잇기가 끝났다면 문제의 답을 계산 words를 순서대로 순환하는 것만으로도 끝말잇기를 재현할 수 있었기 때문에, words의 index인 $i$와 끝말잇기에 참여한 사람의 수인 $n$을 통해 문제의 답을 구할 수 있다.\n몇번째 사람의 순서인지 $i \\mod n$ 1번째부터 세기 때문에 나머지에 +1 단어들의 반복이 몇번째 이뤄졌는지 $[ i \\div n ]$ 최초 iteration도 1로 취급하기 때문에 +1 몫과 나머지를 위처럼 구해도 되고 python built-in function인 divmod를 통해 구해도 된다.\nq, r = divmod(a, b) 코드 # def solution(n, words): answer = [0,0] stack = [words[0]] for i in range(1, len(words)): # stack의 가장 윗 단어의 마지막 글자와 i번째 단어의 시작이 같고 # i번째 단어가 스택에 없다면 if stack[-1][-1] == words[i][0] \\ and words[i] not in stack: stack.append(words[i]) # stack에 추가가 안된다면 answer 갱신 else: answer[0] = (i % n) + 1 answer[1] = i // n + 1 break return answer ","date":"29 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-29-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%98%81%EC%96%B4-%EB%81%9D%EB%A7%90%EC%9E%87%EA%B8%B0/","section":"Posts","summary":"","title":"[프로그래머스] 영어 끝말잇기","type":"posts"},{"content":" NLP + RL # A Deep Reinforced Model for Abstractive Suumarization # ROGUE score를 올리는 행위를 reward로 설정해서 RL을 수행하는 NLP.\nDCN+ # mixed objective and deep residual coattention for question answering.\n기존의 QA model이 Answer를 잘못 추출하는 경우가 있는데, 이를 RL로 해결.\nRL loss, NLP model의 loss(cross-entropy)를 모두 적절히 사용한다.\nMixed objective function 적용 : cross entropy loss + self-critical policy learning \u0026ndash;\u0026gt; evaluation 방법과 loss function 과의 괴리를 줄임 Residual co-attention encoder 적용 : deep self-attention + residual network Dialogue generation # https://github.com/lvwerra/trl\n공감도를 reward로 설정해서 RL 학습. 생성 모델(GPT-2), 공감도를 평가하는 모델(bert, roberta), 공감도에 대한 RL model로 3개의 모델을 구성. NLP + CV # Description generation # Descriptions of Images in Isolation(DII) 이미지를 개별적으로 설명 Descriptions of Images in Sequence(DIS) 여러 장의 이미지에 대해 설명 Stories of Images in Sequence(SIS) 여러 장의 이미지를 통해 스토리 생성 ![](/assets/images/NLP trends/04787c83-fb7c-4f60-8cbb-4396ae35c4c7-image.png)\nShow and Tell # https://arxiv.org/pdf/1411.4555.pdf\nCNN을 통해 이미지에 대한 Embedding을 생성하고 RNN을 통해 문장을 generate하는 모델. 이 논문을 기점으로 Img-to-Text를 딥러닝으로 해결하고자 하는 시도들이 활발해졌다.\nGLAC Net # https://arxiv.org/pdf/1805.10973.pdf\n여러 이미지들을 통해 하나의 스토리를 생성하는 서울대 논문이라고 한다.\n두 개의 attention을 결합해 하나의 attention group(여기서는 GLocal attention이라고 명명)을 만든다. local attention: 개별 이미지들에 대한 embedding global attention: 여러 이미지들에 대한 embedding ","date":"29 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-29-nlp-trends/","section":"Posts","summary":"","title":"NLP trends","type":"posts"},{"content":"ps aux | grep python | awk \u0026#39;{print $2}\u0026#39; | xargs kill -9 네이버 부스트캠프 AI Tech 2기 김지성 캠퍼님께서 공유해주신 내용.\n중간 ( | ) 명령은 파이프로, 앞 명령의 실행 결과를 뒤 명령으로 전달합니다. ps aux 명령으로 모든 실행중인 프로세스 정보를 가져온다. grep python 명령으로 python 이란 이름을 가진 process 행만 추출 awk \u0026lsquo;{print $2}\u0026rsquo; 명령으로 pid를 의미하는 두번째 열만 추출 awk blog awk는 필드와 레코드를 선택할 수 있다. 여기서는 2번째 필드의 내용을 출력하는 action을 수행 xargs kill -9 명령으로 최종 추출된 pid들을 모두 kill -9 명령으로 종료 xargs blog 파이프라인을 통해 전달받은 값을 argument로 사용하는 명령어 kill -9의 argument로 awk의 결과값을 사용 ","date":"29 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-29-grep-process-%ED%95%9C%EA%BA%BC%EB%B2%88%EC%97%90-%EC%A2%85%EB%A3%8C/","section":"Posts","summary":"","title":"grep process 한꺼번에 종료","type":"posts"},{"content":"","date":"29 October 2021","externalUrl":null,"permalink":"/categories/linux/","section":"Categories","summary":"","title":"Linux","type":"categories"},{"content":"","date":"29 October 2021","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"27 October 2021","externalUrl":null,"permalink":"/categories/python/","section":"Categories","summary":"","title":"Python","type":"categories"},{"content":"","date":"27 October 2021","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"huggingface나 코테에서 활용되는 python built-in container들을 정리했다.\ncollections # UserDict # Docs 일반적인 python의 dictionary와 동일하다. 상속받거나 객체로써 다루기 쉽도록 만든 단순한 wrapper다.\nHuggingFace BatchEncoding Github HuggingFace의 tokenizer를 호출하면 BatchEncoding type으로 return해준다. BatchEncoding을 아래와 같이 pop하는 코드들이 있었는데 이해가 가지 않았다. 찾아보니 BatchEncoding은 UserDict의 subclass였다. 즉, python dictionary에서 쓰는 pop의 기능을 그대로 사용한 것이다.\nsample_mapping = tokenized_examples.pop(\u0026#34;overflow_to_sample_mapping\u0026#34;) OrderedDict # ref blog python 3.6이전에는 Dictionary의 순서가 보장되지 않아서 OrderedDict가 필요했다. 3.6 이후부터는 기본 Dictionary도 iteration에서의 순서성이 보장된다.\n비교 연산 시에 Dictionary는 순서성을 고려하지 않고, OrderedDict는 고려한다.\ndeque # 양방향 큐(Double-ended queue). 공식 docs보다 ref blog에 정리가 더 잘 돼있다.\ndeque.append(item): item을 데크의 오른쪽 끝에 삽입한다. deque.appendleft(item): item을 데크의 왼쪽 끝에 삽입한다. deque.pop(): 데크의 오른쪽 끝 엘리먼트를 가져오는 동시에 데크에서 삭제한다. deque.popleft(): 데크의 왼쪽 끝 엘리먼트를 가져오는 동시에 데크에서 삭제한다. deque.extend(array): 주어진 배열(array)을 순환하면서 데크의 오른쪽에 추가한다. deque.extendleft(array): 주어진 배열(array)을 순환하면서 데크의 왼쪽에 추가한다. deque.remove(item): item을 데크에서 찾아 삭제한다. deque.rotate(num): 데크를 num만큼 회전한다(양수면 오른쪽, 음수면 왼쪽). ","date":"27 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-27-python3-container/","section":"Posts","summary":"","title":"python3 container","type":"posts"},{"content":"https://www.acmicpc.net/problem/14891\n풀이1 # ref blog 구현 문제였고 재귀로 회전과 상태 체크를 어떻게 구현할지 떠올려야 했다.\n회전 쿼리 입력 회전 쿼리에 해당하는 톱니바퀴 회전 왼쪽과 오른쪽으로 구분지어서 톱니바퀴 회전 유무를 계산 왼쪽과 오른쪽으로 구별된 회전유무 값에 따라서 rotate 3번을 재귀로 풀면된다. index를 직접 계산해도 되지만 n이 작고 좀 더 간단한 코드가 나온다.\n4번은 index요소 삭제로 간단하게 구현하려고 했는데 생각보다 잘 안됐다. 이것도 n이 크지 않아서 하나하나 다 옮겨서 rotate를 구현했다.\n3번에서 인덱스 문제로 해매서 시간 내에 못 풀고 나중에 답을 봤다..\n풀이2 # ref github\n프로세스는 풀이1과 동일하다.\n회전 쿼리 입력 회전 쿼리에 해당하는 톱니바퀴 회전 왼쪽과 오른쪽으로 구분지어서 톱니바퀴 회전 유무를 계산 왼쪽과 오른쪽으로 구별된 회전유무 값에 따라서 rotate 2, 4번을 list가 아닌 deque로 구현하고, 3번을 재귀가 아닌 인덱스 계산으로 풀이한 점이 다르다.\n3번은 어떻게 구현하든 상관없는데 2, 4번의 경우 deque 사용이 더 좋아보인다. c의 linked list로 구현된 deque를 쓰기 때문에 deque.rotate가 간결하고 더 빠를 것이다. deque를 쓰자.\n코드 # 풀이 1 코드 풀이 2 코드\n","date":"26 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-26-%EB%B0%B1%EC%A4%80-%ED%86%B1%EB%8B%88%EB%B0%94%ED%80%B4/","section":"Posts","summary":"","title":"[백준] 톱니바퀴","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/43236?language=python3\noverview # 문제 이해부터 어려웠다.. 문제에서 선택 가능한 행동들과 판단들은 다음과 같다.\nrocks 배열에서 n개의 돌들을 제거 rocks 요소들 간의 거리의 최소값을 구하기 최소값들의 최대값 구하기 모든 경우의 수를 계산하기에는 distance가 1,000,000,000이다. brute-force가 안될 때는 bs, bfs/dfs 가지치기 등으로 해결하면 되는데 bfs/dfs를 적용할 경우의 수는 보이지 않는다. 거리의 최소값이나 n을 flag로 설정해서 bs를 적용해보자.\n풀이 # ref1: https://taesan94.tistory.com/154 ref2: https://deok2kim.tistory.com/122\n푸는 방법들은 다양했다. 요점은 거리의 최소값이나 n을 mid로 설정하고 돌들간의 거리를 계산하면서 n을 counting하는 것이다. ref2와 내가 푼 방식이 n을 mid로 설정하는 방식이다. ref1은 거리의 최소값을 mid로 두고 푸는 방법이다.\ndef solution(distance, rocks, n): answer = 0 rocks.sort() # bs의 탐색 대상 = rock 거리 간의 최소값 left, right = 0, distance while left \u0026lt;= right: removeRockCnt = 0 mid = int((left + right) / 2) minDistance = float(\u0026#39;inf\u0026#39;) # 현재 어느 돌에 있는지 저장 current = 0 for rock in rocks: diff = rock - current # 실제로 rocks 배열을 조작하지 않고 # current를 변경해주는 것으로 삭제할 돌을 선택한다. if diff \u0026lt; mid: # current를 저장하지 않고 건너뛰어서 마치 돌을 삭제한 효과를 기대 removeRockCnt += 1 else: current = rock # 삭제하지 않을 돌이기 때문에 거리의 최소값 계산 minDistance = min(minDistance, diff) # n보다 더 많이 지웠다면 더 적게 지워야한다. # 즉, 돌들간의 거리의 최소값이 더 작아져야하므로 right를 감소 if removeRockCnt \u0026gt; n: right = mid - 1 # n보다 더 적게 지우거나 같다면 answer를 일단 저장한다. # 두 경우는 문제의 답을 내포한 경우이기 때문이다. else: answer = minDistance # n보다 더 적거나 같게 돌을 지웠으므로 # 거리의 최소값을 늘려서 지울 돌의 개수를 늘려야한다. left = mid + 1 return answer https://github.com/naem1023/codingTest/blob/master/bs/pg-30-43236.py\n","date":"22 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-22-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%A7%95%EA%B2%80%EB%8B%A4%EB%A6%AC/","section":"Posts","summary":"","title":"[프로그래머스] 징검다리","type":"posts"},{"content":"","date":"22 October 2021","externalUrl":null,"permalink":"/categories/naver-boostcamp/","section":"Categories","summary":"","title":"Naver-Boostcamp","type":"categories"},{"content":" 11주차 학습정리 # 강의 복습 내용 # 개인 대회 회고글: https://velog.io/@naem1023/Relation-Extraction-%ED%9A%8C%EA%B3%A0\n과제 수행 과정 / 결과물 정리 # 대회 준비를 위한 server, wandb 등의 environment 세팅 모델 정의 모델 커스텀 및 실험 실험 재현 리더보드 제출 피어세션 정리 # 실험 내용 정의, 공유, 토론 학습 회고 # 21/10/05~08: 특강 수강.\n","date":"22 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-22-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-11%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 11주차 학습정리","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/42885\n풀이 # 문제의 제한 조건을 잘 읽자. 한 번에 2명밖에 타지 못하는 조건을 못 읽고 시간 낭비를 많이 했다.\n하나의 보트에에 2명만 탈 수 있을 때 가장 효율적인 방법은 무거운 사람과 가벼운 사람을 짝지어서 태우는 방법이다. people 배열을 정렬 한 후, 배열의 양 끝단의 index들을 줄여나가는 방식으로 보트를 태운다. 무거운 사람 혼자서만 보트를 타는 경우의 수를 잘 고려한다면 오름차순, 내림차순 정렬 모두 상관없다.\n코드 # https://github.com/naem1023/codingTest/blob/master/greedy/pg-30-42885.py\ndef solution(people, limit): cnt = 0 people.sort() start, end = 0, len(people) - 1 while start \u0026lt;= end: cnt += 1 if people[start] + people[end] \u0026lt;= limit: start += 1 end -= 1 return cnt ","date":"22 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-22-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EA%B5%AC%EB%AA%85%EB%B3%B4%ED%8A%B8/","section":"Posts","summary":"","title":"[프로그래머스] 구명보트","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/42747\n풀이 # 문제에서 서술한 바를 코딩으로 옮기는 발상이 어려웠다.\n\u0026ldquo;h번 이상 인용된 논문이 h편 이상이고, 나머지 논문이 h번 이하 인용\u0026quot;을 코드로 옮기면 되는 구현 문제이다. citations를 오름차순 정렬 후, 문제를 시작해보자.\n문제를 코드로 정리할 수 있게 풀어쓰면\ncitations의 h번째 요소가 citations[h]번만큼 인용될 때, h + 1이후의 citations 요소들이 h개 이상 존재하게 해주는 h의 최대값을 찾자.\n이다.\ncitations의 길이를 length라고 할 때 citations가 오름차순 정렬됐다면, citations[i]의 값이 length - i보다 크거나 같으면 된다. 이 때 length - i가 최대 h가 될 것이다.\n왜냐하면 정렬이 됐기 때문에 i번째 값이 length - i보다 크거나 같다면 i + 1 ~ length - 1까지의 모든 citations 값들은 length - i보다 크거나 같을 것이다. 따라서, 0부터 i를 시작해서 체크하면 될 것이다.\n코드 # def solution(citations): answer = 0 citations.sort() length = len(citations) for i in range(length): if citations[i] \u0026gt;= length - i: return length - i return answer https://github.com/naem1023/codingTest/blob/master/sort/pg-30-42747.py\n","date":"22 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-22-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-h-index/","section":"Posts","summary":"","title":"[프로그래머스] H-index","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/42578\n풀이 # cloth 종류에 대한 경우의 수를 계산해야하는 문제이다. cloth의 종류는 clothes 리스트 요소들의 1번째 인자에 저장돼있기 때문에 해당 인자들을 기준으로 key를 만들어준다.\nfrom collections import defaultdict def solution(clothes): answer = defaultdict(int) for cloth in clothes: answer[cloth[1]] += 1 cnt = 1 for key in answer: cnt *= answer[key] + 1 return cnt - 1 answer에는 cloth 종류별로 몇 개의 옷들이 존재하는지 저장된다. 이들에 대한 조합을 구할 때는 answer[key]에 1씩 더해준다. 왜냐하면 해당 옷을 입지 않는 경우의 수도 존재하기 때문이다.\n옷을 아예 입지 않는 경우의 수가 있기 때문에 cnt에서 1을 빼준다면 옷을 입는 경우의 수가 완성된다.\n코드 # https://github.com/naem1023/codingTest/blob/master/hash/pg-30-42578.py\n","date":"21 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-21-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%9C%84%EC%9E%A5/","section":"Posts","summary":"","title":"[프로그래머스] 위장","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/42577\n풀이 # https://somjang.tistory.com/entry/Programmers-%EC%A0%95%EB%A0%AC-%EC%A0%84%ED%99%94%EB%B2%88%ED%98%B8-%EB%AA%A9%EB%A1%9D-Python\n해쉬문제였는데 정렬, 인덱스 비교로 해결이 됐다. n이 작은 편이 아니라서 brute-force로 모두 비교할 수는 없다.\nphonebook 리스트를 정렬 i, i + 1번째 전화번호를 비교한다. i가 head라는 가정하에 i+1의 앞 부분을 본다. 같다면 anwer=False 2번째 방법에서 사용할 수 있는 방법은 두가지가 있다.\n# 1. indexing으로 직접 head 비교 if phone_book[i + 1][:len(phone_book[i])] == phone_book[i] # 2. startswith로 비교 if phone_book[i + 1].startswith(phone_book[i]) 풀이 # https://github.com/naem1023/codingTest/blob/master/pg-30-42577.py\n","date":"21 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-21-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%A0%84%ED%99%94%EB%B2%88%ED%98%B8%EB%B6%80/","section":"Posts","summary":"","title":"[프로그래머스] 전화번호부","type":"posts"},{"content":"","date":"20 October 2021","externalUrl":null,"permalink":"/tags/mrc/","section":"Tags","summary":"","title":"MRC","type":"tags"},{"content":" Phrase Retrieval in ODQA # Current limitation of Retriever-Reader ODQA # Error propagation Reader가 아무리 뛰어나도 Retreiver가 제대로 된 context를 전달하지 못한다면 전체 프로세스의 성능이 떨어진다. Query-dependent encdoing query에 따라 answer span의 encoding이 달라진다. e.g., BERT retriever를 사용할 때 query와 context를 concat해서 모델의 결과를 얻기 때문에 query가 달라지면 context와 concat된 embedding의 encoding을 매번 다시 해야한다. encoding 결과도 매번 달라진다. Retriever-Reader 단계를 거치지않으면서 이러한 문제점들을 해결할 수 있는 방법론으로 제시된 것이 phrase retrieval이다.\nSolution # ![](/assets/images/QA with Phrase Retrieval/26b4fa3b-f53b-4a50-8bef-cdaa8104e567-image.png)\nContext의 모든 phrase들을 enumeration(열거)한다. phrase에 대한 embedding vector를 key로 하여 mapping한다. query vector는 query가 들어올 때마다 계산. queyr와 key vector를 비교하는 문제로 치환. ![](/assets/images/QA with Phrase Retrieval/3271090b-ce19-44d9-9c61-f6886edca812-image.png) 학습과정을 비교\n기존의 방법\nphrase, question, document를 score 함수에 넣어서 모든 score 조합을 구한다. Retriever-Reader를 F라는 함수 하나로 표현한 것. F는 a(answer candidates), d, q를 넣으면 알아서 예측된 answer를 도출한다. query가 달라질 때마다 score 함수를 다시 계산해야한다. Phrase retrieval\nF 대신 Question encoder와 Pharse encoder의 조합으로 score 계산. H: a, d를 입력으로 받아 vector space로 보내고 이들간의 가장 유사한 vector를 찾는다. 내적을 하든, 거리 계산을 하든. H 함수의 결과들은 미리 모두 계산됨. 따라서 query의 입력이 발생할 때마다 Q함수만 계산하면 된다. 문제점 F를 G, H로 분해할 수 있다는 가정 자체가 틀릴 수 있다. F를 수학적으로 분리하는 것이 아니라, G와 H를 정의하고 이것이 F를 최대한 근사하도록 노력하자. Key Challenge # 어떻게 phrase를 vector 상에 잘 mapping할 것인가? -\u0026gt; Dense, sparse embedding을 둘 다 사용해보자.\nDense-sparse representation for Phrase # Dense vectors: 통사적, 의미적 정보를 얻는데 효과적 Sparse vectors: 어휘적 정보를 담는데 효과적 Concat # ![](/assets/images/QA with Phrase Retrieval/4231fbd6-2d5c-49a7-beef-7552bfd46a82-image.png)\n두 가지 방법을 합치는 방법은 phrase ODQA를 진행할 때, phrase 별로 dense, sparse embedding을 통해서 vector를 구하고 이를 concat하는 것이다.\nDense representation # ![](/assets/images/QA with Phrase Retrieval/b502584b-4123-429e-9023-5060fa60c7ec-image.png)\nanswer span의 start, end token에 해당하는 hidden state vector를 통해 phrase vector(dense vector)를 생성한다.\n![](/assets/images/QA with Phrase Retrieval/5ab5bac3-a51b-44e4-b7cb-18967160612a-image.png)\ncohrency vector\nphrase가 문장 구성 요소에 해당하는지 나타낸다. 구를 형성하지 않는 phrase를 걸러내기 위해 사용 start vector, end vector를 통해 계산 ![](/assets/images/QA with Phrase Retrieval/0ec457bf-9aa3-4a28-8cd7-ea45f3cbc15f-image.png) Question embedding CLS token으로 생성 일반적인 문서 embedding과 동일 Sparse representation # ![](/assets/images/QA with Phrase Retrieval/cd7e6740-c22d-4418-b7c6-dafac5b4c08a-image.png) contextualized(문맥화된) embedding 활용해서 가장 관련성이 높은 n-gram으로 sparse vector 구성.\ntarget으로 하고 있는 phrase의 주변 단어들과 유사성을 측정 유사성을 각 단어에 해당하는 sparse vector에 넣어준다. TF-IFD와 유사하다. 다른 점은 phrase, sentence마다 weight가 dynamic하게 변한다. unigram, bigram도 활용해서 겹치는 정보들을 활용 가능 Scalability Challenge # wikipeida 데이터를 활용하면 보통 60bilion개의 phrase를 활용하게 된다. 이러한 거대 데이터에 접근해 indexing, searching을 하기 위해서는 scalability가 고려되어야 한다.\nstorage: pointer, filter, scalar quantization 등을 통해 (240TB를 1.4TB까지 줄일 수 있다) search: FAISS 활용 FAISS는 dense vector만 검색 가능하고 sparse vector는 검색 불가능. Phrase ODQA를 통해 Dense vector와 sparse vector를 합칠 것이므로, dense vector를 우선 검색해본다. sparse vector에 대해서 다시 score를 측정해 FAISS 검색 결과를 reranking Results \u0026amp; Analysis # ![](/assets/images/QA with Phrase Retrieval/40ba02a7-4094-4b18-9dcf-dc72997869b9-image.png) SQuAD에서 DrQA(Retreier-reader)보다 3.6% 성능향상이 있고, inference speed는 68배 빨랐다고 한다. ![](/assets/images/QA with Phrase Retrieval/9983f403-a203-418c-aa53-ee8cb279ef75-image.png) 다른 Retriever-Reader보다 속도가 빨랐다. 또한 CPU 연산에 의존하기 때문에 GPU 연산이 필요없다는 장점 아닌 장점도 있다.\nLimitation # ![](/assets/images/QA with Phrase Retrieval/2f5eceb0-656a-4018-a0f1-3d025843dd7f-image.png)\nRAM 용량이 많이 필요하다. 최신 Retriever-Reader 모델들 보다 성능이 낮다 Natural Questions에서 성능이 낮았다. Decomposability gap(F 함수를 G, H로 분리)이 원인 Decomposability gap # ![](/assets/images/QA with Phrase Retrieval/fb0883d9-16b0-4877-85a3-e7ffb986a311-image.png) G, H를 통해 F를 근사하는 것 자체가 오류가 발생할 수 있다. 왜냐하면 F는 매우 복잡한 Retrieval-Reader를 나타내는 함수이기 때문이다. 따라서 필연적으로 G와 H를 통해 F를 근사하는게 정확하지 않다는 것이다.\n","date":"20 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-20-qa-with-phrase-retrieval/","section":"Posts","summary":"","title":"QA with Phrase Retrieval","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/43238\n풀이 # 구현 문제로 접근하면 n이 너무 커서 풀 수 없다. 문제의 답은 최소한의 time cost를 묻고 있으므로 time cost를 기반으로 수용 가능한 인원을 정의할 줄 알아야 풀 수 있었던 문제였다.\ntime cost는 임의의 정수로 설정한다. time cost 동안 모든 심사관들을 일을 수행한다. 따라서 time cost를 심사관들의 처리 속도가 정의된 times 배열의 모든 요소에 대해서 나눠주면 각각의 접수원들이 time cost동안 몇 명을 심사했는지 알 수 있다.\ntime cost에 심사원들이 처리 가능한 인원을 알 수 있다면, 해당 time cost가 너무 많은지 적은지 판단할 수 있다. 즉, \u0026ldquo;60분의 시간이 주어졌을 때는 심사관들의 처리속도가 충분해서 처리하고자 하는 인원보다 더 많은 인원을 처리했다\u0026quot;라는 식으로 생각할 수 있다. 혹은 더 적은 인원밖에 처리못한 사실도 알 수 있다.\nn이 매우 크기 때문에 brute force로는 적절한 time cost를 찾을 수 없고, binary search를 통해서 구해야 한다. 일반적인 binary search의 left, right index가 배열의 index를 의미했다면 이 문제에서는 time cost의 최소값, 최대값에 대해서 탐색한다고 생각하면 된다.\n![](/assets/images/[프로그래머스] 입국심사/71dd7b37-9f4b-48a0-9885-01d4d8500b3c-image.png)\nbs에서 mid 변수는 time cost당 몇명을 처리했는지에 따라서 변화시켜준다. mid 변수는 time cost 자체를 의미한다. 만약 주어진 time cost에서 더 많은 인원을 처리했다면 mid는 줄여야 할 것이다. 최소의 시간값을 찾기 위한 문제이기 때문이다. 만약 더 적은 인원을 처리했다면 문제의 요구사항에 맞추기 위해 mid를 늘려야 한다.\n코드 # https://github.com/naem1023/codingTest/blob/master/bs/pg-30-43238.py\n","date":"20 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-20-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EC%9E%85%EA%B5%AD%EC%8B%AC%EC%82%AC/","section":"Posts","summary":"","title":"[프로그래머스] 입국심사","type":"posts"},{"content":" 기존의 negative sampling # query batch는 기존대로 유지한다.\npassage batch가 달라진다. 1개의 positive passage와 batch_size개의 negative passage로 총 batch_szie + 1 개의 데이터로 하나의 batch를 구성한다.\nNegative in-batch # passage batch는 batch_size개만큼 구성한다. 기존 방식과는 다르게 따로 negative sampling을 하지 않는다. positive 관계인 query와 passage들을 한 쌍으로 같이 넣어주기만한다.\nrandom하게 batch의 요소들을 구성한다. n개의 batch 요소들 중 i번째 query와 나머지 i-1개들은 negative passage 관계이다. 해당 batch를 학습할 때, batch 간의 상관관계들이 함께 학습된다. loss를 구할 때는 batch 내의 index들에 해당하는 positive passage를 target으로 설정한다. 여기서는 torch.arange 같은 함수로 등차수열을 만들면 된다. 학습은 전체 batch에 대해서 이루어지고, loss는 positive sample들에 대해서만 이루어진다. e.g., batch_size = 4 sim_scores = tensor([[-1.0768e+01, -3.7684e+01, -1.3255e-04, -9.1018e+00], [-2.1763e+01, -6.3134e+01, 0.0000e+00, -1.6743e+01], [-1.6615e+01, -4.5871e+01, -1.0729e-06, -1.3856e+01], [-1.3989e+01, -5.5973e+01, -1.1598e-04, -9.0696e+00]], targets = [0,1,2,3] sim_scores 내의 item들은 target에 해당하는 index를 답으로 가지도록 확률이 출력된다. sim_scores의 0번째 item의 i번재 요소들은 i번째를 target에 대한 확률을 의미한다.\n따라서 target은 0, 1, 2, 3가 된다.\n","date":"18 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-18-negative-in-batch/","section":"Posts","summary":"","title":"Negative in-batch","type":"posts"},{"content":"","date":"18 October 2021","externalUrl":null,"permalink":"/categories/ml-basic/","section":"Categories","summary":"","title":"ML-Basic","type":"categories"},{"content":" Definition of Bias # Bias는 지양대상이 아니다. 하지만 일부 bias로 인해 모델의 성능에 악영향을 끼치는 경우가 있고, 이러한 bias issue는 해결해야 한다.\nML/DL inductive bias(ref) 학습 시에는 만나보지 않았던 상황에 대하여 정확한 예측을 하기 위해 사용하는 추가적인 가정 (additional assumptions) 사전 지식을 주입하기 위해 특정 형태의 함수를 선호하는 것 개발자가 모델을 디자인하고 데이터를 모델에 맞게 주입하는 것 자체가 일종의 편향성을 가진다는 것 실제 세계 Historical Bias 현실 세계 자체가 편향된 성향을 가진다면 모델 또한 편향된 성질을 가질 것이다. Co-occurrence bias 성별, 직업 등 표면적인 상관관계 때문에 원치 않는 속성이 학습되는 것 Data generation Specification bias 입력과 출력을 정의하는 방식에서 발생되는 편향 Sampling bias 데이터를 샘플링하는 방식 때문에 생기는 편향 Annotator bias annotator 자체의 특성 때문에 발생하는 편향 Gender bias # ![](/assets/images/Reducing Training Bias/d2f45ed8-ae11-4789-88a9-27bd42c0d60c-image.png)\n사진을 보고 여러 속성을 추출하는 모델이다. Cooking과 여성의 사진이 train data에 많이 포함됐기 때문에 남자가 요리를 하고 있음에도 여자라고 판별하는 경우가 많았다고 한다.\n![](/assets/images/Reducing Training Bias/14acf299-66af-4726-9806-3fcbdab7dbdb-image.png) https://ai.googleblog.com/2020/04/a-scalable-approach-to-reducing-gender.html\n\u0026lsquo;어떤 사람이 의사다\u0026rsquo;라는 단어를 터키어에서 영어로 번역하면 \u0026lsquo;he\u0026rsquo;로 특정된다고 한다. 구글이 의도하지 않았다고 할지라도 train data에서 의사와 남자에 대한 상관관계가 많이 존재했다면 위와 같이 정확하지 않은 assumption을 하게 된다. 사회적인 문제라기 보다는 정확하지 않은 모델 출력의 문제라고 생각한다.\nSampling bias # Biased한 방식으로 sampling하게 된다면 표본이 모집단의 성질을 반영한다는 신뢰성이 사라진다. Random하고 fair하게 sampling할 수 있도록 하자.\nBias in ODQA # Reader model은 train data에서 항상 정답이 문서 내에 포함된 데이터 쌍에 대해서만 positive라고 학습할 것이다. e.g.,) SQuAD의 positive는 (Context, Query, Answer)가 고정됨.\n따라서 Reader model은 positive가 아닌 전혀 다른 성질의 데이터 쌍에 대해서 독해 능력이 매우 떨어질 것이다. e.g.,) 소설, 수필, 비문학 등의 train data로 학습된 reader model은 의학, 공학, 자연과학에 대한 inference에서 성능이 매우 떨어질 것이라고 예상할 수 있다.\nMitigate training bias # Train negative examples negative example을 통해 negative input data를 정답과 먼 곳에 배치 가능 random하게 뽑기보다는 헷갈리는 negative samples를 뽑도록 하자 마치 dense embedding에서 최대한 비슷한 문서들을 통해 negative sample을 뽑은 것처럼 높은 BM25/TF-IDF score지만 답을 포함하지 않은 sample을 활용 같은 문서에서 나온 다른 passage/question을 활용 Add no answer bias no answer에 대한 경우를 처리해야 한다. input sequence 외에 1개의 token이 더 있다고 가정 answer prediction에서 start, end 확률이 해당 bias에 있을 경우 no answer로 취급. Annotation Bias from Datasets # ![](/assets/images/Reducing Training Bias/98739eba-aaed-4025-99d3-f2173a65bfed-image.png)\n질문을 하는 사람이 answer를 모른다는 시나리오대로 dataset이 만들어져야 한다. 왜냐하면 question 자체에 answer에 대한 내용이 포함되거나 answer에 대한 힌트가 너무 많이 담겨있을 수도 있기 때문이다.\n위 표에서 이러한 dataset의 예시는 파란색 박스 안의 datsets다.\n하지만 질문을 하는 사람이 answer를 알고 있는 상태의 시나리오로 데이터 annotation이 발생할 수도 있다. 대표적인 예시가 TriviaQA와 SQuAD다.\nSQuAD의 경우 question과 evidence paragraph에서 많은 단어들이 겹치는 bias가 발생한다. 따라서 모델은 독해 능력을 키워서 답을 찾는 것이 아니라 단순히 단어 맞추기를 위한 학습을 진행할 수도 있다. 물론 이러한 행위가 잘못된 것은 아니지만 모델의 독해 능력 향상을 원한다면 의도된 학습 방향이라고 할 수 없다.\n또한 SQuAD는 사람들이 가장 많이 보는 wiki article의 500개 문서를 train data로 사용하고 있다. 따라서 해당 문서에 대해 bias가 심할 것이다.\nEfficient of annotation bias # ![](/assets/images/Reducing Training Bias/05353d86-b769-4cd1-a891-5b6907eae76c-image.png)\nannotation bias가 존재하는 dataset에 대해서는 객관적인 평가가 이루어지 어렵다. 위 표에서 대부분의 모델들은 DPR을 사용했을 때 더 좋은 score가 나오지만, SQuAD의 경우만 BM25에서 가장 좋은 성능을 보여준다. 왜냐하면 question과 evidience 사이의 overlapped 단어들이 SQuAD는 많기 때문이다.\n이에 대한 해소법으로 BM25, DPR 두 방법을 모두 사용할 수 있다. 이것 또한 annotation bias가 발생된 dataset에 대한 해소법이고 다른 모델들의 경우 오히려 두 방법을 혼용하니 score가 떨어지는 경우도 있었다. dataset에 따라서 적절하게 시도를 해보자.\nDealing with annotation bias # Annotation bias를 방지하기 위한 datset을 사용하자. e.g., Natural Quesitons는 구글 검색 엔진에서 Supporting evidence가 주어지지 않은 실제 유저의 question을 통해 dataset을 구성했다. ODQA와 최대한 비슷한 세팅이기 때문에 Annotation bias를 많이 해소했다고 볼 수 있다.\nODQA에 적합하지 않은 질문들은? # \u0026ldquo;미국의 대통령은?\u0026ldquo;이라는 question은 MRC에서는 풀 수 있는 종류의 문제라고 할지라도 ODQA에서는 아닐 수도 있다. 왜냐하면 현재 대통령을 묻는 것인지, 모든 대통령의 리스트를 원하는 것인지, 과거의 특정 대통령을 원하는 것인지 전혀 모르기 때문이다.\n이러한 종류의 질문들에 대한 처리도 필요하다.\n","date":"18 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-18-reducing-training-bias/","section":"Posts","summary":"","title":"Reducing Training Bias","type":"posts"},{"content":" Open Domain Question Answering(ODQA) # 앞선 MRC와는 다르게 웹 전체, 혹은 위키피디아 전체와 같이 광범위한 Domain에서 Passage retrieval을 수행해야 한다. input, output format은 동일하다.\n![](/assets/images/Linking MRC and Retrieval/93ec9529-7aa0-414d-b8d4-ae4678b4a60a-image.png)\n![](/assets/images/Linking MRC and Retrieval/f2a1963a-29fb-44b8-a928-f56b8d24f681-image.png)\nContext가 따로 주어지지 않는다. World Knowledge에 기반해서 QA 진행 Modern search engine들이 여기에 해당. 연관 문서들뿐만 아니라 연관된 answer들도 제공. History of ODQA # ![](/assets/images/Linking MRC and Retrieval/156edb4a-ae2f-4273-b2c5-995b74906aa4-image.png)\nText retrieval conference(TREC)에서 다뤄진 QA Tracks(1997-2007)에서 ODQA의 형태를 띈 task를 연구.\nQuestion Processing, Passage retrieval, Answer processing으로 이루어지는 것은 현재의 구조와 매우 유사하다. 단순한 Information retrieval(IR)이 아니라 short answer with support를 목표로 했다고 한다. support가 의미하는 것은 답이 담겨 있는 문서를 의미한다. 즉, Question에 대한 답과 해당 문서를 함께 반환하길 원하는 task다.\nQuestion processing # DL, ML model이 발전되지 않았기 때문에 질문으로부터 키워드를 선택하고, 해당 키워드에 대한 answer type selecting을 진행하는 형태였다.\nPassage retrieval # 현재 ODQA와 매우 유사하다.\n기존 IR을 이용해 연관된 document 추출 document를 passage 단위로 자르고 선별 Named entity, passage 내 question 단어 개수 등의 hand-crafted features 활용해 선별. e.g.,) TF-IDF, BM25 Answer processing # Hand-crafted features와 heuristic을 활용한 Classifer를 통해 주어진 Question에 대해서 어떤 document가 사용될지 결정.\n최근의 MRC가 passage 뿐만 아니라 span level에서 답을 도출하기에 answer processing에서는 현재와 차이가 있다.\nRecent ODQA Research # ![](/assets/images/Linking MRC and Retrieval/3d2fb8a1-7627-4bb8-98c0-e524311b29c4-image.png)\nRetriever-Reader approach # 앞선 MRC에서 활용했던 Retriever와 Reader(MRC model)을 활용해서 ODQA를 해결할 수 있다.\nRetriever: DB에서 관련 있는 문서를 검색 input: document corpus, query output: document Train TF-IDF, BM25: labeling data 사용 안하고 self supervised learinig Dense: QA Dataset을 통해 train Reader: 검색된 문서에서 질문에 해당하는 답 도출 input: Retrieved documnet, query output: answer Train SQuAD와 같은 MRC Dataset으로 학습 Distant supervision을 통해 학습 데이터 추가 가능 Distant supervision # Quesition, Answer만 존재하는 dataset들은 answer가 어느 document에 존재하는지 알려주지 않는다. e.g., CuratedTREC, WebQuestions, WikiMovies\n하지만 Reader를 학습하기 위해서는 question, answer 외에 document가 필요하다! 따라서 supporting document가 필요한 경우 직접 answer가 어디에 위치하는지 찾아야 한다. 이러한 행위를 distance supervision이라고 한다.\nRetriver를 이용해 관련성이 높은 문서 검색 필터링을 한다. (ˉ﹃ˉ) 너무 짧고 긴 문서, 질문의 고유명사를 포함하지 않는 문서 제거 (ˉ﹃ˉ) answer가 exact match로 들어있지 않는 문서 제거 필터링되고 남은 문서들 중 사용 단어를 기준으로 하여 연관성이 가장 높은 단락을 supporting evidence로 사용 Inference # Retriever 질문과 가장 관련성이 높은 5개 문서 출력 Reader 5개 문서를 읽고 답변 예측 Reader의 예측 답변 중 가장 score가 높은 것을 최종 답으로 사용 Issues, Recent Approaches # Different granularities of text at indexing time # Wikipedia 데이터를 사용한다고 할 때, 어떠한 기준으로 index를 정의할지 사전에 정의해야 한다. 보통 article, paragraph, sentence를 기준으로 한다.\n![](/assets/images/Linking MRC and Retrieval/2b5da2bb-92f3-45de-bf99-3d5952eaf7ef-image.png)\n따라서 Retriever는 top-k를 정의할 때, granularity에 따라 k가 달라지게 된다. 보통은 위 표와 같이 k를 정의하게 된다. 또한 granularity에 따라서 점수도 약간식 차이가 발생한다.\nk를 늘릴 때 보통 성능이 향상되지만 아닌 경우도 존재한다. MRC 대회에서는 k를 튜닝하는 것도 관건이다.\nSingle passage training \u0026amp; Multi-passage training # Single-passage training 지금까지 배운 방식이다. Retriever는 k개의 문서를 반환하고 Reader는 k개의 문서에 대한 답변과 점수를 구해 최선의 답을 구하는 방식이다. 여러 개의 문서를 보지만 결국 reader의 관점에서는 passage를 개별적으로 보기 때문에 single passage training이라고 한다. Retriever가 건네준 문서들 간의 상관관계를 고려하지 않기 때문이다.\nMulti-passage trainig Retrived passage 전체를 하나의 passage로 간주하는 것이다. single-passage 방식과는 다르게 전체 passage 간의 관계를 고려해서 Reader를 학습시키고자 한다. 학습 문서가 매우 길어지므로 GPU cost가 커진다.\nImportance of each passage # ![](/assets/images/Linking MRC and Retrieval/97556851-9de6-42e9-97b2-a8c4ec1fa922-image.png) Retriever에서 얻은 Passage들의 score를 버리지말고 최종 score를 얻을 때 활용하고자 하는 방법론이다.\n","date":"17 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-17-linking-mrc-and-retrieval/","section":"Posts","summary":"","title":"Linking MRC and Retrieval","type":"posts"},{"content":" Passage retrieal and Similarity Search # ![](/assets/images/Passage Retrieval - Scaling up/4ceaaed8-f971-40c9-8a48-4e21634d2255-image.png)\nPassage와 query를 encoding해서 vector space로 보냈다면 아래의 방법들 중 하나를 수행한다.\nnearest neighborhood search inner dot production에서 highest dot product 결과만 search 위와 같이 similarity serach를 진행할 때, Passage의 수가 늘어난다면 query에 대해서 가장 similarity가 높은 passage를 찾는 것은 어려울 수도 있다. passage의 수가 수천만개가 된다면 brute force로 모든 경우의 수에 대한 inner product를 구하는 것조차도 매우 많은 cost가 들기 때문이다.\nNearest neighbor search # eucliden distance나 L2를 통해서 query vector와 가장 가까운 거리의 passage vector를 찾을 수 있다. 하지만 계산의 효율성을 고려하면 보통 inner dot product를 해서 similarity를 찾게 된다.\nMIPS # Maximum Inner Product Search\n$$v_i\\in V, argmax(q^Tv_i)$$\n$i$개의 passage에 대해서 모든 inner product를 수행하면 되는 간단한 방법론이다. 문제는 passage가 매우 많을 때, 효율적으로 계산하는 방법론을 찾는 것이다.\n실제로 검색해야 될 문서는 위키피디아를 기준으로는 약 5백만개이고, task에 따라서는 수십억, 조까지 늘어날 수도 있다. 이는 document 단위이고 passage를 paragraph 단위로 고려한다면 검색 대상은 더욱 늘어날 것이다.\n즉, 모든 문서의 embedding을 brute force하게 검색할 수 없다.\nThreshold of similarity search # Serach speed query 당 유사한 passage vector k개를 찾는데 얼마나 걸리는지? Memory Usage Vector space 상의 모든 vector들이 어디에 저장될 것인지? 모두 RAM에 있다면 편하겠지만 사실상 불가능 모두 HDD에 있다면 매우 느릴 것 Accuracy brute-force가 아니라면 accuracy를 희생해야 할 것인데 어느 정도까지 희생할 것인가? ![](/assets/images/Passage Retrieval - Scaling up/e24de2b9-1476-40d2-bf57-99346a3fa57e-image.png)\nserach time과 recall은 보통 비례하게 된다. 즉, 정확한 검색을 위해서는 더 오랜 시간이 소몬된다.\n이러한 threshold들에서 나타나는 문제들은 corpus의 크기가 커질수록 아래와 같이 구체화된다.\n탐색 공간이 늘어나므로 검색이 어려워진다. 더 많은 memory space가 요구 Sparse Embedding은 해당 문제가 더욱 심화 compression으로 어느 정도 해소 가능 ![](/assets/images/Passage Retrieval - Scaling up/8afc7957-1279-4f37-b2e9-acdb4d49dd08-image.png)\ndimension = 768 일 때, 위키피디아 문서 수 정도인 10억개의 문서가 존재한다고 가정하면 필요하면 memory space는 3TB이다. 1조가 넘어가면 3PB이다.\n대부분의 대회나 연습용 코드들에서 사용하는 corpus들이 miilion 단위라고 할지라도 3GB도 매우 크다.. compression을 통해 memory space를 효율적으로 사용하는 것은 필수적이다.\nApproximating Similarity Search # Compression - SQ # Scalar Quantization. 통상적으로 4byte(int) size를 사용해서 vector를 표현한다. 하지만 실제로 4byte를 모두 사용하진 않는다. 따라서 사용되는 byte를 줄여서 vector 자체르 압축하는데, 이를 SQ라고 한다.\ne.g.,) 4byte flaoting point -\u0026gt; 1byte unsigned integer\nPruning - IVF # Pruning(가지치기). Passgae retrieval에서는 clustering을 통해 cluster(군집) 형성함을 의미. clustering이 잘 됐다면, MIPS를 매우 효율적으로 수행할 수 있다. 전체 데이터에 대해 brute-force해야 되는 문제를 cluster 개수 k보다 적은 m개의 인접한 cluster만 방문하는 문제로 변경할 수 있기 때문이다.\nIVF(Inverted file). 검색해보니 Inverted index를 의미하는 것이다. https://cloudingdata.tistory.com/45\nRetreival에서의 IVF는 cluster의 대표 vector인 centroid의 id에 해당 cluster의 모든 vector id르 가지고 있음을 의미한다. 즉, query에 가장 근접한 cluster centroid id를 찾는다면 빠르게 해당 cluster의 모든 vector들과 비교 연산이 가능하다.\n![](/assets/images/Passage Retrieval - Scaling up/7e15239c-3b4f-48c4-91bb-a9a7418a0770-image.png)\nFAISS # Facebook에서 similarity search, clustering을 위해 만든 라이브러리다. 오픈소스다. large scale에 특화돼있고 C++로 만들어졌지만 python으로 wrapping되서 매우 빠르고 쓰기도 쉽다!\nHow to use # Train index and map vectors # clustering passage의 clustering을 위해서 passage 데이터에 대한 학습이 필요하다. Scalar quantization 4byte floating point를 1byte integer로 변환할 때, 데이터의 max, min을 파악하고 변환한다. 보통 train data로 학습하지만 학습의 효율성을 위해서 train data에서 sampling해서 FAISS train data로 사용한다. origin train data가 크다면 1/40 정도로 sampling하기도 한다. ![](/assets/images/Passage Retrieval - Scaling up/69716955-8db4-4ff1-88c9-58614bcff882-image.png) trian을 통해 cluster와 SQ 범위 정의되면 train data를 cluster에 SQ대로 투입.\nSearch based on FAISS index # query가 들어온다면 train에서 정의한 cluster, SQ rule에 따라서 query를 reformat. 가장 가까운 cluster를 찾는다. cluster nearest top-k를 정한다. ![](/assets/images/Passage Retrieval - Scaling up/044202c1-7e0a-475e-9a6e-e7055d69e627-image.png) Product Quantization # ref blog 실제로는 SQ말고 PQ를 많이 쓰기도 한다. SQ보다 compression을 더 잘한다고 한다.\n","date":"17 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-17-passage-retrieval---scaling-up/","section":"Posts","summary":"","title":"Passage Retrieval - Scaling up","type":"posts"},{"content":" Sparse Embedding의 문제점 # ![](/assets/images/Dense Embedding/759edf0c-3b21-4ea7-9af2-bcd7a210a0c1-image.png)\nPassage Embedding 중 Spare Embedding은 보통 90% 이상의 벡터값들이 0이다. 차원의 수가 매우 크다. compressed format으로 극복 가능 유사성을 고려하지 못한다. 매우 유사한 단어라도, character가 달라지면 전혀 다른 차원으로 Embedding 된다. 또한 유사한 단어들의 차원이 유사하다는 정보를 표현할 방법이 없다. Dense Embedding # ![](/assets/images/Dense Embedding/5861876f-3fec-4150-95a5-c6a95d4a6f4d-image.png)\n고밀도로 vocab과 차원이 mapping 보통 50 ~ 1000 차원 모든 차원의 정보를 조합해 얻은 위치가 term의 위치가 된다. BoW처럼 하나의 차원이 하나의 term을 나타내지 않는다. 대부분의 요소가 non-zero이다. = 의미가 있다. dimesion이 BoW에 비해 매우 작기 때문에 더 많은 종류의 알고리즘 활용 가능 Sparse Embedding과의 차이점 # Sparse Embedding 정확히 일치하는 term을 찾고자 할 때 성능이 뛰어남 Embedding이 구축되면 추가 학습 불가능 Dense Embedding 단어의 유사성, 맥락 파악에서 성능이 뛰어남. 학습을 통해 Embedding 생성, 추가 학습 가능. 보통 Sparse Embedding만을 쓰는 경우는 거의 없다. 보통 Dense Embedding만을 쓰거나 두가지를 함께 사용해서 Retrieval을 수행.\nOverview of Passage Retrieval with Dense Embedding # ![](/assets/images/Dense Embedding/17abd514-45b0-4b13-b4ea-0b615b6ca892-image.png) query에 대한 hiddens state(CLS 값)을 추출한다. 각각의 passage에 대해서도 동일하게 hidden state를 뽑는다. 이렇게 추출한 두 hidden state의 diemension이 일치한다는 가정 하에, inner product로 유사도를 구한다.\n$$BERT_Q, BERT_B$$는 동일한 모델을 쓸 수도 있고 다르게 미리 학습해두고 사용할 수도 있다. Task에 적합하게 구성하자. Training Dense Encoder # 예제는 BERT지만 뭐든 상관없다. PLM(Pre-trained language model)이기만 하면 된다.\nMRC model: 하나의 PLM에 query, answer를 함께 넣어준다. Dense Encoder: Query, passage 별로 서로 다른 PLM을 준비하고 각각의 PLM에 query와 passage를 넣는다. 서로 다른 CLS token의 최종 output을 활용하기 위해서다! ![](/assets/images/Dense Embedding/c782ea0b-bb5b-42d4-b13e-7e415995db20-image.png)\n앞서 언급했듯 Question encoder, Passage encoder는 동일한 것을 사용해도 무방하고 서로 다르게 Fine-tuned 후 사용해도 된다.\nTraining goal # question과 passage dense embedding 사이의 거리를 좁히는 것. = inner product 값을 높이는 것 = higher similiarity를 찾는 것.\n![](/assets/images/Dense Embedding/4d4ceaa4-5cde-4bc2-8f9d-1c70efbb0866-image.png)\nHow to train?\n기존 MRC dataset의 context와 answer를 활용한다. e.g., SQuAD 서로 연관된 question, passage는 dense embedding 거리를 좁혀준다. high similiarity, positive MRC Dataset에서 실제 context, answer를 그대로 사용한다. 서로 연관되지 않은 question, passage는 dense embedding 거리를 멀게 해준다. MRC Dataset에서 answer에 대한 임의의 context를 사용한다. ![](/assets/images/Dense Embedding/55ba80a5-198f-4d7f-b657-f61f52967ba4-image.png) Choosing negative samples\ncorpus 내에서 random 추출 모델이 헷갈리도록 추출 높은 TF-IDF 값을 가지지만 답을 포함하고 있지 않은 sample Objective function # Positivie passage에 대한 negative log likelihood(NLL) loss 사용 ![](/assets/images/Dense Embedding/f74f181e-4973-4308-a835-8fe97fc4a198-image.png)\n목표: Postive passge의 score를 확률화\npositive passage와 question의 simliarity score negative sample에 대한 score 두 score를 가져와 softmax를 취하고 negative log likelihood를 취해서 학습한다. 위 수식은 negative log likelihood를 표현한 것이다. negative log니까 -log가 붙고 log 안에는 softmax를 넣어준다.\ncorpus를 question, positive, negative로 나눴는데 softmax의 분자에는 positive 요소가, 분모에는 negative 요소까지 포함되서 표현됐다. softmax의 정의가 (target / 전체) 이기 때문이다.\nEvaluation Metric # Retrieval passage 중에서 ground truth passage가 포함됐는지 확인 Retrieval passage 중에서 답을 포함하는 비율을 확인 extractive-base mrc라면 passage에 답이 없으면 답을 낼 수 없다. 따라서 upper bound 형태. Passage Retrieavl with Dense Encoder # ![](/assets/images/Dense Embedding/f00885ca-6af8-438c-99fa-ff2d676e3ddd-image.png)\nOverview에서 보여줬던 내용이다. 미리 계산된 passage들의 embedding 값을 query embedding과 거리 비교를 해서 가장 가까운 passage를 고른다.\n![](/assets/images/Dense Embedding/9f7f3336-9f17-4167-85bd-a0e5405a05c0-image.png)\n선택된 Passage와 query를 MRC model에 넣고 answer를 얻는다.\nImproving Dense Encoder # 학습 방법 개선 e.g., DPR Encoder model 개선 BERT보다 더 좋은 모델 데이터 개선 더 많은 데이터 더 좋은 전처리 ","date":"14 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-14-dense-embedding/","section":"Posts","summary":"","title":"Dense Embedding","type":"posts"},{"content":" Passage Retrieval # query에 맞는 문서(Passage)를 검색(Retrieval)하는 것. ![](/assets/images/Passage Retrieval/b5526456-14fc-409d-83c4-ac2894f973f8-image.png)\nDatabase\n실제로는 DBMS 활용할 수도 있다. 여기서는 Wiki data Passage Retrieval with MRC # Open-domain Question Answering: 대규모 문서에서 질문에 대한 답 찾기\n![](/assets/images/Passage Retrieval/540e7cbd-b9dd-49e6-8a8f-617385ab55b6-image.png)\nPassage Retrieval과 MRC를 결합하면 Open-domain Question Answering이 된다.\nPassage Retrieval 답이 있으리라 예상되는 context를 MRC 모델에게 준다. MRC 주어진 context를 통해 답 도출 Overview # ![](/assets/images/Passage Retrieval/fba64164-0725-4dd1-a80a-ffccca06482a-image.png) Passage Retrieval은 Embedding space를 기반으로 검색한다.\nQuery, Passage를 Embedding Space에 Embedding한다. Passage는 미리 한꺼번에 Embedding해서 효율적으로 사용 Query, Passage Embedding 사이의 similarity score 계산 vector 사이의 거리 계산, 거리가 짧을수록 유사 inner product 계산, 값이 클수록 유사 Query에 대해서 Passage들의 similarity ranking을 매겨서 출력 Passage Embedding # 마치 Word Embedding을 하듯이 Passage를 vector화 하기 위해 Embedding한다. ![](/assets/images/Passage Retrieval/dcad009f-066d-4cbb-9305-6b451cdbec23-image.png) 우리가 일반적으로 아는 Embedding space처럼 vector간 inner prodcut, 거리 계산을 통해 similarity를 계산한다.\nSparse Embedding # Sparse: dense의 반대. 0이 아닌 숫자가 매우 적게 존재함을 의미.\n대표적으로 BoW(Bag Of Words)가 있다. 단어들이 하나의 차원을 구성하게 되면서 문서를 Embedding할 때 Vocab의 수만큼 차원이 존재해야한다. 즉 필연적으로 대부분은 0으로 매꾸지고 매우 적은 숫자들만이 0이 아니게 될 것이다.\nBoW에서 n-gram의 n을 늘릴수록 구성가능한 경우의 수는 기하 급수적으로 늘어난다. 따라서 bigram(2-gram)까지만 활용하고 간혹 trigram까지 활용한다.\n** Term value **\nTerm(단어)이 document에 등장하는지에 대한 여부만 판단(binary) Term이 몇 번 등장하는지 판단(term ferquency). e.g., TF-IDF Sparse Embedding의 특징\nDimension of embedding vector = number of terms n-gram의 n이 커질수록 커진다. Term overlap을 정확하게 파악해야 할 때 유용 e.g., 검색 시 term이 문서에 포함됐는지를 판단 의미(semantic)가 비슷하지만 다른 단어인 경우 비교 불가 전혀 불가능하다! TF-IDF # Term frequency - Inverse Document Frequency\nTF(Term frequency): 단어의 등장 빈도 raw count / num words를 구하고 normalization을 한다. log normalization을 하기도 한다. IDF(Inverse Document frequency): 단어가 제공하는 정보의 양 자주 등장하는 단어들: 제공하는 정보량이 적다고 판단 자주 등장하지 않는 단어들: 제공하는 정보량이 많다고 판단 IDF ![](/assets/images/Passage Retrieval/2fe016b3-3cc6-4ebd-8e60-a8f2ad0ff9c6-image.png) $$DF(t)$$: Term t가 등장하는 문서의 수\nTF와 다르게 term에만 의존한다. Term t가 등장하는 문서가 적을수록 IDF(t)의 값이 커진다. TF-IDF $$TF-IDF = TF(t,d) \\times IDF(t)$$\n관사들은 Low TF-IDF일 것이다. TF가 높을 수 있지만, IDF가 0에 수렴하기 때문이다. 자주 등장하지 않는 고유 명사들은 High TF-IDF일 것이다. TF가 낮더라도 IDF가 많이 커지기 때문이다. TF-IDF 계산 # 각각의 term에 대해서 한번씩 구할 수도 있지만 표끼리 곱하는 형식으로 구할 수 있다. ![](/assets/images/Passage Retrieval/5e75ae76-a297-45e7-b3b1-5066bd213300-image.png)\nrow는 하나의 문서를 나타낸다. column은 vocab이다. 예시에서는 하나의 문서에는 하나의 vocab 요소만 들어가서 모든 값이 0 혹은 1이다. 즉, 표의 값들이 바로 TF 값들이 된다.\n![](/assets/images/Passage Retrieval/8fd60152-c21d-46c5-9325-919129b69437-image.png)\nIDF 값을 term에 대해서 구했다. term에만 의존되기 때문에 문서에 무관하게 같은 값을 가진다.\nTF-IDF는 TF와 IDF 값을 곱하면 되므로 위의 두 표를 element-wise하게 곱하는 것만으로도 전체 TF-IDF를 구할 수 있다.\n![](/assets/images/Passage Retrieval/777f7b23-43cf-48e7-9c53-9707fac353e3-image.png)\nTF-IDF 활용 # Passage retrieval에서 query와 passage 사이의 similarity를 얻는데 사용할 수 있다.\nquery를 toeknization vocab에 없는 token들은 제외 query를 document로 간주하고 TF-IDF 계산 미리 구해둔 passage들의 TF-IDF와 유사도 점수 계산 가장 높은 점수를 가지는 passage 채택 ![](/assets/images/Passage Retrieval/02dbcb40-5880-4361-bbb1-5b35dd7c33f8-image.png)\nBM25 # TF-IDF에 기반. 문서의 길이까지 고려하여 점수를 매긴다.\nTF 값에 한계 지정. 평균적인 문서의 길이보다 더 작은 문서에서 term이 매칭된 경우, 더 작은 문서에 가중치 부여 실제 검색엔진, 추천 시트템 등에서 아직까지 많이 사용되는 알고리즘 ![](/assets/images/Passage Retrieval/9d7e3b01-2b05-4954-9c90-b38278c77f1d-image.png) ","date":"13 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-13-passage-retrieval/","section":"Posts","summary":"","title":"Passage Retrieval","type":"posts"},{"content":"ref blog\n문제 # 우리가 흔히 아는 그 게임이다. 다만 제약조건이 추가된다.\n한 번의 이동에서 이미 합쳐진 블록은 다시 못 합침 3개 이상의 블럭이 합쳐질 수 있다면, 이동하는 방향에 가장 가까운 것들부터 합친다. e.g., 위로 이동하는거면 위에 것들부터 풀이 # 합치는 아이디어 자체는 쉽게 도출 가능하다. 비어있으면 값을 옮기고, 값이 같으면 2배로 바꾸고, 이것도 아니라면 윗 칸에 옮기면 된다.\n다른 분의 해설을 봐도 이해하기 어려웠던 점은 한 번 합쳐진 블럭에 대해서 막아주는 것이다. 뭔가 flag를 설정할 줄 알았는데 합쳐지는 row 혹은 column에 대해서 계속 값을 증가시키는 쪽으로 합쳐지는 row 혹은 column을 정했다.\n두번째로 어려웠던 점은, 4방위에 대해서 어떻게 처리할 것인가이다. 이것은 row와 column에 대해서 어떤 것을 먼저 처리할 것인지 제대로 숙지해야했다.\n가령 좌우측으로 옮길 때는 column을 기준으로 row를 1개씩 옮겨야한다. 상하측은 row를 기준으로 column을 1개씩 옮겨야 한다.\nif direction == 0: # arr의 모든 column을 보기 위해서 n번 본다 for j in range(n): idx = 0 # arr의 0번째 row를 빼고 모든 row에 대해서 봐야하므로 for i in range(1, n): # arr의 요소가 0이 아니라면 == 비어있지 않다면 if arr[i][j]: # 일단 옮길 것을 저장해둔다. temp = arr[i][j] # 일단 옮길 것의 본래 공간을 비워둔다. arr[i][j] = 0 # 옮겨갈 곳이 0일 때 if arr[idx][j] == 0: # 비어있으므로 그냥 옮기면 된다. arr[idx][j] = temp # 옮기는 것과 옮겨갈 것이 같다면 elif arr[idx][j] == temp: # 합친다 arr[idx][j] = temp * 2 # 다시 합치면 안되므로 idx를 더해준다. idx += 1 # 옮기는 것과 옮겨갈 것이 다르다면 else: idx += 1 # 막아주고 arr[idx][j] = temp # 복원 [0, 3]을 순서대로 상하좌우에 맵핑했다. direction = 0이므로 위 코드는 위로 블럭을 옮기는 예제다.\n위로 옮기는건 쉽다. 좌우가 문제다. 좌는 아래처럼 해줬다.\nelif direction == 2: for i in range(n): idx = 0 for j in range(1, n): if arr[i][j]: # 일단 옮길 것을 저장해둔다. temp = arr[i][j] # 일단 옮길 것의 본래 공간을 비워둔다. arr[i][j] = 0 # 옮겨갈 곳이 0일 때 if arr[i][idx] == 0: # 비어있으므로 그냥 옮기면 된다. arr[i][idx] = temp # 옮기는 것과 옮겨갈 것이 같다면 elif arr[i][idx] == temp: # 합친다 arr[i][idx] = temp * 2 # 다시 합치면 안되므로 idx를 더해준다. idx += 1 # 옮기는 것과 옮겨갈 것이 다르다면 else: idx += 1 # 막아주고 arr[i][idx] = temp # 복원 i는 무조건 row를 위한 변수로, j는 무조건 column을 위한 변수로 쓸 것이다. 이 때, 좌측으로 옮기는 task는 column을 기준으로 row를 한개씩 옮겨가면서 살펴봐야한다. 따라서 for문의 순서를 바꿔서 i, j 순서로 돌도록 해야한다.\n나머지는 위로 옮기는 것처럼 자연스럽게 쓰자.\n아래로 옮기는건 i를, 우측으로 옮기는 것은 j를 반대로 range하면 된다. 즉, n-2부터 1까지 보도록 해주자.\n코드 # https://github.com/naem1023/codingTest/blob/master/implementation/acmipic-12100.py\n","date":"13 October 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-10-13-%EB%B0%B1%EC%A4%80-2048-easy/","section":"Posts","summary":"","title":"[백준] 2048 (Easy)","type":"posts"},{"content":" Generation-based MRC # context와 question을 보고 답변을 생성하는 task. Extraction-based MRC가 context의 token별로 정답 확률을 추출했다면, Genration-based는 이름처럼 Generation task다.\n즉, Extraction-based MRC는 Generation-based MRC task로 변환이 가능하지만 역은 불가능하다.\n평가 방법 # Extraciton-based처럼 EM, F1 score를 쓸 수도 있지만 BLEU, ROUGE를 쓰는 것이 일반적이다.\nOverview # ![](/assets/images/Generation-based MRC/32a731fb-0dc3-4d28-b252-8be2e304e21f-image.png) Extraction-based와 다르게 Generation-based는 모델이 정답을 즉시 생성한다. 일종의 Seq2Seq이다. BERT같은 경우 Encoder만 있기 때문에 Seq2Seq처럼 활용이 안된다.\nExtraction-based와의 차이 # Extraction-based PLM(Pre-trained Language Model) + Classifier Context 내의 답의 위치를 찾기 위해 loss 계산 모델의 출력을 answer로 변환하는 과정 필요 Generation-based Seq2Seq PLM Free-form text 생성 Pre-processing # 정답의 위치를 특정할 필요가 없어서 Extraction에 비해서 간단하다. Question, answering을 있는 그대로 주기만 하면 된다.\nToeknization\nWordPiece Tokenizer Special token ![](/assets/images/Generation-based MRC/1e3354fe-1249-43d6-a755-04703431eb46-image.png)\n일반적인 LM과 같이 CLS, SEP, PAD 등이 사용될 수 있지만 모델마다 우측처럼 question, context로 문장을 구분하기도 한다. 모델마다 다르니 사용하고자 하는 모델이 요구하는 형식을 살펴보자.\nAttention mask Extraction-based와 동일하게 일반적인 LM처럼 처리\nToken type IDs BERT와 달리 BART는 sequence에 대한 구분이 없어서 token type IDs가 없다.\n출력 표현 처리 ![](/assets/images/Generation-based MRC/7d2da16f-64d5-4977-9319-74b830ba8d07-image.png)\n익히 알고 있는 Seq2Seq이기 때문에 Decoder의 출력 form에 대해서는 별도의 처리가 필요하지 않다.\nModel # ![](/assets/images/Generation-based MRC/0cc99423-340d-4156-b459-1f0c551e9353-image.png) MRC에서는 Seq2Seq가 필요하기 때문에 BERT, GPT처럼 Encoder, Decoder만이 존재하는 모델이 아니라 Encoder, Decoder를 모두 가지고 있는 모델이 필요하다.\nBART는 denoising autoencoder라고 한다. BERT처럼 masking이 된 문장을 input으로 하고 GPT처럼 문장을 생성한다. 이것이 마치 노이즈가 발생된 문장에 대한 autoencoder 형태와 같다고 해서 붙여진 것 같다.\n![](/assets/images/Generation-based MRC/e4de4ada-097a-4cb5-9c7b-bb80ed0d76e0-image.png)\nBART\nEncoder: BERT처럼 Bi-directional Decoder: GPT처럼 uni-directional(autoregressive) Pre-training BART # ![](/assets/images/Generation-based MRC/5e37832b-03d0-4635-8094-8cf90ba60ea3-image.png)\nBART는 문장에 masking을 하고 원래 문장을 복구하는 모델이다. 즉, 이를 Generation에 사용하는 것이다.\nPost-processing # Decoder에서는 여러 방법론을 선택할 수 있다.\nGreedy search Exhaustive search: 모든 가능성을 보는 것 Beam search: Exhaustive를 하되 top-k만 본다. ","date":"13 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-13-generation-based-mrc/","section":"Posts","summary":"","title":"Generation-based MRC","type":"posts"},{"content":"","date":"13 October 2021","externalUrl":null,"permalink":"/tags/boostcamp/","section":"Tags","summary":"","title":"Boostcamp","type":"tags"},{"content":" Extraction-Based MRC # 질문의 답변이 항상 주어진 지문(context)내에 span으로 존재. 답변을 생성하지 않고 답변을 context에서 찾는 것으로 문제를 좁힐 수 있다. e.g.,) SQuAD, KorQuAD, NewsQA, Natural Questions ![](/assets/images/Extraction-Based MRC/722a0e0b-0d30-4a1b-9385-39bff9ad45ab-image.png)\n이러한 dataset들은 HuggingFace Datsets에서 다운 받는게 제일 편하다.\nMetric # Exact Match(EM) Score # 예측값과 정답이 character 단위로 완전히 일치할 경우에만 1점 부여. 하나라도 다르면 0점.\nF1 score # 예측값과 정답의 overlap을 비율로 계산하기 때문에 [0, 1]가 점수의 범위가 된다. ![](/assets/images/Extraction-Based MRC/312d9333-731d-4210-91e8-c34a0cdc3522-image.png)\nOverview # ![](/assets/images/Extraction-Based MRC/e320aa87-c2e7-4e26-bb65-a541bdbce9f5-image.png)\nPre-processing # Tokenization\n최근에는 Byte Pair Encoding(BPE)를 많이 사용한다. Out-of-vocabulary(OOV) 문제 해결 가능 정보학적으로 이점(?) BPE 중 WordPiece Tokenizer 사용할 것 자주 나오는 token 위주로 구분짓는다. Attention mask\nPositional Embedding에서 발생 보통 0은 무시, 1은 연산에 포함된다는 의미 Token type IDs\nQuestion에는 0, Context는 1로 mask를 줘서 1이 나타나는 범위에서만 답을 찾도록 유도 따라서 PAD 토큰도 0으로 처리 답의 위치 Tokenization을 하면 답의 인덱스도 달라질 것이다. 이에 대한 전처리가 필요하다. 보통은 start, end index만 알면 되기 때문에 답을 포함하고 있는 span을 찾기만 하면 된다.\nFine-tuning # ![](/assets/images/Extraction-Based MRC/88682340-7783-48f2-bdf1-12f6ef10e851-image.png)\nContext의 모든 token들이 두 개의 Output을 출력하도록 BERT의 출력단을 변경한다.\n해당 token이 답의 시작 token일 확률 해당 token이 답의 끝 token일 확률 확률값들을 모두 구할 수 있다면 groun truth와 비교해서 cross-entropy loss를 구할 수 있다. 이후의 과정은 익히 알고 있듯이 softmax를 취하고 negative log likelihood를 구하면서 학습이 진행된다. ref\nPost-processing # 불가능한 답 제거\nEnd position이 start position보다 앞에 있는 경우 예측한 위치가 context 범위 밖인 경우 max_answer_length보다 긴 경우 *최적의 답 찾기\nStart/end position prediction에서 score(logits)가 가장 높은 N개를 각각 찾는다. 불가능한 start/end조합을 제거한다. 가능한 조합들을 score의 합이 큰 순서대로 정렬한다. Score가 가장 큰 조합을 최종 예측으로 선정한다. Top-k가 필요한 경우 차례대로 내보낸다. ","date":"13 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-13-extraction-based-mrc/","section":"Posts","summary":"","title":"Extraction-Based MRC","type":"posts"},{"content":" XLNet # 기존의 모델들의 문제점\nBERT [MASK] 토큰을 독립적으로 예측하기 때문에 Token 사이의 관계 학습이 불가능 Embedding length의 한계로 Segment간 관계 학습 불가능 GPT 단일 방향성으로만 학습 이러한 한계를 극복하고자 XLNet이 등장한다.\nRelative positional encoding # 512 token으로만 학습하는 한계를 벗어나기 위해 도입. 기존의 positional encoding(Ref)을 relative하게 한다고 한다.\n![](/assets/images/최신 모델/894a3f8c-280e-46dc-afc3-da6e55068235-image.png)\n기존 Positional encoding은 0, 1, 2, 3 \u0026hellip; 과 같이 절대적인 위치를 활용 Relative positional encoding은 0번째, 1번째, 2번째, \u0026hellip; 와 같이 상대적인 거리 표현법을 사용. -\u0026gt; Sequenec 길이 제한이 없어졌다.\nPermutaion language modeling # [MASK]를 없앴다. 대신 순열(Permutation)을 활용해 순서를 섞은 데이터로 학습을 진행해서 순서에 종속되지 않도록 학습을 유도했다. ![](/assets/images/최신 모델/69701f1d-1b5b-44b8-b848-359d80d9d070-image.png)\nPerformance # ![](/assets/images/최신 모델/2102ee3c-4d28-42ad-9caa-0970e790bbc8-image.png)\nGLUE에서 이전 모델들보다 더 좋은 성능을 냈다.\nRoBERTa # BERT와 동일한 구조지만 학습 방법에서 변화점을 찾음.\nModel 학습 시간 증가 + Batch size 증가 + Train data 증가 Next sentence prediction 제거 Fine-tuning과 관련없는 task 너무 쉬운 task이기 때문에 오히려 모델의 성능이 하락한다고 논문에서 밝힘 Longer sentence 추가 Dynamic masking 동일 데이터에 대해서 masking을 10번 다르게 적용하여 학습 BART # BERT와 GPT의 학습 방법을 함께 적용해서 학습하자. ![](/assets/images/최신 모델/4d3287c1-654e-4bcf-b813-43feaf37d360-image.png) ![](/assets/images/최신 모델/13d2157f-66c7-4fdd-b91c-8050d5c77219-image.png) BERT, RoBERTa보다 좋았다고 한다.\nT5 # ![](/assets/images/최신 모델/1bcdfcf3-5185-48d0-9f11-17f6b956c6eb-image.png)\nTransformer Encoder-Decoder 통합 LM으로 현재 가장 좋은 LM. 여러 어절에 대해 Masking을 하고 한꺼번에 복원한다.\n![](/assets/images/최신 모델/5a0180ab-e3d7-436a-9f54-e948514bc204-image.png)\n![](/assets/images/최신 모델/f577a4da-6d1e-48c7-9f2e-7162ecacb72e-image.png)\nGLUE에서 제일 좋은 성능의 모델이다.\nMeena # 대화만을 위한 LM. ![](/assets/images/최신 모델/8e407b53-6ec5-47f8-9be0-67e0cb7217a8-image.png)\nTransformer encoder 1개와 Transformer decoder 여러개로 구성된다.\n341GB의 소셜 미디어 데이터로 학습 , 26억개의 parameters. 챗봇 평가 metric인 SSA(Sensibleness and Specificity Average)를 제시 구체적이고 명확한 답변을 하면 SSA 점수가 높다. 애매모호한 답변만으로도 챗봇을 만들 수 있는 허점을 매꾸기 위한 점수. ![](/assets/images/최신 모델/f87c8902-9836-48c8-8618-50092c9c29e8-image.png)\nControllable LM # 윤리성과 같이 인간의 가치판단적인 면이 제어 가능한 LM.\nPPLM(Plug and Play Language Model) # ![](/assets/images/최신 모델/f4265dd7-4e65-40f2-a6b4-184cf5438438-image.png)\n일반적 LM 확률 분포에 기반해 다음 단어 예측 PPLM 다음 단어의 예측을 개발자가 원하는대로 조정 다음에 나올 단어들을 Bag of word에 저장 \u0026ldquo;The chicken tastes delicious\u0026quot;라는 문장을 원한다고 가정해보자. 그런데 모델의 결과는 마지막 어절로 ok를 준다. PPLM에서는 backpropagation을 통해서 chicken의 vector를 수정해서 마지막 어절로 delicous가 나오도록 유도를 한다.\n장점: gradient update가 발생하지 않고 단순히 출력 vector를 수정해 원하는 출력 유도.\n활용 # Bag of word에 여러 카테고리의 단어들을 저장한다면 해당 카테고리가 중첨된 결과물을 만들 수도 있다. e.g., 기쁨 + 놀람 + 게임 특정 카테고리에 대한 감정을 다른 언어로 생성 e.g., 종교적, 정치적, 인종적 키워드를 중성적 단어로 생성 확률분포조절로 인터넷 밈인 그라데이션 분노 생성 가능 e.g., 단계적으로 분노에 대한 확률을 높여준다. 필요성 # bias가 제거된 데이터만으로 학습된 LM은 반드시 bias가 제거된 출력을 하지 않는다. 따라서 PPLM과 같은 방법으로 현재 LM의 한계성을 극복한다.\nMulit-modal LM # LXMERT # Cross-modal reasoning language model Learning Cross-Modality Encoder Representations from Transformers. ![](/assets/images/최신 모델/ae49ffc2-4884-464b-9445-5267fa4e41c3-image.png)\nImage와 Language 정보를 각각 Embedding하여 해당 정보를 이미지와 자연어에 대한 정보를 생성하는 Cross-modality Encoder에 각각 모두 준다.\n![](/assets/images/최신 모델/6f9ccfa5-8cf6-445d-b85c-4097188eb254-image.png)\n위 예시처럼 자연어에 대한 Question에 이미지, 자연어적 대답을 모두 할 수 있었다고 한다.\nBERT for vision-and-language # BERT와 구조는 동일하다. 단지, 이미지와 자연어를 [SEP]를 기준으로 묶어서 학습했을 뿐이다. 이전과 동일하게 CLS token에 classifier를 부착해서 자연어와 이미지가 합쳐진 정보에 대한 분류가 가능하도록 설계됐다.\n![](/assets/images/최신 모델/9b72842e-e625-45de-a456-823fe475a816-image.png)\nDall-e # 부스트캠프에서 벌써 3번째로 맞이하는 그 모델이다. ![](/assets/images/최신 모델/66f3549d-2b77-4907-b5fb-63bf3048cddb-image.png) 이미지를 생성하기 위해서는 이미지 토큰을 학습해야 한다. (256, 256, 3)의 이미지만 하더라도 매우 큰 사이즈이기 때문에 위와 같은 VQ-VAE를 통해서 차원 축소를 진행한다. VQ-VAE를 매우 간략화한 도식도이다.\n이렇게 이미지를 latent vector화 할 수 있게 되면 나머지는 GPT와 동일하다. ![](/assets/images/최신 모델/2278744b-5c9f-4e93-ba02-f5702923c539-image.png) GPT가 다음 문장을 예측하듯이 Dall-E도 256 token의 Text Embedding 뒤에 올 Image Embedding을 예측하도록 Autoregressive하게 학습한다.\n","date":"13 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-13-%EC%B5%9C%EC%8B%A0-%EB%AA%A8%EB%8D%B8/","section":"Posts","summary":"","title":"최신 모델","type":"posts"},{"content":"","date":"12 October 2021","externalUrl":null,"permalink":"/tags/gpt/","section":"Tags","summary":"","title":"GPT","type":"tags"},{"content":" BERT: embedding 모델 Transformer encoder 사용 GPT: 생성 모델 Transformer decoder 사용 GPT 개요 # ![](/assets/images/GPT 언어 모델/2460ab0e-0bd1-426d-a664-61210c94f9f3-image.png)\n일반적으로 배웠던 Language model의 언어 생성 과정과 동일하다. 순차적으로 다음에 올 가장 적절한 단어들을 확률적으로 예측한다.\n![](/assets/images/GPT 언어 모델/e9d75507-faa0-4245-adaa-10d7239d739f-image.png)\nGPT-1은 마치 BERT처럼 모델의 뒷단에 원하는 classifer를 붙여서 특정 task에 적합하도록 fine-tuning할 수 있도록 구성됐다. 시기상으로는 GPT-1이 BERT보다 앞선다.\nGPT-1은\n자연어 문장 분류에 매우 유용한 디코더다. 적은 양의 데이터로도 높은 분류 성능 달성. 다양한 자언어 task에서 바로 SOTA 달성. Pre-train 언어 모델의 지평을 열면서 BERT 발전의 밑거름이 됐다. 지도 학습이 필요해 많은 labeld data가 필요. 특정 task를 위해 fine-tuning된 모델은 다른 task에서 사용 불가능. GPT 연구진의 새로운 가설\n언언의 특성 상, 지도학습의 목적 함수는 비지도 학습의 목적 함수와 같다. 즉, fine-tuning이 필요없다.\n왜냐하면 labeld data의 label도 언어이기 때문이다.\n다시 말하면, 굉장히 거대한 데이터 셋을 학습한 language model은 모든 자연어 task를 수행 가능하다는 것이다.\nzero-shot, one-shot, few-shot # ![](/assets/images/GPT 언어 모델/d8863c8a-706a-45a6-bc3c-fb552188cb23-image.png)\nfine-tuning으로 하나의 task만을 위한 모델을 만드는 것은 불필요하다고 판단. 마치 인간이 새로운 task 학습을 위해 많은 데이터가 필요하지 않다는 것과 같은 이치로 language model에 접근해서 zero, one, few-shot으로 inference하는 방법론이 제시됨.\n즉, 특정 task를 위해 gradient update를 하지 않고 task를 수행하는 것이다. 이러한 방법을 적용하기 위해 거대 데이터셋을 학습하는 모델을 개발했는데 이것이 GPT-2이다.\nGPT-2 # ![](/assets/images/GPT 언어 모델/b59b94a8-92a9-412f-bed9-7ac1a0cf90b0-image.png) GPT-1에 비해서 약간의 decoder 구조 변경이 있다.\n또한 train data는 11GB에서 40GB로 늘었다.\n![](/assets/images/GPT 언어 모델/9fb5cd35-efa5-4193-8b92-46c112fdae40-image.png)\nMRC, summarization, translation 등의 자연어 task에서는 일반적인 신경망 모델 수준의 성능이었다. 다음 단어 예측에서는 SOTA zero, one, few-shot learning의 새 지평을 열었다. GPT-3 # ![](/assets/images/GPT 언어 모델/8fbc30df-8739-4bdd-b7f3-f6b92e8bdcb1-image.png)\ntrain data는 45TB에서 정제된 570GB를 사용 Parameters는 1,500M에서 175,000M으로 증가. ![](/assets/images/GPT 언어 모델/a207ac5b-5f92-480d-8a6a-6a6b4c5025cb-image.png)\nInitialization 수정 Sparse Transformer 사용 Task of GPT-3 # 기사 쓰기 GPT-3가 작성한 기사들 중 52%는 실험자들이 인간이 작성한 기사 같다고 평가. 덧셈 연산 2 ~ 3자리수 숫자들의 덧셈 연산은 거의 정확하게 수행 QA에서도 기존 모델보다 더 좋은 성능을 기록하기도 함. 데이터 파싱 특정 문서에서 알아서 데이터를 파싱해 표를 그려준다. Restriction # GPT도 NSP(Next senetence prediction)를 통해 Pre-train된 모델이다.\nWeight update가 없다. 새로운 지식 학습 불가 모델 사이즈만 키우는 것이 만능인가? 아무도 모르지만 아마도 아닐 것. 멀티 모달 정보 활용 불가 ","date":"12 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-12-gpt-%EC%96%B8%EC%96%B4-%EB%AA%A8%EB%8D%B8/","section":"Posts","summary":"","title":"GPT 언어 모델","type":"posts"},{"content":" Unicode # e.g., U+AC00\n\u0026lsquo;U+\u0026rsquo;: unicode를 뜻하는 접두어 \u0026lsquo;AC00\u0026rsquo;: 16진수 code point python # ord: character to unicode code point\nchr: unicode code point to character\n완성형 한글 11,172자\nlen을 적용하면 2 반환 조합형 한글\nlen을 적용하면 1 반환 Tokenization # 사람이 직접 정의한 rule로 tokenizing에 한계가 있음에 동의하는 추세이고, 최근에는 데이터 기반으로 접근하는 추세라고 한다.\nSubword 자주 쓰이는 글자 조합을 하나의 단위로 취급 자주 쓰이자 않는 조합은 subword로 분할 BPE(Byte-Pair Encoding) 가장 자주 나오는 글자 단위 Bigram(or Byte Pair)를 다른 글자로 치환 ","date":"12 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-12-unicode-tokenization/","section":"Posts","summary":"","title":"Unicode, Tokenization","type":"posts"},{"content":" MRC # 기계독해. Machine reading comprehension. 주어진 지문(context)를 이해하고, 질의(Query/Question)의 답변을 추론하는 task.\n최종적으로는 train에 사용한 MRC Dataset에 존재하지않는 QA에 대해서도 외부 데이터를 활용해 답을 하고자 한다.\nExtractive Answer Datasets # 질의(question)에 대한 답이 항상 주어진 context의 segment(or span)으로 존재\nCloze Tests # e.g., CNN/Daily Mail, CBT Question, Answering의 형식이긴하지만 우리가 MRC에서 원하는 것과 같이 온전한 형태의 질문이 아니다.\nSpan Extraction # e.g.,) SQuAD, KorQuAD, NewsQA, Natural Questions Descriptive Narrative Answer Datasets # Context 내의 span으로 답을 추출하지 않고, Question을 보고 생성된 sentence(or free-form)의 형태를 답으로 결정.\ne.g., MS MACRO, Narrative QA Multiple-choice Datasets # Answer candidates 중에서 Question의 답을 고르는 task. MRC QA model을 만드는데 적합하지는 않다고 한다. e.g.,) MCTest(2013년에 공개된 최초의 public MRC dataset이라고들 한다), RACE, ARC\nChallenges in MRC # Paraphrased paragraph # P1과 P2는 동일한 의미를 가지는 문장이다. paraphrased된 문장들이다.\nP1 문장은 Question에서 등장하는 핵심 단어들인 \u0026lsquo;selected\u0026rsquo;, \u0026lsquo;mission\u0026rsquo;이 등장하면서 문장의 구조도 매우 쉽다. 따라서 P1 문장을 context에서 찾을 수 있다면 question에 대해서 쉽게 답을 할 수 있을 것이다.\n하지만 P2 문장은 question에서 등장하는 단어들이 등장하지도 않을 뿐더러, 문장의 구조는 어려워졌다.\nMRC model은 P1, P2에서 모두 답을 찾을 수 있어야 한다.\nCoreference resolution # coreference는 상호간에 동일한 의미를 가지며 지칭되는 entity들을 의미한다. coreference resolution은 이러한 entity들을 같은 entity로 인식하는 것이다. ref: Blog\nUnanswerable questions # 지문만을 보고 답을 할 수 없는 경우는 분명 존재한다. 하지만 미숙한 모델일 경우 억지로 답을 도출해낼 수도 있다.\n따라서 unasnwerable question에 대해서는 답을 내릴 수 없다는 답을 내줘야 한다.\nMulti-hop resoning # 답을 찾기 위해서 여러 document의 supporting fact를 찾아야만 답을 할 수 있는 task.\ne.g., HotpotQA, QAngaroo 평가방법 # Extract Match, F1 score # 답변이 지문 내에 존재하는 경우(extractive answer)와 multiple-choice dataset의 경우에 사용하는 평가방법들.\nExtract match(EM) or Accuracy 예측한 답과 ground truth이 정확히 일치 비율 (Number of correct samples) / (Number of whole sampels) F1 score 예측한 답과 ground truth 사이의 token overlap을 F1 score로 계산 ROUGE-L, BLEU # descriptive answer를 위한 평가 방법.\nROUGE-L Score 예측한 값과 ground truth 사이의 overap recall BLEU 예측한 답과 ground truth 사이의 precision ","date":"12 October 2021","externalUrl":null,"permalink":"/posts/ml/2021-10-12-mrc/","section":"Posts","summary":"","title":"MRC","type":"posts"},{"content":" 10주차 학습정리 # 강의 복습 내용 # 개인 대회 회고글: https://velog.io/@naem1023/Relation-Extraction-%ED%9A%8C%EA%B3%A0\n과제 수행 과정 / 결과물 정리 # 대회 준비를 위한 server, wandb 등의 environment 세팅 모델 정의 모델 커스텀 및 실험 실험 재현 리더보드 제출 피어세션 정리 # 실험 내용 정의, 공유, 토론 학습 회고 # 21/10/05~08: 특강 수강.\n","date":"8 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-08-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-10%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 10주차 학습정리","type":"posts"},{"content":"부스트캠프에서 받은 서버와 aihub에서 받은 서버 2개를 사용 중인데, 종종 nvidia driver가 안 잡히는 문제가 있다.\n보통은 apt remove 후 재설치하면 되는데 안되는 경우가 있어서 난감했다. 특히 Ubuntu 16.04에서 이런 문제가 빈번했다. 16.04에서 nvidia-driver 설치를 하기 위해 구글링했던 내용들이 전부 먹히지 않았다.\nnvidia docs보니 해결됐다.\nhttps://docs.nvidia.com/datacenter/tesla/tesla-installation-notes/index.html\n","date":"6 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-06-nvidia-driver/","section":"Posts","summary":"","title":"nvidia-driver","type":"posts"},{"content":"ref: https://www.unixtutorial.org/disable-sleep-on-ubuntu-server/\n개인용 서버로 구매한 오드로이드가 가동 1년쯤부터 자꾸 꺼지기 시작했다. 단순히 전원 케이블이 잘못 꼽혀서 발생한 문제인줄 알았는데, 빈번하게 서버가 자주 꺼졌다.\n구글링 결과\n대부분의 사람들이 /var/log/messages 체크를 추천했지만 난 이러한 로그 파일이 전혀 존재하지 않았다. /var/log/messages에 적혀야 할 내용들은 dmesg에 있었다. 시스템 로그를 살펴보니 NetworkManager에 sleep 명령어가 전달되고 시스템이 종료됐다.\nNetworkManager[755]: \u0026lt;info\u0026gt; [1633287633.3651] manager: sleep: sleep requested (sleeping: no enabled: yes) NetworkManager[755]: \u0026lt;info\u0026gt; [1633287633.3661] manager: NetworkManager state is now ASLEEP ModemManager[809]: \u0026lt;info\u0026gt; [sleep-monitor] system is about to suspend 일반적인 시스템 로그: /var/log/syslog\n하드웨어적인 문제로 트러블슈팅되지 않아서 다행이었다. 리눅스 시스템 자동 sleep 관해서 찾아서 위에 첨부한 링크대로 수행했다. sleep.target service에서 loaded 부분만 바꿔준걸 보니 기존에 특정 스케쥴러들이 sleep.target에 관여하던 내용들을 지워준 것으로 보인다.\nsudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target 위 명령어 수행 후 다행히 아직까진 서버 이상 종료 현상은 발생하지 않고 있다.\n","date":"3 October 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-10-03-linux-server-sleep/","section":"Posts","summary":"","title":"Linux server sleep","type":"posts"},{"content":" 9주차 학습정리 # 강의 복습 내용 # 1~6번 포스팅: https://velog.io/@naem1023/series/NLP-code\n과제 수행 과정 / 결과물 정리 # 대회 준비를 위한 server, wandb 등의 environment 세팅 모델 정의 모델 커스텀 및 실험 실험 재현 리더보드 제출 피어세션 정리 # 실험 내용 정의, 공유, 토론 학습 회고 # 21/09/09: 특강 4개 수강. 21/09/10: 특강 4개 수강.\n","date":"1 October 2021","externalUrl":null,"permalink":"/posts/records/2021-10-01-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-9%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 9주차 학습정리","type":"posts"},{"content":"","date":"28 September 2021","externalUrl":null,"permalink":"/tags/bert/","section":"Tags","summary":"","title":"BERT","type":"tags"},{"content":" 모델 # ![](/assets/images/문장 토큰 분류/604bc451-9007-4544-8e84-40a229e90656-image.png)\n주어진 문장의 각 token들이 어떤 범주에 속하는 분류하는 task. classifier가 token마다 붙게된다.\nNER # Named Entity Recognition. 문맥을 통해 문서에서 인명, 기관명 같은 특정 의미를 가진 단어 / 어구 / 개체를 인식하는 과정.\n같은 단어라도 다양한 Entity로 인식될 수 있기 때문에 문맥을 파악하는 것이 중요하다.\nhttps://github.com/kakaobrain/pororo\n카카오에서 개발한 NLP, Speech-related task library다. 한국어로 처리 가능한 대부분의 task를 수행해주는 라이브러리인데, NER 또한 포함돼있다.\nPOS Tagging # Part-of-speech tagging.\n문서를 품사, 형태소로 분리 한국어 데이터 # kor_ner 한국어해양대학교에서 발표한 NER 데이터셋 NER 데이터셋은 보통 pos tagging 정보도 포함돼있고 kor_ner도 포함돼있다. BIO tag로 라벨링 wikidocs 학습 # ![](/assets/images/문장 토큰 분류/3aa9a28b-2465-4a76-874e-41d9c22ade6b-image.png)\n앞서 설명한 것처럼 token별로 classifier를 부착해서 학습을 수행한다.\n![](/assets/images/문장 토큰 분류/2cb82532-ad96-4ef1-a2f5-b741de436d52-image.png)\n** 문장 토큰 분류 시, 음절 단위의 tokenizing 추천! ** 형태소, 어절 단위로 분류하게 되면 Entity의 정의 자체가 모호해지는 경우가 있기 때문이다. 가령, \u0026lsquo;이순신은\u0026rsquo;을 \u0026lsquo;이순\u0026rsquo;과 \u0026lsquo;신은\u0026rsquo;으로 분류할 수도 있는데 \u0026lsquo;신은\u0026rsquo;은 아무리 학습을 해도 사람으로 분류하기 쉽지 않다.\n","date":"28 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-28-%EB%AC%B8%EC%9E%A5-%ED%86%A0%ED%81%B0-%EB%B6%84%EB%A5%98/","section":"Posts","summary":"","title":"문장 토큰 분류","type":"posts"},{"content":"","date":"28 September 2021","externalUrl":null,"permalink":"/tags/klue/","section":"Tags","summary":"","title":"Klue","type":"tags"},{"content":" 의존 구문 분석 # ![](/assets/images/KLUE 의존 구문 분석, 단일문장 분류/2f3d6ea6-828b-4bdc-ad3e-4aafd0f00625-image.png)\n지배소: 의미의 중심 의존소: 지배소가 갖는 의미를 보완(수식) 어순과 생략이 자유로운 한국어같은 언어에서 주로 연구 규칙 # 지배소는 후위언어 지배소는 항상 의존소보다 뒤에 위치 각 의존소와 지배소는 한 개씩 존재한다. 교차 의존 구조는 없다. 중첩은 된다. 즉, A가 누군가의 지배소라면 동시에 다른 단어의 의존소도 될 수 있다. 분류 방법 # Sequence labeling을 통해 분류.\n활용 # 복잡한 자연어 형태를 그래프로 구조화해서 표현 가능해진다. 각 대상(Entity)에 대한 정보를 추출할 수 있다. 단일 문장 분류 task # 주어진 문장이 어떤 분류에 속하는지 구분한다.\nSentiment Analysis(감정 분석) 문장의 긍정/부정/중립 등 성향을 분류 험오 발언 분류 기업 모니터링 Topic labeling(주제 분류) 문장을 category로 분류 대용량 문서 분류 VoC(Voice of Customer): 고객의 피드백을 분류 Language Detection(언어 감지) 문장이 어떤 나라의 언어인지 파악 번역기 데이터 필터링 Intent Classification 문장이 가진 의도를 분류 챗봇: 문장의 의도를 파악해 적절한 답변 생성 한국어 문장 분류를 위한 데이터 # Kor_hate 혐오 표현에 관한 데이터 욕설 말고 bias 표현 모음 Kor_sarcasm 비꼬는 표현 데이터 Kor_sae 질문의 유형에 관한 데이터 e.g., yes / no로 답변 가능한 질문 대안 선택을 묻는 질문 금지, 요구 명령 Kor_3i4k 의도와 관련된 데이터 문장 분류 모델 구조도 # ![](/assets/images/KLUE 의존 구문 분석, 단일문장 분류/9ad8aee3-cc16-4e21-9fe9-ca11aefd0de7-image.png)\nBERT를 기본으로 하고, CLS token에 classifier를 붙여서 문장 분류를 한다.\n사용될 parameters는 다음과 같다. 일반적으로 BERT에서 정해지는 설정 값들이다.\n• input_ids : sequence token을 입력 • attention_mask : [0,1]로 구성된 마스크이며 패딩 토큰을 구분 • token_type_ids : [0,1]로 구성되었으며 입력의 첫 문장과 두번째 문장 구분 • position_ids : 각 입력 시퀀스의 임베딩 인덱스 • inputs_embeds : input_ids대신 직접 임베딩 표현을 할당 • labels : loss 계산을 위한 레이블 • Next_sentence_label : 다음 문장 예측 loss 계산을 위한 레이블\n학습 과정 # Dataset 준비 Dataset 전처리, 토큰화 Dataloader 설계 Train, Test Dataset 준비 TrainingArguments 설정 Pretrained Model import Trainer 설정 Model 학습 Predict 구현, 평가 ","date":"28 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-28-klue-%EC%9D%98%EC%A1%B4-%EA%B5%AC%EB%AC%B8-%EB%B6%84%EC%84%9D-%EB%8B%A8%EC%9D%BC%EB%AC%B8%EC%9E%A5-%EB%B6%84%EB%A5%98/","section":"Posts","summary":"","title":"KLUE 의존 구문 분석, 단일문장 분류","type":"posts"},{"content":" 두 문장 관계 분류 task # 주어진 2개의 문장에 대해, 두 문장의 자연어 추론과 의미론적인 유사성을 측정하는 task.\n![](/assets/images/BERT 두 문장 관계 분류 task/9f79eea6-6b03-4120-bd92-3aea6143f05f-image.png)\n문장 분류와 유사하게 CLS token에 대한 classifier로 분류를 한다. 다른 점은 두 문장이 SEP token을 통해 함께 모델에 입력된다는 점이다.\nNLI # Natural language inference.\nLanguage model이 자연어의 맥락을 이해하는지 검증하는 task Premise(전제)와 Hypothesis(가설)을 다으모가 같이 분류한다. Entailment(함의): hypothesis가 true Contradiction(모순): hypothesis가 false Neutral(중립): hypothesis가 true인 것으로 추정되거나, 명백하게 판단하기 어려운 경우 Semantic text pair # 두 문장의 의미가 서로 같은 문장인지 검증하는 task\nIRAQ # Information Retrieval Question and Answering. 질문에 대해, 사전에 정의한 QA set에서 가장 적절한 답을 찾아내는 task. ![](/assets/images/BERT 두 문장 관계 분류 task/dabde1fc-11e7-4882-98b9-5c5ff0114093-image.png)\n기본적인 구조는 일반적인 챗봇과 같다. 사용자의 Query와 사전에 정의한 Query의 유사도를 비교해서 유사도가 높은 Query에 대한 답변을 반환한다.\n다른점은 모델의 마지막에 Paraphrase Detection이 붙는 점이다. 앞선 task를 통해 유사도가 높은 상위 n개의 답변이 준비될텐데, 여기서 가장 적절한 답을 찾는 모델이다. 의미론적으로 유사한지 추론하는 모델이 된다.\n","date":"28 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-28-bert-%EB%91%90-%EB%AC%B8%EC%9E%A5-%EA%B4%80%EA%B3%84-%EB%B6%84%EB%A5%98-task/","section":"Posts","summary":"","title":"BERT 두 문장 관계 분류 task","type":"posts"},{"content":" Process # Create Tokenizer Make Dataset NSP(Next Sentence Prediction) Masking Training # 앞서 배웠던 내용이랑 조금 상반되는 내용이라 일단 적어본다.\n도메인 특화 task에서는 Pretrained model을 fine-tuning하는 것보다, 도메인 특화 데이터만 사용해 새롭게 학습하(scratch)는 것이 더 성능이 좋다.\n![](/assets/images/Training BERT/54e501c8-a585-4e61-b0dd-0f09f295c423-image.png) ref: https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract\n생리학 저널들을 아카이빙하는 곳들 중 가장 큰 PubMed의 데이터를 사용해 새롭게 BERT를 학습한 논문이다.\n![](/assets/images/Training BERT/4e949c96-0eef-437c-a499-dcf459faa0b6-image.png)\n생리학 관련 task들에 대한 BERT들의 score다. 가령, BC5-chem은 화학 관련 개체명 인식 task다. 이러한 도메인 특화 task들은 fine-tuning보다 생리학 관련 데이터만 사용해 처음부터 BERT를 학습시키는 것이 성능이 좋다.\nData # ![](/assets/images/Training BERT/b2f590fc-3d8a-46d0-b00a-2e19fd8f2ffd-image.png)\nDataset # model에 들어갈 수 있는 형태로 data를 바꿔준다. BERT 입장에서는 다음과 같이 data를 새롭게 생성해 모델에 넣어줘야된다.\ninput_ids: Token Embedding을 통해 생성된 Vocab Id. token_type_ids: Segment Embedding을 통해 생성된 Segment Id. Positional encoding 정보 target_seq_length # https://github.com/huggingface/transformers/blob/5e3b4a70d3d17f2482d50aea230f7ed42b3a8fd0/src/transformers/data/datasets/language_modeling.py#L247\ngithub에 있는 BERT 코드다. 여기서 BERT의 Embedding size를 다음과 같이 조절한다.\nmax_num_tokens: BERT에 들어갈 수 있는 최대 token 개수 target_seq_length = max_num_tokens if random.random() \u0026lt; self.short_seq_probability: target_seq_length = random.randint(2, max_num_tokens) short_seq_probability에 의해서 확률적으로 target_seq_probability가 랜덤한 값을 가진다.\n이렇게 조절해주는 이유는 모델의 범용성 때문이다. 만약 max_num_token대로 모든 데이터를 꽉꽉 채워서 학습하면 해당 모델은 max_num_token 이외의 token 수를 입력받으면 제대로 처리하지 못할 가능성이 높다. 따라서 확률적으로 최대 Embedding size를 조절해줘서 유연한 모델을 만들고자 한다.\nSegment 조절 # https://github.com/huggingface/transformers/blob/5e3b4a70d3d17f2482d50aea230f7ed42b3a8fd0/src/transformers/data/datasets/language_modeling.py#L258\n258번줄부터는 Segment를 조절하기 위한 코드다. Dataset은 max embedding size를 꽉 채우는 데이터를 만들려고 노력할 것이다. 즉, \u0026lsquo;문장_1[SEP]문장_2\u0026rsquo;의 token size가 부족하다면 \u0026lsquo;문장_1+문장_2[SEP]문장_3+문장_4\u0026rsquo;과 같이도 만들겠다는 것이다. 물론 Segment는 여전히 2개다. \u0026lsquo;문장_1+문장_2\u0026rsquo;가 하나의 Segment가 되는 것이다.\n이 때, 코드에서는 랜덤하게 Segment A(첫번째 segment)의 길이를 자른다. 말 그대로 랜덤한 정수 값을 얻고 해당 값까지만 Segment로 사용한다.\nTruncation # https://github.com/huggingface/transformers/blob/5e3b4a70d3d17f2482d50aea230f7ed42b3a8fd0/src/transformers/data/datasets/language_modeling.py#L293\n\u0026lsquo;SegmentA[SEP]SegmentB\u0026rsquo;가 최대 Embedding size를 넘어설 수도 있다. truncation 작업이 필요한 조건이다.\ntruncation은 아래의 작업을 반복한다.\n랜덤하게 Segment A와 B 중 한 가지를 선택한다. Segment의 가장 뒤에 오는 token을 제거한다. token 개수를 검사하고 다시 truncation이 필요하면 1번으로 회귀. Dataloader # model에게 어떻게 data를 전달할 것인가를 결정. BERT 입장에서는 Masking을 어떻게 할 것인가에 대한 문제가 된다.\n","date":"28 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-28-training-bert/","section":"Posts","summary":"","title":"Training BERT","type":"posts"},{"content":"주재걸 교수님 강의에서 길게 풀어썻던 내용들이다. 스마게 김성현 강사님의 수업과 함께 BERT 모델을 다시 요약해보자.\nIntroduction # ![](/assets/images/BERT 응용/c3c6d777-d94f-4750-b8d6-c0a23993c127-image.png)\nLanguage model은 위와 같은 순서로 발전했다. 초기에는 Encoder와 Decoder를 분리해 각각 RNN으로 개발했다. Seq2Seq에 Attention을 도입해서 Decoder 시의 성능을 높이고, transformer에서는 이를 하나로 결합했다.\n** Image AutoEncoder ** ![](/assets/images/BERT 응용/5dc975bc-a5e6-41ec-9fd3-ee2b599848ac-image.png) AutoEncoder는 원본 이미지 재현, 복원하는 것이 목적이다. 즉, 네트워크는 원본 이미지를 재현하기 위한 정보를 압축해서 저장할 것이다.\n** BERT ** ![](/assets/images/BERT 응용/43454042-079c-4bc8-a830-a5f3c57d4dc2-image.png) BERT 또한 AutoEncoder와 같이 원본을 재현, 복원하는 것이 목적이다. 다만 문제의 난이도를 위해 원본에 Masking을 해서 재현, 복원하고자 한다. 원본은 자연어 자체를 의미하기 때문에 네트워크는 자연어에 대해서 학습하도록 노력하 것이다.\n** BERT , GTP ** ![](/assets/images/BERT 응용/4c3f4be7-eef3-4a54-8d5f-35c721c944c1-image.png) GPT-1이 BERT보다 먼저 나왔다고 한다.\nGPT-1: Transformer decoder만을 활용해 자연어 학습 BERT: Trasformer encoder만을 활용해 Masking된 자연어 학습 GPT-2: Transformer decoder만을 활용. sequence의 특정 지점 이후를 제거하고, 이후의 내용을 추론하도록 학습 BERT # BERT를 학습하는 과정은 아래와 같다. 이를 통해 Pre-trained BERT를 얻는다. ![](/assets/images/BERT 응용/78580aee-2eab-4188-8d4a-634fbd5d00cf-image.png)\nSentence 1, 2가 [SEP] token으로 묶여서 들어간다. [CLS] token에는 sentence 1, 2가 next 관계인지에 대한 정보가 담겨있다. [CLS] token vector에는 입력된 sentence의 정보가 모두 녹아서 embedding 된다! [CLS] token에 classifier를 붙여서 next 관계를 파악하는 classification 학습을 한다. Data 처리 # Word Tokenizing BERT는 WordPiece tokeninzing을 사용한다. 즉, 빈도 수를 기반으로 tokenizing을 한다. Sentence 첫번째 문장의 다음 문장은 next sentence거나 random chosen sentence를 사용한다. Masking token들은 15%의 확률로 Masking 대상이 된다. Masking 대상 중 8:1:1의 비율로 Masking, Randomly replacing, Unchanging을 선택한다. BERT 응용 # GLUE, KLUE 등은 통해 Benchmark를 해보자.\n대표적인 Benchmark는 아래와 같다. ![](/assets/images/BERT 응용/2353a73e-49ae-49b5-8253-65542b5ba94c-image.png)\n단일 문장 분류 1개의 문장 입력에 대한 분류 두 문장 관계 분류 두 문장의 유사도, 관계, next 관계 등을 분류 문장 토큰 분류 각 token마다 classifier를 부착해서 token을 분류한다. NER(Named entity recognition) 기계 독해 정답 분류 질문과 질문의 정답이 포함된 문서가 주어진다. 문서에서 정답의 위치를 파악 각 Benchmark들의 예시들은 다음과 같다.\n감성 분석 # ![](/assets/images/BERT 응용/e6d28e3d-e774-423e-ba79-032574bcc259-image.png)\n입력된 문장에 대한 긍정, 부정을 판단하는 task. 2018년에 등장한 BERT 이전에도 이러한 task를 수행해야했고 보통 85%의 acc를 보여줬다. BERT 이후로는 91% acc는 나와야 잘 나왔다고 한다.\n관계 추출 # ![](/assets/images/BERT 응용/5fade764-73d5-4e3e-a275-bf090bdf2069-image.png) Entity라는 관계 추출의 대상을 정하고 Entity들 간의 관계를 추출하는 task다. 가령, subject인 \u0026lsquo;이순신\u0026rsquo;과 object인 \u0026lsquo;무신\u0026rsquo;이라면 이에 대한 관계는 \u0026lsquo;직업\u0026rsquo;일 것이다. 한국어 데이터 기준으로 성능이 좋지 않았던 task도 BERT가 훌륭하게 수행했다.\n의미 비교 # ![](/assets/images/BERT 응용/ff6a2d90-04a4-4614-810d-38218acc8771-image.png) 두 문장 사이의 의미적인 유사도를 판단하는 것이다. 즉, 의미가 비슷한 문장을 뽑아주는 task다.\n이 때, 유사도가 높은 문장들을 통해 학습 데이터를 구성해야 한다. 왜냐하면 유사도가 너무 다른 학습 데이터들만 있다면 모델이 학습할만한 요소는 \u0026lsquo;사용되는 단어의 차이\u0026rsquo;일 뿐이다. 하지만 사용되는 단어가 비슷할지라도 분명 의미가 다른 문장들이 있다. 따라서 유사도가 높은 문장들로 학습 데이터를 구성하도록 한다.\n** 이러한 유사도가 높은 학습 데이터만을 사용하는 것은 전처리, 데이터 설계의 문제다. **\n개체명 분석 # ![](/assets/images/BERT 응용/fe16fe87-ba56-44e7-ae2a-9d854c177845-image.png)\n기존에는 SVM과 같은 전통적인 ML을 통해 NER을 수행했는데, BERT를 사용하면 SVM보다 월등하게 성능이 좋다.\n기계 독해 # ![](/assets/images/BERT 응용/66f5f38a-22f0-4523-be66-a0c3ee0f6431-image.png)\n기존에는 거의 유일하게 리더보드를 가지고 있는 KorQuAD라는 Benchmark를 통해서 MRC task를 평가했다. 2021년 현재는 KLUE가 발표되면서 KLUE를 쓰면된다!\nTokenizing에서 어절 단위는 음절 단위보다 성능이 월등히 나빴다. 왜냐하면 의미론적으로 같은 단어라도 어절 단위 tokenizing에서는 다른 token으로 분류되기 때문이다. e.g., \u0026lsquo;이순신\u0026rsquo;과 \u0026lsquo;이순신은\u0026rsquo;\n한국어 BERT # ETRI KoBERT # ![](/assets/images/BERT 응용/6bf71531-8402-48f7-8190-8b9683609d37-image.png)\nKoBERT는 바로 WordPiece를 수행하지 않고, 형태소 분리를 한 후 WordPiece를 수행한다. 이는 의미를 가진 최소 단위로 Tokenizing을 하겠다는 의도였다. 실제로도 성능이 매우 좋아 출시되자마자 바로 KorQuAD 1등을 한달 이상 찍었다. 구글 모델보다 10점 이상 좋은 점수였다고 힌다.\nSKT KoBERT # https://github.com/SKTBrain/KoBERT SKT에서도 한국어 BERT인 KoBERT를 출시했다. 이것은 형태소 단위로 분리하지 않고 바로 WordPiece를 수행했다.\n차이점 # KorQuAD에서 ETRI KoBERT는 SKT KoBERT보다 성능이 더 좋다. 따라서 performance적인 측면에서는 ETRI 모델이 더 좋다. 하지만 이를 사용하기 위해서는 ETRI 모델에 적합한 형태로 데이터를 전처리해야하는 수고로움이 존재한다. 바로 사용하기에는 SKT 모델이 편할 것이다.\nTokenizing에 따른 성능 차이 # ![](/assets/images/BERT 응용/0fbfdaec-4a34-4d1a-8805-7f46a124d215-image.png) ref: https://arxiv.org/abs/2010.02534 카카오와 스캐터랩(이루다를 만든 그 회사다)이 합작해서 만든 논문이다. 논문은 한국어 Tokenizing에 따른 성능을 비교했다.\n논문은 가장 성능이 좋았던 방법론은 형태소 분리를 하고 WordPiece를 수행한 Morpheme-aware Subowrd라고 했다.\nAdvanced BERT # 김성현 강사님의 경험담이라고 하셨다. KorQuAD에서 BERT가 정답을 만드는 feature는 Entity일 것이다. 하지만 BERT 내에는 근본적으로 Entity를 명시할 수 있는 구조가 없다.\n따라서\nEntity linking을 통해 주요 entity를 먼저 추출한다. 1번에 대해 Entity tag 부착 Entity embedding layer 추가 형태소 분석을 통해 NNP와 entity를 우선으로 하여 chunking masking을 한다. ![](/assets/images/BERT 응용/90e6b330-8aa4-499b-b028-1d32b8d7bcce-image.png)\n즉, 기존의 Token Embedding, Segment Embedding, Position Embedding 외에 Entity Embedding이라는 개념을 Embedding layer 단에 추가하신거라고 하셨다.\n이렇게 하니 KorQuAD에서 더 좋은 점수를 얻었다고 하신다. ![](/assets/images/BERT 응용/78c5c2d9-0915-493c-9574-ac0150e9a4e8-image.png)\n이러한 방법론이 영미권에서는 ERNIE라는 모델로 제시됐다고 하고 현재 SOTA model이라고 한다. Blog\n","date":"27 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-27-bert-%EC%9D%91%EC%9A%A9/","section":"Posts","summary":"","title":"BERT 응용","type":"posts"},{"content":"","date":"27 September 2021","externalUrl":null,"permalink":"/categories/computer-science/","section":"Categories","summary":"","title":"Computer-Science","type":"categories"},{"content":"","date":"27 September 2021","externalUrl":null,"permalink":"/tags/regex/","section":"Tags","summary":"","title":"Regex","type":"tags"},{"content":"import re password_checker = re.compile(\u0026#34;^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[!@#$%^\u0026amp;*])[A-Za-z\\d!@#$%^\u0026amp;*]{8,30}$\u0026#34; ) mat = re.search(password_checker, password) 쓰는 것만 써서 이렇게 새로운 형태를 만나면 굳어버린다. 이 참에 연습할겸 분석을 했다. ref: Blog, COGNEX, Mozilla\n^: 행(라인)의 시작 $: 행의 가장 마지막을 의미 |: and \\t: tab \\n: 줄 바꿈 .: 모든 문자를 의미 []: 대괄호 내의 모든 문자를 순서를 고려하지 않고 찾고자 한다. e.g., [a-ZA-Z가-힣] [^]: 대괄호 내의 모든 문자를 제외하고 찾고자 한다. e.g., [^a-Z]: 알파벳 소문자를 제외하고 검색 *: 0번 이상의 문자 \u0026lsquo;*\u0026rsquo; 앞에 오는 문자를 반복한다. e.g., \u0026lsquo;bo*\u0026lsquo;는 \u0026lsquo;b\u0026rsquo;도 되고 \u0026lsquo;booooo\u0026rsquo;도 된다. ?: 물음표 앞에 오는 문자가 0 혹은 1개 존재함을 의미 x(?=y): lookahead라고 한다. 오직 y가 뒤따라오는 x에만 대응됨을 의미한다. Blog \\d: 0-9와 동일. 숫자를 의미한다. 적용 # import re password_checker = re.compile(\u0026#34;^(?=.*[a-z])(?=.*[A-Z])(?=.*\\d)(?=.*[!@#$%^\u0026amp;*])[A-Za-z\\d!@#$%^\u0026amp;*]{8,30}$\u0026#34; ) mat = re.search(password_checker, password) 위 정규표현식은 다음의 조건을 충족시키는 문자를 찾기 위한 것이다.\n8자리 이상 30글자 미만 영어 대문자와 소문자 최소 1개씩 포함 하나 이상의 숫자 포함 하나 이상의 특수기호 (!@#$%^\u0026amp;*) 포함 조건 검사 # (?=.[a-z])(?=.[A-Z])(?=.\\d)(?=.[!@#$%^\u0026amp;*])은 이러한 조건들을 하나씩 체크하는 것이다. 즉, 4개의 조건 중 1개라도 거짓일 경우 4개의 그룹은 모두 거짓으로 계산될 것이다.\n아래의 블로그 포스팅들을 보면 마치 AND 연산처럼 여러개의 조건을 이어붙인거라고 할 수 있다.\nAND Negative lookahead ","date":"27 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-27-regex-%EC%97%B0%EC%8A%B5/","section":"Posts","summary":"","title":"Regex 연습","type":"posts"},{"content":" Language modeling # Seq2Seq task다. 주어진 문맥을 활용해 다음 단어를 예측하는 task.\n![](/assets/images/Language model Benchmark 간단 정리/c4574267-c36a-47a2-8735-213136b0523f-image.png)\n특정 시점의 문장에 대한 다음 단어가 나타날 확률을 예측하는 task로도 생각할 수 있다.\nRNNs # ![](/assets/images/Language model Benchmark 간단 정리/eade94db-783b-4d14-97ce-ba671fdb5f24-image.png)\nSequence의 순서대로 model에 sequence를 입력한다. 이전 hidden state를 활용해 다음 step의 input으로 활용한다.\nRNN은 task를 위해서 설계된 모델이기 때문에, 특정 task에서만 잘 작동한다. e.g., Seq2Seq model은 Seq2Seq만 처리.\nBidirectional Language Modeling # ELMo # 양방향 언어 모델링. 이러한 개념을 처음 제시한 것은 ELMo(Embeddings from Language Models)다.\n자연어를 Embedding 후 언어 모델링 task를 수행하는 것 자체가 다른 NLP task를 처리할 수 있다는 가능성을 보여줬다.\n![](/assets/images/Language model Benchmark 간단 정리/76453581-7a9e-45d5-81f6-df575506d647-image.png)\n위 도식의 오른쪽 그림과 같이 ELMo는 Forward, Backward 두 방향으로 Language model을 수행한다.\n![](/assets/images/Language model Benchmark 간단 정리/a7b68d9f-1073-4494-afc5-4383d18764ed-image.png)\n기존에는 SQuAD(질의응답), SNLI(문장 사이의 모순 파악), SRL(의미역 결정), Coref(Entity 찾기, Blog), NER(Entity 인식), SST-5(문장 분류)에 특화된 모델이 별도로 존재했다.\nELMo는 하나의 Language Model을 통해서 6개의 task를 모두 수행했고 정확도 또한 유의미함을 보여줬다.\nBERT # Bidirectional Encoder Representations from transformers. Transformer를 사용해 양방향 언어 모델링을 수행한 논문이다. ![](/assets/images/Language model Benchmark 간단 정리/1118b4bd-da20-492a-8b94-7282d751cf00-image.png)\n대량의 corpus에서 encoding, decoding transformer를 사전 학습하고 이를 통해 Embedding을 추출한다. ELMo에서는 두 개의 RNN을 통해 Bidirectional Encoder르 구현했다면 BERT는 transformer를 통해 양방향 학습을 동시에 진행한다.\n![](/assets/images/Language model Benchmark 간단 정리/5ee98e48-41c5-4b9c-9134-dc97ef254a51-image.png)\n![](/assets/images/Language model Benchmark 간단 정리/2cfb4633-fe7c-4b83-acc7-c019d407509c-image.png) ELMo와 동일하게 Language modeling을 통해서 여러 nlp task를 수행할 수 있다는 것을 보여줬다.\n![](/assets/images/Language model Benchmark 간단 정리/0b9ec46d-9a05-4ac1-86a4-510ea9b1d20e-image.png)\n기존에도 single langauge model만으로 여러 nlp task를 처리하고자 했지만 BERT의 성능이 제일 좋았고 사용법도 보다 쉬워졌다.\nGLUE와 SQuAD 1.1, 2.0에서 효과적으로 동작함을 보여줬다.\nGLUE # General Language Understading Evaluation. Language model이 자연어를 얼만큼 이해하는지 평가하는 데이터셋과 task 정의.\n표준화된 데이터셋과 이에 대한 nlp task는 정의 덕분에 BERT와 이후의 모델 평가가 매우 용이해졌다. Facebook의 RoBERTa, Stanford의 ELECTRA, Google의 ALBERT 등 모두 이러한 체계 아래서 공정하게 평가가 이뤄졌다.\nGLUE Benchmark는 아래와 같다. ref:https://vanche.github.io/NLP_Pretrained_Model_BERT(2)\nMNLI(Multi-Genre Natural Language Inference): entailment classification task QQP(Quora Question Pairs): Quora에 올라온 질문 페어가 의미적으로 동일한지 확인하는 테스크 QNLI(Question Natural Language Inference): SQuAD의 이진분류 버전. paragraph가 answer를 포함하는지 안하는지 확인하는 문제. SST-2(Stanford Sentiment Treebank): 단문장 이진분류문제. 영화리뷰에서 추출된 문장에 감정이 표기되어있음. CoLA(Corpus of Linguistic Acceptability): 영어문장이 언어학적으로 acceptable한지 확인하는 이진분류문제 STS-B(Seemantic Textual Similarity Benchmark): 문장쌍이 얼마나 유사한지 확인하는 문제. MRPC(Microsoft Research Paraphrate Corpus): 문장쌍의 유사성 확인하는 문제. RTE(Recognizing Textual Entailment): MNLI와 유사하나 데이터가 적음. WNLI(Winograd NLI): 자연어 추론 데이터셋이나 현재 채점에 이슈가 있어서 BERT 실험에서는 제외됨. ![](/assets/images/Language model Benchmark 간단 정리/cccf4ba8-a0b8-46f6-9f2e-fcc534e6477b-image.png)\nGLUE는 Benchmark를 통해 지속적으로 모델을 발전시킬 수 있는 계기가 됐다.\n자연어 생성 Benchmark # ![](/assets/images/Language model Benchmark 간단 정리/5372dd0d-92c6-4a22-abaf-0b8a475d4507-image.png)\nGLUE 외에도 Google의 T5(Blog), Facebook의 BART(Blog) 등의 Langauge model 평가를 위한 Benchmark가 생겼다. Benchmark를 통해 자연어를 얼만큼 잘 생성하는지를 평가할 수 있는 Benchmark들이다.\nMasking되거나 noise가 발생된 자연어에 대한 복원을 encoder가 하지 않고 decoder의 output을 통해 복원하도록 한다. 즉, decoder까지 pre-train의 범주에 넣어서 사용하도록 했다.\n다국어 Benchmark # GLUE가 영어이기 때문에 다른 언어들은 영어 기반의 접근법읠 활용할 수 밖에 없었다. 해당 언어의 특성이 고려되지 않았기 때문에 영어 기반 접근법을 수정하거나 해당 언어에 특화된 방법론을 따로 고안해야하는 비효율적인 과정이 발생했다.\n따라서 다국어 Benchamark들이 등장하면서 해당 언어들에 특화된 Benchmark를 직접 고안하고평가하기 시작했다.\nFLUE: 프랑스어 CLUE: 중국어 IndoNLU benchmark: 인도네시아 IndicGLUE: 인도어 RussianSuperGLUE: 러시아어 한국어 Benchmark # KLUE(Korean Lanuguage Understading Evaluation)\n개체명 인식(NER, Naemd entity Recognition) 정해진 카테고리에 따라 단어를 분류 품사 태깅 및 의존 구문 분석(POS tagging, Dependency Parsing) 단어들의 품사 파악 단어들 간의 의존 관계 분석 문장 분류(Text classification) 자연어 추론(Natural Language Inference) 두 문장이 모순 관계인지, 설명 관계인지 등을 파악 문장 유사도(Semantic Textual Similarity) 관계 추출(Relation Extraction) 질의 응답(Question \u0026amp; Answering) 목적형 대화(Task-oriented Dialogue) ","date":"26 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-26-language-model-benchmark-%EA%B0%84%EB%8B%A8-%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"Language model Benchmark 간단 정리","type":"posts"},{"content":"","date":"26 September 2021","externalUrl":null,"permalink":"/categories/ai/","section":"Categories","summary":"","title":"AI","type":"categories"},{"content":" Bias # 미국 Northpointe社의 재범가능성(recidivism)을 예측하는 COMPAS는 아래와 같이 인종, 성별에 관해서 편향된 추측을 하는 경향과 법률적 근거가 모호해 폐기된 전례가 있다. ![](/assets/images/AI \u0026amp; Ethics/12702dce-9ede-49d5-86ba-785eb013d5a4-image.png)\n개인적인 생각으로 COMPAS는 법적 설득력의 부족과 잘못된 모델 설계의 문제로 인해 발생된 Bias 이슈라고 생각된다.\n특정 성별, 인종, 종교에 편향되어 데이터를 분석했다면 해당 모델의 데이터 분석 방법론이 잘못된 것이지 개발자들의 신념에 따른 프로파간다가 아니기 때문이다. 왜냐하면 Imbalanced class distribution을 잘못 처리했고 이를 모델이 그대로 학습했다면 모델의 신뢰성은 보장할 수 없을 뿐더러, 모델의 추정 또한 정확하지 않기 때문이다.\n중요한 것은 COMPAS의 개발 과정과 사용 데이터는 **영업 비밀(trade secret)**이다. 관련 기사 즉, 개발자가 인종차별적인 혹은 젠더 관련 신념에 의해서 Biased model을 개발한 것인지 혹은 단순 통계적 실수인지 모른다는 것이다.\n또한 링크에 걸어둔 변호사님의 언급과 같이 적법절차원칙위반이라고 한다. 영업 비밀을 통해 채택된 증거는 피고인의 방어권이 발동될 여지가 없고 재판부 또한 판결의 근거로 사용할 수 없기 때문이다.\n미국 NSTC(국가과학기술위원회)의 보고서 ‘Preparing For The Future Of Artificial Intelligence’는 ‘Fairness’에 관하여 한 챕터를 할애할만큼 Blackbox 형태의 학습 방법은 대중에게 알 권리를 보장하지 못한다. 따라서 이러한 Blackbox 형태로 인해 발생하는 인종, 성적 차별적 형태가 일어나지 않았음을 보증하는 것이 어렵고, 보증하기 위해 노력할 사안임은 분명하다.\nBias 관련 연구 # Big data에서 Bias가 어떻게 발생하는지 연구되는 법학적 논문도 있다고 한다. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2477899 논문의 abstact는 문제의 근원을 알기 어렵기 때문에 정확한 solution을 찾는 것이 어렵다는 내용이다.\n알고리즘은 데이터에 기반할 수 밖에 없다. 가령, 데이터가 편향됐다면 출력도 편향될 것이다. 이전 판사들의 판결 결과들이 편향됐다면, 알고리즘도 편향된 판결 결과를 내놓을 것이다. 즉, 사회적 편향성이 알고리즘에도 반영이 될 것이다. 사회적 데이터들에 소수자, 약자들에게 불리한 패턴이 존재할 수 있다. 하지만 이에 대한 정확한 데이터의 근원을 아는 것은 어렵다. (?) 개발사들이 bias한 모델 개발을 의도하진 않았지만 bias한 모델이 개발될 여지가 있다. Define target variable and class labels # Target variable과 Class label을 정의할 때, 여기서부터 bias가 발생할 수 있다고 논문에서는 말한다. 가령, good employee를 정의해보자.\n근속년수 하루 근무 시간 생산성 다른 근무자와의 관계 이러한 variable과 class의 조건을 정의할 때 bias가 발생할 수 있다는 것이다. 생산성을 어떻게 정의할 것인지, 하루 근무 시간을 어떻게 정의할 것인지에는 극히 주관적인 요소들이 포함될 수 있기 때문이다.\nLabeling # LinkedIn Talent Match에서 Employee에 대한 평가는 Employer가 한다. Employer들이 implicityly biased할 수 있다. 이러한 평가 데이터가 학습 데이터에 반영될 수 있다. (?)\nCollection # 데이터 수집 자체에서 bias가 반영될 수 있다는 주장이다.\nUnerrepresentation 데이터가 취약계층을 대변하지 못하는 경우들이 있다. 가령, 보스턴 시에서는 시민들이 도로 파손 상태를 사진으로 찍어서 보고하여 수리를 하는데, 취약계층이 밀집한 지역에서는 스마트폰 보급이 안되서 이러한 보고가 제대로 이루어지지 않았다. Overrepresentation 데이터가 취약계층을 과다하게 반영 취약계층인 고용인의 활동이 고용자에게는 유독 주목받는 경우가 있을 수 있어서 객관적인 평가가 안될 수 있다. Feature selection # Feature 자체에 bias가 반영돼있다는 주장이다. 이를 극복하기 위한 대표적인 예시가 블라인드 채용이다. redlining: general criteria를 보는 것. 개인의 평가보다 주변 환경에 대한 평가.\nProxies # Unintentional discrimination 의도하지 않았지만 모델이 스스로 bias한 패턴을 찾아내는 경우 Intentional discrimination 설계자가 의도해 bias를 알고리즘에 투입 Bias metrics # ![](/assets/images/AI \u0026amp; Ethics/cebc13d0-9abf-4940-a53a-b60ac8401221-image.png) Source: May, et al. NAACL 2019\nNLP model에서 European American names는 긍정적인 단어와 context상 더 적절하고, African American names는 부정적인 단어와 context가 더 적절하게 학습되는 것이 알려져 있다고 한다.\n![](/assets/images/AI \u0026amp; Ethics/626691f1-c2b7-457a-9294-da78f20313e9-image.png) 위 도식과 마찬가지로 sentence to sentence로 긍/부정 평가를 했을 때도 bias를 가지고 있다는 것이 해당 논문의 주장이다.\nBias 결론 # 민감한 주제이기 때문에 다른 포스팅보다 더 신경써서 다뤘다.\nCOMPAS는 AI와 Bias 사례를 소개할 때 설명하기 가장 적절한 예시이다. Bias data를 학습해 prediction 결과가 좋지 않았고, Blackbox 형태의 모델은 법률 증거로 채택될 설득력도 없으며, 피고인의 방어권을 보장할 수 없기 때문이다.\n하지만 Bias된 결과에 대해서 집착하지 않았으면 하는 개인적인 생각이 있다. 통계적, 개발론적으로 완벽한 방법론을 설계했다면 그 결과에 대해서 Bias를 따지는 것은 무의미하다.\n사회에 잘못된 Bias는 분명 존재하고 AI model이 이를 학습해서 사회의 잘못된 Bias를 강화시키는 것은 지양하는게 타당하다. 하지만 \u0026lsquo;이루다\u0026rsquo;와 같이 법률적, 통계적 근거가 아닌 프로파간다를 위한 분석이 사회에 만연했던 사례도 분명 존재한다.\n\u0026lsquo;이루다\u0026rsquo;가 가졌던 문제점들은 여러 기사들을 통해 지적됐다. 기사 링크 염두할 것은 이루다 서비스가 가졌던 개인정보보호와 법률적 문제점이지 결코 Bias된 모델에 대한 것이 아니다. 일부 집단들의 프로파간다로써 이루다 이슈가 묻힌 것은 AI를 공부하는 사람으로서 아쉬운 부분이다.\n즉, 통계적, 법률적 관점을 고려하지 않고 모델의 결과만을 통해 모델이 Bias하게 개발됐다고 하는 것은 어폐가 있다고 생각한다.\nPrivacy # 싱가포르의 COVID-19 방역을 위한 위치추적 앱인 \u0026lsquo;TraceTogether\u0026rsquo; Privacy에 대해 분석하고 개선점을 제안한 논문이다. ref: https://arxiv.org/pdf/2003.11511.pdf\n논문에서 주장하는 Privacy 관련 이슈는 아래와 같다.\n사용자의 디바이스에는 본인의 정보만이 있다. 하지만 모든 사용자의 정보는 개인을 식별할 수 없는 random string으로 가려진 후 central server에 저장된다. Privacy from snoopers 개개인의 정보는 매번 바뀌는 random string을 통한 식별자로 관리되고 개인 디바이스에는 개인의 정보만이 있다. 따라서 snoofing의 위험이 있다고 할지라도 privacy에 관한 이슈는 크지 않다. Privacy from contacts 개인이 COVID-19 환자와 접촉했음이 감지된다면, 앱은 싱가포르 정부에게 해당 사실을 알린다. 하지만 개인 정보를 추적할만한 정보를 넘겨주지는 않는다. (이름, 성별 등) ** 개선점 ** 기술적인 관점에서 Privacy 보호를 위한 개선점을 논문에서는 아래와 같이 제시했다.\n분산 서버에 개인 정보 저장 Cryptography-based solution을 쓰자 random noise를 추가하는 것은 privacy 보호에 도움이 안된다. ** 한국 ** 한국은 다른 나라에 비해 확진자의 수가 많지 않기 때문에 확진자의 정보를 모두 공개한다. Privacy 측면에서 좋지 않은 방법이다. 또한 확진자가 많다면 의미가 없는 방법이다.\nSocial Inequality # 백악관에서 AI가 10년 내에 사회적으로 미칠 영향을 정리한 보고서이다. 2016년, 2018년, 2019년 보고서가 있고 ref는 2016년 보고서이다.\nref: https://ainowinstitute.org/AI_Now_2016_Report.pdf\n보고서의 내용은 아래와 같다.\nAI가 사회적으로 중요한 결정을 내릴 수도 있다.\nHousing Health insurance: 특정 질병, 유전력이 있다면 건강보험 가입 시 돈을 더 내거나 가입 불가 통보를 받을 수 있다. 기존의 건강보험도 마찬가지의 결정을 하지만, AI는 보다 잠재적인 질병, 유전력을 미리 감지해 판단할 여지가 있다. ** Benefits **\nAI를 잘 활용하는 직군들(개발자, 금융 등) 대규모 자원을 가용할 수 있는 집단 ** Harmed**\nIT를 접근하지 못하는 취약계층 소규모 집단, 학교 Labor # 피고용자의 일자리 감소는 필연적일 것이다. 또한 피고용자에 대한 일괄적이고 시스템적인 관리가 더욱 체계적으로 이루어질 때 피해를 입는 사례가 발생할 여지가 있다. e.g., Uber\nMisinformation # News GPT-3와 같이 사람과 같은 글을 쓰는 language model은 가짜 뉴스의 폭발적인 생산 가능성을 가지고 있다. ref: https://tinkeredthinking.com/?id=836\nDeepfakes ref: https://arxiv.org/pdf/2001.00179.pdf 1세대 GAN이 어색한 이미지를 생성했던 것에 비해 2세대 이상의 GAN에서는 매우 자연스러운 이미지를 생성한다. 이에 대한 피해를 막고자 face replacement detection에 대한 논문이다.\nIdentity # Identity Prediction ref: https://www.pnas.org/content/pnas/110/15/5802.full.pdf 사용자의 페이스북 좋아요를 분석해서 사용자에 대한 정보(age, gender, political)를 예측하는 논문이다.\n![](/assets/images/AI \u0026amp; Ethics/a20d8724-5198-424a-8902-8fff6ffd8ada-image.png)\n꽤 다양한 정보에 대해서 예측이 가능하고, 의미 있게 예측한다.\n** Detecting cheaters in coding test ** ref: https://dl.acm.org/doi/abs/10.1145/3386527.3406726 ![](/assets/images/AI \u0026amp; Ethics/fe3ff752-4b41-4c89-86b9-551e700b236b-image.png) 코테에서 부정행위자를 검출하기 위한 논문이다. Keystroke를 감지해서 사용자를 분류한다고 한다.\nHealth # ** Early detection ** 당뇨병 환자 중 시력을 잃는 합병증에 걸리는 사례가 있는데, 이러한 경우를 사전에 빠르게 탐지하는 기술이 개발됨. ref: https://irisvision.com/diabetic-retinopathy/\n** 영상 판독 ** ref: https://youtu.be/Mur70YjInmI 영상 판독을 통한 암 판별에도 사용되어 진료에 도움이 된다고 한다. 개인적으로 이러한 사례에 대해서 CV를 통해 절대적인 해결방법을 찾았다고 생각하는 개발자들이 있는데, 나는 이러한 사례들이 아직까지는 보조 도구로써만 사용되어야 한다고 생각한다. 실제로 현재 법률 제한 또한 의학적 판단도구보다는 의학 관련 보조 도구로써 분류된다.\n** COVID-19 Detection ** ref: https://www.nature.com/articles/s41591-020-0931-3 여러 DL 모델을 조합해 COVID-19를 탐지하는 모델에 대한 논문이다.\nClimate change # ** $CO_2$ 배출 ** ref: https://s10251.pcdn.co/pdf/2021-bender-parrots.pdf AI와 환경오염과 관련되어서 빠지지 않는 이슈다. 한 사람이 1년에 대략 5t의 $CO_2$를 배출한다고 했을 때, 일반적인 거대 transformer를 한번 학습할 때 284t의 $CO_2$를 배출한다고 한다. 따라서 연구자들은 효율성, 성능과 더불어 engergy cost에 대해서도 고려해야 된다는게 논문의 주장이다.\n** 환경에 도움이 되는 경우 ** ref: https://arxiv.org/pdf/1906.05433.pdf 긴 논문이지만 AI의 사용처에 대해 요약하면 다음과 같다고 한다.\n탄소 연료를 사용하지 않는 경우에 대한 solution 개발 기존 자원에 대한 소비를 효과적으로 줄이는 solution 개발 전지구적으로 이러한 solution들을 어떻게 실행할지에 대한 계획 개발 교통수단의 에너지 효율성 증대(전기차) 거대 운송수단(기차, 비행기)등의 효율적인 운송시간표를 계획해 연료소비 감소 온수 사용 시간을 분석해 효과적으로 온수 공급 공용자전거의 배치를 수요에 따라 배치해서 자전거 이용량을 늘리는데 사용 Uber의 동선을 최적화해서 짧은 동선을 게획 ![](/assets/images/AI \u0026amp; Ethics/4218cae7-cb2a-464f-85dc-85cef50c65c7-image.png)\n가령, 전기 소비에 대한 forecast가 효과적으로 이루어진다면 필요한 자원만큼만 배분할 수 있다. 전기와 관련된 이야기이기 때문에 자원 배분에 초점을 맞춘다기보다는 전기 생산량을 조절할 수 있다는 것이 요지인거 같다. 댐처럼 전기를 저장하는데 용이한 SoC가 존재하는 것이 아니니까..\n![](/assets/images/AI \u0026amp; Ethics/faeb6bb5-c874-44b7-8ba9-a542309fee04-image.png) ref: https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40\nDeepmind에서 Google data center의 냉방 효율성에 대해서 기고한 포스팅이다. ML 관련 작업을 할 때와 하지 않을 때의 스케쥴러를 ML을 통해 분석해서 냉방 시에 소모되는 에너지를 줄이고자 하는 것이 목적이다.\n","date":"26 September 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-09-26-ai--ethics/","section":"Posts","summary":"","title":"AI \u0026 Ethics","type":"posts"},{"content":"","date":"26 September 2021","externalUrl":null,"permalink":"/tags/ethics/","section":"Tags","summary":"","title":"Ethics","type":"tags"},{"content":" ML Engineer # ML/DL을 이해, 연구하고 Product을 만드는 Engineer. ![](/assets/images/Full stack ML Engineer/eb9eb9c1-bc28-40c0-ac2f-ac3c54855c8f-image.png)\nResearcher와 Engineer의 사이에서 모호한 위치에 있다. 발전속도가 워낙 빨라서 연구와 동시에 Product에 적용할 사례가 많기 때문이다.\nFull stack Engineer # ![](/assets/images/Full stack ML Engineer/cc790f50-9fe5-4605-9946-f9fc4af539f5-image.png) Front-end, Back-end를 모두 개발 가능한 Engineer. Product을 만들 시간만 있다면 모두 혼자 개발 가능한 개발자.\nFull stack ML Engineer # DL Research를 이해하고 ML Product를 만들 수 있는 Engineer.\nBack-end에서의 ML ![](/assets/images/Full stack ML Engineer/4486a601-8052-4c92-8602-da7faf181f65-image.png)\nFront-end에서의 ML ![](/assets/images/Full stack ML Engineer/d61bbf30-d230-4963-b553-cb4b4b21bfce-image.png)\nML model 개발을 위한 pipeline ![](/assets/images/Full stack ML Engineer/a74e3312-4cef-4b03-a23c-ef6ac982d7b6-image.png)\nPros(장점) # 프로토타이핑에 용이 프로토타이핑은 협업에 곤란한 경우가 많아서 직접 해결하는게 좋은 경우가 많다. Stack간 시너지 stack에 대한 이해가 각 stack에 대한 효율적인 개발에 용이한 경우가 있다. 협업 갈등이 생길만한 포인트에서 협업 포인트 찾을 수 있다. 잠재적 위험을 예측할 수 있다. 성장의 다각화 성장의 밑거름이 된다. 매너리즘을 떨쳐내기 위한 트리거가 되기도 한다. Conf(단점) # 하나의 stack에 대한 깊이가 없어질 수도 있다. 모든 stack들의 발전 속도가 매우 빨라서 학습 자체가 너무 어렵다. 절대적 시간의 부족 공부할 분야는 많은데 주어진 시간은 모두 동일하다. ML Product # ![](/assets/images/Full stack ML Engineer/fea45e30-6992-4980-bd5c-de925c475dec-image.png)\n요구사항 전달 고객사 미팅(B2B), 서비스 기획(B2C) 요구사항, 제약사항 정리 ML Probelm으로 회귀 데이터 수집 Raw 데이터 수집 Annotation tool 기획 / 개발 Annotation guide 작성 / 운용 ML 모델 개발 기존 연구 조사 및 내재화 실 데이터 적용 실험, 평가 / 피드백 모델 차원 경량화 실 서버 배포 엔지니어링 경량화 작업 연구용 코드 수정 작업 모델 버전 관리 / 배포 자동화 ML Team # 이상적인 업무 분할이 일어난 ML Team 구성의 예시\n프로젝트 매니저 1명 개발자 2명 연구자 2명 기획자 1명 데이터 관리자 1명 업무가 혼용된, 적은 인원의 ML Team\n프로젝트 매니저, 기획자, 연구자 1명 개발자, 연구자, 데이터 관리자 1명 개발자, 데이터 관리자 1명 Full stack ML Engineer in ML Team # 한 명이 개발자, 기획자, 데이터 버전 관리자 등의 역할을 겸하는 것이다.\n1. 실 생활 문제를 ML 문제로 Formulation # 고객/서비스의 요구사항을 구체화 하는 작업이다. 기존 ML 연구에 대한 폭넓은 지식과 최신 연구 수준을 파악하고 있어야 문제의 해결방안과 수행 가능 정도를 판단할 수 있다.\n2. Raw Data 수집 # Web cralwer(scraper)를 직접 구현.\n3. Annotation tool 개발 # 수집/제공 받은 데이터와 데이터의 정답을 입력하는 작업을 수행하는 Application 개발.\n작업 속도와 정확성을 고려한 UI 디자인 필요 Annotation tool 개발을 위해서 ML model 자체에 대한 이해가 필요한 경우도 많다. ![](/assets/images/Full stack ML Engineer/9705aef5-6565-4c05-88de-a81c31c2a525-image.png)\n4. Data version 관리, Data loader 개발 # 데이터의 Version을 관리해야 한다. 대부분의 경우 데이터에 직접 접근하지 않고 DB를 통해 접근하기 때문에 관련 Loader package를 개발해야 한다. 멘토님은 Amazon S3와 python을 통해 구현하셨다고 한다. ![](/assets/images/Full stack ML Engineer/b6ba9bd2-c039-4237-bef7-d4449815cfe9-image.png)\n5. Model 개발, 논문 작성 # 기존 연구 조사, 재현 재현 성능은 public benchmark 데이터로 검증 수집된 서비스 데이터 적용 모델 개선 및 아이디어 적용 필요하다면 논문 작성 ![](/assets/images/Full stack ML Engineer/2956ed76-bd5a-46ec-b42c-e9cd777ff19c-image.png) 6. Evaluation tool, Demo 개발 # Model prediction 결과를 채점하는 application 개발 Evaluation tool을 통해 모델의 발전 포인트를 잡을 수 있다. 7. 모델 실 서버 배포 # 6번까지의 연구용 코드를 Production server에서 사용 가능하도록 정리해주느 작업 File server에 code, parameters weight를 저장해 version 관리 Production server는 python worker에 MQ를 통해 job 전달 ![](/assets/images/Full stack ML Engineer/fc215f70-56d9-4b42-a380-47243e2129b9-image.png) Stack # ![](/assets/images/Full stack ML Engineer/bb761f82-eea7-4089-989d-39992ea229d5-image.png)\n조언 # 멘토님 조언을 정리하면 아래와 같다.\n빨리 만들려고 해봐라\n모든 stack에 대한 체계적인 architecture를 고려하면 개발 기간이 너무 길어진다. 처음부터 끝까지 최대한 빠르게 일단 완성해보고 기능을 더해보자. 전문 분야를 정해라\n모든 stack을 초~중급으로 유지하는 것은 별로다. 전문 분야는 중급 수준 이상으로 실력을 갖춰라. 각 분야의 engineer라면 코드의 high level implementation을 보고 내부 작동과정을 알 수 있어야 한다. 두려움을 없애기 위해 반복해라\nML Engineer를 위한 추천 # ML 논문을 읽고 구현/재현 Web에서 implementation 구동 DB를 활용해 2번 implementation 발전 ","date":"26 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-26-full-stack-ml-engineer/","section":"Posts","summary":"","title":"Full stack ML Engineer","type":"posts"},{"content":"","date":"26 September 2021","externalUrl":null,"permalink":"/tags/full-stack/","section":"Tags","summary":"","title":"Full-Stack","type":"tags"},{"content":"","date":"26 September 2021","externalUrl":null,"permalink":"/tags/ml-engineer/","section":"Tags","summary":"","title":"ML Engineer","type":"tags"},{"content":"개인적인 연구가 아니라 공개를 목적으로 한 프로젝트에서는 데이터의 저작권과 라이센스를 중요하게 다뤄야한다.\n![](/assets/images/AI 저작권법/8c16cf0c-b974-4f93-978c-a26f4f41a926-image.png) 학계에서도 데이터 저작권, 라이센스가 제대로 지켜졌는지 평가하고 있다. https://2021.aclweb.org/ethics/Ethics-review-questions/\n저작권 보호 수준 # 창작성이 인정받는 수준에서는 저작권이 보호된다.\n판례는 창작성이 인정되지 않아 저작권 보호가 되지 않기 때문에 판례 검색 서비스는 자유롭게 판례 데이터를 사용해도 무방하다. 댓글은 창작성 여부에 따라 저작권이 적용된다. 일상적이고 관용적인 문구는 보호가 안된다. 저작권 사용 절차 # 창작성이 인정돼 저작권이 자연 발생한 데이터에 대해서는 저작자와 협의를 해야 한다.\n저작재산권 독점적/비독점적 이용허락에 대한 협의 저작재산권 전부/일부 양도 라이센스 # 저작권 이용에 대한 절차가 매우 귀찮다. 그래서 라이센스를 통해 이용허락 규약을 설정하여 라이센스에 따라서 저작권을 사용하면 된다. ref: http://cckorea.org/xe/ccl e.g., CCL, 공공누리\n나무위키: CC BY-NC-SA, 비영리목적으로 사용 가능. 데이터 출처 명시 KorQuAD: CC BY-ND, 저작자 표시 / 변경 금지 뉴스 데이터 # 언론사: 한국언론진흥재단에서 대부분의 언론사 저작권을 위탁 관리\n저작자(한국언론진흥재단, 언론사)와 협의. 조중동은 직접, 그 외는 한국언론진흥재단에 문의. 아주 아주 드물게 CCL이 적용된 뉴스 기사가 존재함(위키트리) 뉴스 데이터를 0원에 구매했더라도 구매 이용 약관을 따라라.\n신문 제목은 저작권의 보호를 받지 못한다고 한국저작권위원회에서 명시됨.\n공정이용(Fair-use) # 교육, 재판, 전시 등의 목적으로는 저작권자의 허락을 받지 않고 사용 가능하다.\nhttps://www.copyright.or.kr/education/educlass/learning/what-the-copyright/definition/index06.do\n저작권법와 AI 사이의 회색지대 # GPT-3가 생성한 데이터는 저작권법으로 어떻게 다뤄야하는가? 뉴스기사 요약 모델이 생성한 뉴스기사의 저작권은 어떻게 다뤄야하는가? ","date":"26 September 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-09-26-ai-%EC%A0%80%EC%9E%91%EA%B6%8C%EB%B2%95/","section":"Posts","summary":"","title":"AI 저작권법","type":"posts"},{"content":"","date":"26 September 2021","externalUrl":null,"permalink":"/tags/%EC%A0%80%EC%9E%91%EA%B6%8C/","section":"Tags","summary":"","title":"저작권","type":"tags"},{"content":" 8주차 학습정리 # 강의 복습 내용 # https://velog.io/@naem1023/NLP-%ED%97%B7%EA%B0%88%EB%A0%B8%EB%8D%98-%EC%A0%90%EB%93%A4 https://velog.io/@naem1023/Kaggle-tip https://velog.io/@naem1023/AI-model-as-Service%EC%84%9C%EB%B9%84%EC%8A%A4-%ED%96%A5-AI-%EB%AA%A8%EB%8D%B8 https://velog.io/@naem1023/MLOps-%EC%A0%95%EB%A6%AC\n과제 수행 과정 / 결과물 정리 # 대회 준비를 위해 MLOps 관련된 사항들을 미리 조사해보고 테스트해볼 내용들을 미리 수행했다.\nGithub actions\nWandb action을 많이 사용할줄 알았지만, 결과를 정리해주는 csv generator였다. Wandb가 더 좋기 때문에 안 쓰기로 했다. 피어세션 정리 # Validation set의 class 분포를 항상 균등하게 주는 것이 모델을 robust하게 만들어주지 않을까? 라는 논의를 했다. 내 생각과 결과는 \u0026lsquo;아니다\u0026rsquo;이다. 모델의 학습 행위는 모집단 추정이다. 모집단의 성질을 알기 위해서 모집단의 성질을 미리 예측하는 것은 학습에서 위험할수 있다. 왜냐하면 validation set의 score가 잘 나오도록 학습할 것이기 때문에 validation set에 적합한 model이 생성될 것이기 때문입니다. 또한 validation set에 대한 가설이 언제나 모집단을 대표해준다는 보장은 없다. 즉, validation set의 분포를 미리 결정하는 것은 모집단 추정에 도움되는지 모르기 때문에 불필요한 행위라고 생각한다. validation set의 class 분포는 train set과 맞춰주는 것이 좋다고 생각한다. train set과 validation set의 class 분포를 맞춰주는 것은 train, validation data의 통일성을 유지시켜줘서 학습에 불필요한 노이즈를 발생시켜주지 않고, 다른 학습 방법론들이 imbalanced class 분포를 조정해줄 여지를 남겨준다. 만약 validation set과 train set의 분포가 다르다면 불필요한 노이즈가 발생해서 학습 방법론의 결과에 대한 신뢰성이 사라질 것이다. 하지만 validation set 조작 자체가 무의미한 것은 아니다. 정확히는 validation set이 아니라 dataset의 조작이 필요한 경우는 분명 존재할 것이다. 가령 학습 데이터의 class 분포가 99대1이라면 학습이 전혀 안 될 가능성이 매우 크다. 이러한 경우는 dataset의 분포를 어느정도 조정해서 99대1보다는 balanced하게 만들어주고 이에 대해서 train, validation set을 만들면 되겠다. 결론 train, validation set의 분포가 다르다면 학습 과정에 노이즈가 발생해 학습 방법론의 신뢰성이 사라진다. 따라서 통상적으로 정확한 validate가 아니라 할 수 있지만, 극단적 dataet에 대해서는 dataset의 분포 조작이 필요할 것 같다. 학습 회고 # 21/09/09: 특강 4개 수강. 21/09/10: 특강 4개 수강.\n","date":"24 September 2021","externalUrl":null,"permalink":"/posts/records/2021-09-24-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-8%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 8주차 학습정리","type":"posts"},{"content":"","date":"23 September 2021","externalUrl":null,"permalink":"/tags/quant-trading/","section":"Tags","summary":"","title":"Quant Trading","type":"tags"},{"content":" Trading # trading: 단기적 1초 ~ 3일내에 재거래한다. Investment: 장기적 Quant trading # 말 그대로 Quantative(계량적)인 trading이다. 느낌대로 사는게 아니라 수학, 모델을 활용해 trading을 하는 것. = Automated / system / algorithmic trading\n과거: 파생상품의 가격이 수학적 성질을 가진다고 보고, 수학적 조작을 통해 가격을 이끌어냈다. 과거의 데이터 사용 안 함 최근: 과거 데이터에 접근이 용이하고 편리해졌다. 통계, 머신러닝을 활용한 데이터 기반의 가격 접근법이 매우 성공적으로 활용되고 있다. Strategy of Quant trading # 수많은 전략들이 존재하는데, 널리 알려진 방식들은 다음과 같다.\nPosition을 얼마나 유지하는가? 1초 미만, ~3분, ~1시간, ~하루, ~3일 등 매우 다양하다. 어떤 상품군을 거래하는가? 주식, 선물, 옵션, 채권, 외환, 암호화폐 등.. e.g., 종목 거래량을 보면 선물은 많아봐야 10 ~ 100개 이하지만, 주식은 매우 많은 수를 거래한다. 가령 5000개. Automation 여부 Blackbox: 100% 자동화 Greybox: trader의 주관이 개입 Trade execution(주문 집행) vs 자체 수익 종목 자체에서 발생하는 수익을 극대화하는 것인가 종목의 커다란 주문에 대한 처리에 대한 전략인가 어디에서 Edge가 오는가? What is trading edge? 시장의 특성 파악 훌륭한 모델을 통한 수익 발생 Arbitrage # 차익거래. 싼 곳에서 사서 비싼 곳에서 파는 행위를 통해 수익을 발생시키는 전략이다. 보통 여러 거래소가 존재하는데 거래소마다의 시세를 감안하여 거래한다.\nArbitrage는 같은 상품의 가격을 맞춰주는 역할을 한다. 소비자가 어느 거래소에 가더라도 비슷한 가격에 물건을 사고 팔 수 있음을 알려준다.\nArbitrage가 깨지는 것은 거래소마다의 가격 예측이 안 되는 것이다. 대표적인 예시가 한국의 암호화폐가 다른 나라에 비해 비쌋던 현상을 나타내는 \u0026lsquo;김치 프리미엄\u0026rsquo;이다.\n직관적이고 쉬운 방법론이기 때문에 속도 경쟁이 치열하다. Arbitrage 기회가 발생하면 시장과 거래소에 따라 다르지만 짧게는 수μs에서 수ms 사이에 거래가 이루어진다.\n거래 성공의 요소를 비율로 따지자면 90%의 속도와 10%의 알파로 이루어진다. 알파는 얼마나 전략이 똑똑한지를 의미한다. 따라서 가장 좋은 모델이 독식하는 전략이라고 한다.\nMarket Making # 시장 조성. Market maker(시장 조성자)가 자본 시장의 특성을 활용해 수익 창출.\ne.g., 은행은 외화 거래를 통해 수익을 발생시킨다.\n유동성을 공급한다 = 누구나 거래하고 싶을 때, 안정적으로 쉽게 사고 팔 수 있다.\nMarket maker는 시장을 조성해서 유동성을 공급해 차익을 발생시킨다. Market maker는 자본시장을 안정시켜준다. Market maker가 없다면 사람들은 P2P로 거래를 해야하고 사람에 따라서 거래액수도 차이가 나며 자본 시장이 불안정해진다.\nMarket에서 모든 사람들이 일제히 한 방향으로 거래할 때, 가격이 움직이면서 Market maker는 손해를 볼 수도 있다. 따라서 전략을 잘 세워 이러한 움직임을 파악해 주문을 취소하는 것이 중요하다.\n50%의 속도, 50%의 알파다.\nStatistical arbitrage # 이름만 이렇고 arbitrage와는 별 관계가 없다고 한다. 미래 가격의 변화를 예측하는 모든 방법론들을 통칭한다고 한다.\n최근 호가 움직임을 이용한 가격 예측 종목 간의 상관관계를 이용한 가격 예측(cross section) 종목 간의 가격 차이 예측(basis trading) funmamental(ref), 기술적 지표 등 정보의 출처에 상관없이 모든 정보 활용.\n데이터 기반 접근이 필수적이다!\n10%의 속도, 90%의 알파다.\nFunmanetal trading players # Quant hedge fund(ref), robo-adviser(wiki) 규모가 큰 고객의 자본(수천억~수십조)을 운영 운용자금과 이익의 일부를 보수로 얻는다 종목의 보유 기간이 상대적으로 길다. Quant trading보다는 Quant investment에 가깝다고 한다. Propriety trading(자기 자본 거래, ref) Hedge fund와 다르게 회사 자신 혹은 회사 파트너들의 자본(수십~수백억)을 거래 Hedge fund에 비해 상대적으로 규모는 작다. HFT, market making을 통해 높으 수익률을 추구. 성공적인 팀들은 연간 100% 이상의 수익률을 낸다. 금융위기 이후 규제 변경으로 인해 Prop trading을 안하고, quant trading service를 제공하는 쪽에 힘을 쓴다고 한다. e.g., 주문집행서비스: 일반적인 시스템으로 감당하지 못하는 대용량 매도를 quant trading을 통해 집행 Stat Arb strategy # Statistical Arbitragy strategy를 딥러닝으로 생각한다면 이상적으로는 아래와 같이 생각할 수 있다.\n![](/assets/images/Quant trading / Researcher/383cf31e-6b5b-4930-aca5-9fa40921ab09-image.png)\nend-to-end의 멋지고 간단한 구조다. 하지만 현실적으로는 모든 요소에 대해서 end-to-end를 적용하지 않고 아래와 같이 구성한다.\n![](/assets/images/Quant trading / Researcher/227d39a7-0aa2-43cb-95e6-eb5d8a9d9b14-image.png)\n대부분의 가격 예측은 선형회귀를 통해서 이루어진다. 물론 머신러닝과 딥러닝 모델을 통해서도 예측한다. portfolio optimizer: 종목 간의 상관 관계성, 종목의 가격이 얼마나 움직이는지 등을 고려할 때 어떤 포지션이 가장 안정적인 수익을 창출할 수 있는지 판단하는 모델 데이터 기반 접근법보다는 모델 기반 접근 실제로 수행할 주문들을 여기서 뽑는다. Quant trading이라는게 가능한가? # 초과수익을 얻을 수 있는 시장에서 수익을 발생시킬 수 있는 것은 사기꾼이 아닌지 궁금할 수 있다. 즉, quant trading 자체의 효용성에 대한 의문이다. 왜냐하면 이러한 접근법으로 수익을 창출하는게 불가능하다는 가설, 썰들이 많기 때문이다.\n효용설 시장 가설(Eugne Fama) # \u0026ldquo;가격은 상품에 대한 모든 정보를 포함하고 있기 때문에 장기적으로 초과수익을 얻을 수 없다.\u0026rdquo;\n가격은 미래와 현재의 모든 정보들이 선반영돼있기 때문에 일정 수준 이상의 수익을 창출할 수 없다는 가설이다. 노벨경제학상을 타신 분의 가설이라고 한다.\n이를 뒷받침하는 이야기들은 아래와 같다.\n액티브 펀드 매니저(펀드 매니저 개인의 판단 하에 매수/매도)와 시장 인덱스(시장의 모든 종목들을 시가총액 비율로 매수)를 비교하면 시장 인덱스가 더 좋다. Long Term Capital Management라는 회사가 1997년 아시아 금융위기 때 정말 거하게 말아먹은 전례가 있다. 예측 가능한 이유들 # 거래를 통한 정보 반영 # 예측 범위(horizon)에 따라 다르지만 미래 예측이 가능한 경우들과 그에 대한 이유들이 분명 존재한다.\n상품에 대한 새로운 정보가 가격에 포함되기 위해서는 누군가 거래를 해야 한다.\n효용성 시장 가설에서 가격에 정보가 선반영된다고 했지만, 결국 정보 반영은 거래를 통해 이루어진다. 따라서 거래 관측을 통해 미래 정보를 일정 수준으로 예측 가능하다!\n포지션이 큰 참가자들은 움직이는데 오랜 시간이 걸린다. 하나의 종목에 대한 대용량 매도는 오랜 시간이 걸린다. 따라서 이러한 움직임을 파악해 가격 예측을 할 수도 있다. 큰 가격 변화에는 군중심리가 나타난다. 가격이 상승할 때, 피크를 찍고 적정 수준으로 조정된다. 전문가들은 리스크를 줄이는 합리적인 행동을 하고 실제로 의미가 있다. 옵션을 사고파는 market maker들은 많은 양을 매도할 때 리스크를 줄이기 위해 현물 시장에서 거래할 수 밖에 없다. 따라서 옵션 시장의 거래량을 통해 짧은 미래(수ms)를 예측할 수 있다. 새로운 정보(뉴스, 공시 정보, 매출, 펀더멘털 등)가 시장에 반영되는데 시간이 걸린다. 해당 정보가 시장에 반영되는 추이를 모델링해서 수익을 창출할 수 있다. 기술적인 문제들: 거래소/종목의 특성, 특정 규칙에 따라 움직이는 참가자들 해당 종목과 이용자들에 대해서는 특성과 규칙을 고려해 미래를 예측할 수 있다. 거래량이 많은 상품이나 거래소가 가격 발견 과정을 선도 Arbitrage들은 거래량의 차이가 큰 거래소 사이의 가격을 맞춰주는데, 거래량이 큰 거래소를 기준으로 맞춰줄 수 밖에 없다. 거래량이 큰 쪽에 market maker가 있고 물량도 많기 때문에 거래량이 적은 쪽의 물량이 먼저 소진된다. 직관과 다른 성공 기준 # 미래 예측의 성공을 평가하는 지수로 $r^2$(wiki)가 있다. 간단하게 미래 가격 예측에 대해 얼만큼 맞췄는지를 알려주는 지표라고 이해할 수 있다. 음수의 경우 예측을 안하는게 더 나은 경우고, 양수의 경우 0 ~ 100% 사이의 지표를 표현한다.\n![](/assets/images/Quant trading / Researcher/9ffa1d05-2fcc-4396-82a5-1559050b97de-image.png)\n6개의 forecast(예측)을 보자. 높은 $r^2$의 forecast가 돈을 버는데 유용할 것이라고 생각할 수 있다. 하지만 $r^2$가 0.05%, 1.00%, 3.00%인 forecast가 실제로 실현 가능하고 돈을 벌 수 있는 forecast라고 한다. 오히려 $r^2$가 15% 이상으로 나올 경우 문제가 있다고 생각할 수 있다. pipeline에 버그가 있거나, $r^2$를 잘못 측정하고 있거나, data snooping(ref) 문제때문에 인위적으로 $r^2$가 높게 나오는 것을 의심한다.\n**즉, 안 좋은 예측으로도 돈을 벌 수 있기 때문에 우리의 직관과 실제 quant trading에서 성공의 기준이 달라진다.\n안 좋은 예측이 유효한 이유\n엄청나게 작고 많은 예측 수천개의 종목에 하루에 수만번의 배팅을 한다. 베팅 성공 확률이 51%라면, 대수의 법칙에 따라 결국 이론값(51%)에 성공 확률이 수렴 많은 forecast 알고리즘 사용 HFT(high frequency trading) 전략의 평균 수익은 거래량의 약 0.01% (1 basis point) 실제 수익률은 모두 다르지만, 전체 평균을 내면 아주 조금의 이익이 발생 HFT를 통해 연수익 100% 이상을 기록하기 위해서는 매우 많은 거래가 필요 Quant trading에서 딥러닝의 사용 # 선형회귀를 많이 사용하고 ML, DL은 조금만 사용해서 가격 예측을 한다. 왜냐하면 시장을 예측하는 문제 자체가 정의하기도 어렵고 해결도 어렵기 때문이다. 시장 예측이 어려운 이유는 아래와 같다.\n시장에 영향을 미치는 요소들 중 관측 가능한 요소들은 극히 일부에 불과 다른 참가자들의 포지션, 뷰, 코로나, 암호화폐 규제, 정치, 경제, 외교, 스캔들 등등.. 정보를 볼 수 있는 시점은 정보 발생보다 늦을 수 밖에 없다. 시장의 특성은 계속 변한다. 참가자들은 접근 방법을 계속 고도화한다. 참가자들이 계속 바뀐다. 규제 조건이 계속 달라진다. 새로운 시장 / 상품군의 출현 문제와 환경의 정의 # 딥러닝이 효과적으로 해결하는 문제들은 다음과 같다.\n이미지 인식: 이미지의 객체들은 변하지 않는다. 고양이, 강아지들의 특성은 수백만년이 지나지 않는 한 바뀌지 않는다. 바둑 AI: 어떠한 형태의 기보라도 바둑판 전체를 관측할 수 있다. NLP: 언어의 형태와 특성은 바뀌지만, 언어의 요체는 매우 오랜 기간 변하지 않는다. 즉, 문제 난이도와 별개로 문제 자체가 변화하지 않는 것들을 효과적으로 예측한다. 이러한 문제들은 시장 예측에 비유하면 아래와 같다.\n털 한가닥을 관측해 어떠한 동물인지 파악 동물들이 분류를 피하기 위해 털 모양을 변화 동물들의 기준을 정부가 바꿈 문제가 어렵다는 것은? # 오버피팅의 위험이 크다 insample error를 줄이는 것은 쉽다. ML, DL이든 아주 쉽다. 하지만 낮은 error를 찾는 것은 오퍼피팅이 될 여지가 아주 크다. 오버피팅을 막았다할지라도, 미래에도 오버피팅이 되지 않을 가능성은 없다. 시장에서 변화하는 속성과 변화하지 않는 속성으 구분해야 한다. 이를 제대로 분류할 수 있다면 선형회귀만으로도 가치 있는 모델을 얻을 수 있다. **즉, 문제의 정의가 어렵고 문제 자체가 계속 변화하며 가치 있는 정보에 대해서도 오버피팅의 위험이 크기 때문에 딥러닝을 잘 활용하지 않는다고 정리할 수 있다. **\nResearch # Hypothesis # 대부분의 시장에 대한 research는 가설에서 출발한다.\n여름에 홍수가 나고 날씨가 작물에 악영향을 끼친다면 곡물 선물 가격은 상승할 것이다. 같은 크기의 매수 주문이 반복된다면 가격이 상승할 것이다. 모델을 대략적으로 설계하고 데이터를 투입해서 결과를 얻는 데이터 기반 접근법을 실패하기 쉽다. 왜냐하면 설득력 있는 가설 없이는 모델 수정, 방향 수정, 데이터 조정 등의 작업이 어렵기 때문이다. 설득력 있는 가설이 있을 때도 프로덕션까지 생성되는 일은 거의 없는데, 가설조차 없다면 더욱 힘들다.\n항상 Waterfall 형태의 연구만 이루어지는 것은 아니다. 개발된 모델의 형태나 결과만을 보고모델과 파이프라인을 수정해 좋은 결과를 얻는 과정도 필수적이다.\nData # Quant trading 회사들은 굉장히 다양한 데이터를 구입해 사용한다.\n거래 시세, 공지사료 뉴스, 트위터, 웹 크롤링 결과, 애널리스트 리포트 루머로는 인공위성 사진(주차장의 차량 개수에 따른 경기 예측), 날씨정보(흉작 여부 에측), 신용카드 사용 정보 등을 활용하기도 한다. 데이터에서 엑기스만 걸러내는 작업들은 익히 알고 있는 ML/DL 작업들이 포함된다.\nfiltering, noise 제거, clipping, outlier 감지, normalization, regression regularization, NLP \u0026hellip; Algorithm # 가설을 잘 표현할 수 있는지? 최적화 대상인 목적함수가 제대로 설계된 것인지? (e.g., L1, L2 norm에서 어떤 것을 사용하는 것이 맞을까?) 기존 코드를 재활용 엔지니어링적 관점: 분산화가 안된 코드라면 코드 수정 모델링적 관점: 모델 경량화 Monetiazation # 이익창출. 알고리즘의 결과를 활용해 돈을 벌어야 한다.\n거래소 선정 주문 타입 선정 어떤 포트폴리오가 출렁임 없이 꾸준한 성능을 내는지 파악 내 알파와 포지션을 노출하지 않고 거래하기 내 거래가 시장에 영향을 끼치지 않게 하기 시뮬레이터와 현실의 차이를 어떻게 고려할지 주의점 # Production system과 Backtest system의 간극 Backtest system: 평가를 위한 system e.g., production은 c++로, backtest는 python으로 구현된 경우가 많았는데 간극이 존재했다. Market impact(가격 충격): 나의 거래로 인해 발생하는 시장 영향력 Data snooping: 리서치에서는 보지 못했던 데이터들을 고려하지 않게 되는 실수 최근 거래량이 많은 주식 위주로 백테스트: 이후에 어떤 주식의 거래량이 많아질지 모르기 때문에 위험하다. 거래 당일에 공개되지 않는 정보들을 활용한 백테스트 어제까지의 정보만을 활용해야되는데 실수로 오늘까지의 정보를 데이터에 넣어서 좋은 테스트 결과를 얻는 실수 Reseach pipeline # ML과 마찬가지로 리서치는 개발과 다르게 결과가 안나오는 경우가 허다하다. 개발은 어떻게든 requriements들을 충족시키다보면 개발 완료를 선언할 수 있지만 리서치는 아무리 좋은 방법들을 고안해도 결과가 나오지 않는다면 무의미한 작업의 연속일 뿐이다.\n따라서 가설 검증을 효율적으로 반복할 수 있는 플랫폼이 매우 중요하다.\n회사 단위의 투자 자체적인 노트북 리서치 플랫폼, 자체 클라우드, 자체 DSL 팀 단위의 투자 쉽게 재현 가능한 리서치 스크립트, 리서치 효율성에 대한 투자 업계 # 국내에도 트레이딩 스타트업, 전통적 증권사의 프랍 데스크, robo-adviser, 핀테크 스타트업 등 여러 종류의 quant trading 기업들이 생기고 있다.\n일할 곳은 플랫폼과 프로세스에 대한 투자를 열심히 하고, 내부적으로 연구 결과와 자료를 많이 공개하는 분위기의 회사가 좋다.\n","date":"23 September 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-09-23-quant-trading--researcher/","section":"Posts","summary":"","title":"Quant trading / Researcher","type":"posts"},{"content":"","date":"23 September 2021","externalUrl":null,"permalink":"/categories/quant-trading/","section":"Categories","summary":"","title":"Quant-Trading","type":"categories"},{"content":" 경진대회 플랫폼 # Kaggle 카카오 아레나: 계열사 전용이라고 한다. 데이콘: public 대회. Kaggle 스타일이 적용되는 중이라고 한다. Ranking # Ranking system: competition 내의 point로 정해지는 순위 팀을 이뤄 출전하면 $\\sqrt{N}$빵한다. Tier system: competition medal 수에 따라 결정 Competition # Purpose # Featured 상업적 대회 우승한 모델을 기업에서 쓰는 경우도 있다. Research 연구 목적 대회 재밌는 주제는 많은데 상금이 낮다고 한다. Getting started \u0026amp; Playground Titanic survivor같이 초심자를 위한 대회 포인트, 메달용 아님 Analytics 데이터 분석 목적 대회 데이터 탐색, 시각화 노트북을 제출 Recruiment 채용 목적 Submit # General competition 리소스 제약 없음 submission.csv만 제출 Code competition kaggle notebook을 실행시켜 submission.csv 생성해야 한다. 리소스 제한 있음 쓸모 있는 모델을 만들도록 유도 Processing Competition # ![](/assets/images/Kaggle tip/a6770f33-28bd-4a3e-a95a-30f9e76912e4-image.png)\n익히 봐왔던 도식도이다! 다른점은 다음과 같다.\nKaggle notebook 사용 다른 사람이 생성한 Kaggle notebook 조회 가능 notebook마다 용도가 다르다. e.g., train, inference, preprocessing\u0026hellip; For Winning # 빠르고 효율적인 pipeline 반복 # GPU 장비 투자 한국 캐글 그마분은 Ryzen3700, RAM 64GB, RTX 2080ti 2개를 쓰신다고 한다. 2개 이상의 GPU라면 비블로워(GPU wrapper) 타입을 추천하셨다. 연구자분들은 병렬 GPU가 필수일줄 알았는데 의외로 RTX 3090, 3080 1대도 좋다고 하시더라. 물론 3090 2개가 더 좋다. RTX 3070을 72만원에 사게 해준 CDPR이 있는 폴란드를 향해 오늘도 절을 한다. 본인의 시간 투자 1~2달 동안 평일 하루 평균 4시간 이상, 주말 하루 평균 8시간 이상 투자하신다고 한다. 템플릿처럼 사용할 수 있는 본인만의 baseline 개발 속도 뿐만 아니라 실수도 줄어든다. 링크를 통해 최근 3개월 동안 3개의 금메달을 따셨다고 한다. Score improvement # Competition 내의 Notebook tab, Discussion에서 좋은 아이디어를 찾아보자 Augmentation, deep learing architecture 참고할만한 논문 마지막까지 방심하지말 것을 꼭 명심하라고 하셨다. Validation strategy # Training set과 Test set에서 얻은 score차이를 좁히기 위한 방법론.\n최종 순위 하락을 막기 위해 필수적이다. Public LB(Leader board)와 Private LB는 다르기 때문에 Public LB에 overfitting되지 않도록 하자.\n최근에는 Test set을 공개하지 않는 추세라고 한다. Train set에서 Validation set 추출 K-fold validation Startified k-fold Class별로 validation set를 생성 Ensemble # 대부분의 경우 single model보다 더 좋은 성능을 얻을 수 있다. 최근 Kaggle은 아래처럼 정리되는 추세다. 서로 다른 아키텍쳐끼리 ensemble할수록 잘 나온다고 한다. e.g., LSTM, BERT\nStartified k-fold ensemble validation check만 하지말고 해당 model들을 ensemble 정형 데이터 LightGBM, CatBoost, XGBoost, NNs 이미지 데이터 Resnet, efficientnet, resnext 텍스트 데이터 LSTM, BERT, GPT-2, RoBert Single model improvement # Ensemble을 처음부터 할 수는 없다. 어느 정도 single model을 개선한 후에 Ensemble을 시도해야하는데 개선의 한도선을 정해야 한다.\n상위 랭커들이 discussion에 언급한 single model 점수 대회 종료 1~2주 전에 single model로만 50등 내에 잡다 팁 # 팀이 좋다. 혼자 하기엔 2달 이상의 여정은 너무 길다. 팀은 해체가 안되기 때문에 신중하자. 동료 후보의 현재 대회 순위를 확인하자. 생각 외로 게으른 사람이 많다고 한다. 폴더별로 v1, v2와 같은 version 관리. Ensemble할 때 폴더 별로 정리한 version끼리 할 가능성을 열어두기 위해서 이렇게 하신다고 하더라. VCS는 최종 업로드 시에만 사용한다고 하신다. ??? Version control을 아예 안하신다고 하셨다. ","date":"23 September 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-09-23-kaggle-tip/","section":"Posts","summary":"","title":"Kaggle tip","type":"posts"},{"content":" AI model as Researching # ![](/assets/images/AI model as Service/b0c684b9-202f-40b7-8a82-eeebc246bbcd-image.png) Imagenet처럼 데이터셋이 명확하고, 이를 해결하기 위한 모델링에 집중한다.\nAI model as Servicing # ![](/assets/images/AI model as Service/0bfb577d-4358-44fe-a2a4-09f16450e819-image.png) 보통의 산업환경에서는 데이터셋 자체가 존재하지 않는 경우가 대부분이다. Software Requirements들만 존재하고 이를 해결하기 위한 도구로써 AI를 요구하는 경우가 대부분이다.\n따라서 데이터셋을 직접 만들어야하는 경우가 대부분이다!\nRequirements # 학교에서 배웠던 Software engineering을 떠올리면 된다. Functional, non-Functional requirements들을 뽑아내기 위해 수단과 방법을 가리지 말자.\nDataset # ![](/assets/images/AI model as Service/8ebf7128-e569-4c5e-b5fe-2ae4faa638c6-image.png)\n종류, 수량, 정답 관련 requirements들을 명확히하자 각각의 requirements들을 반드시 명확하게 정의 종류: Requirements에 크게 의존하나 유연하게 하자 e.g., OCR에서 수식의 이미지를 어떠한 종류로 나눌 것인가? 초등, 중등, 손글씨, 인쇄글씨 등의 카테고리들을 고려할 수 있다. 데이터셋을 처리하기 위한 모듈 정의 e.g., 여러 수식이 적힌 이미지에서 개별적인 수식을 추출하는 AI 모듈 정의. 정답 정의 e.g., 수식관련 OCR이라면 LaTex string 수량 정의 예산과 모델의 성능을 고려하여 적절하게 설정 Modeling # 처리 시간 실제 서비스에서 입력이 처리되어 출력이 나올 때까지의 시간 목표 정확도 정량적으로 정해진다 목표 QPS(Query per Second) 초당 처리 가능 쿼리를 결정 장비, 처리 시간, 모델의 크기에 영향을 받는다 모델의 크기는 threshold에서만 QPS에 영향을 미친다 GPU MEM이 10GB일 때, 모델의 크기가 5GB보다 크다면 모델의 크기를 아무리 줄여도 하나의 모델밖에 탑재되지 않는다. 즉, QPS 관점에서는 5기가 이상의 모델은 QPS에 영향을 주지 않는다. Serving 방식 Local, Cloud, Mobile 등\u0026hellip; 장비 사양 모델 분할 # 데이터셋에 크게 의존되어서 설계된다. 필요하다면 검증된 여러 모델을 결합하여 하나의 모델을 생성하는 것이 좋다. 데이터셋 또한 여러 모델에 맞게 따로 준비해야 된다.\n손글씨 수식을 인식하는 OCR을 예를 들어보자.\nAI Model\ninput: 수식 이미지 output: LaTex string 해당 task만을 위한 모델이 따로 존재하지 않고 OCR을 위한 모델링을 하기에는 Task가 고차원적이다. 따라서 다음과 같이 AI model을 분리할 수 있다.\nAI model\n검출기: LaTex symbol 검출 인식기: LaTex symbol 분류 정렬기: LaTex symbol들을 한 줄에 정렬 변환기: LaTex string 생성 4개의 모델로 분리했기 때문에, 4개의 모델에 맞는 입출력을 제공하는 데이터셋이 필요하다.\n모델 후보군 # 하나의 AI model만을 통해 서비스를 출시하는 것은 위험할 수 있다. 여러 후보 AI model을 생성하고 정량, 정성 평가 후 서비스 출시 버전을 선택하자.\nTest # 테스트를 위한 데이터셋을 따로 구축하기도하고, 학습 데이터에서 테스트 데이터셋을 일부 사용하기도 한다. 이 또한 Requirements를 통해 도출하자.\nOffline test: 실 서비스 적용 전의 개발환경에서 이루어지는 정량 평가 AI model 후보군을 선택하기위해 사용 e.g., model의 accuracy가 99% Online test: 실 서비스 적용 시의 정량 평가 VOC(Voice of Customer)을 통해 개선 포인트 파악 ** e.g., 1 vs 1 Ai Game player **\ninput dataset: 프레임당 캡쳐 이미지, 프로게이머의 로그 output dataset: 사용할 스킬 set(no action도 포함) 해당 task를 Classification task로 분류할 수 있다고 가정하고 이에 대한 99% accuracy(Offline test)의 model을 개발했다고 하자. 하지만 실제 유저와 겨뤄본다면(Online test) 해당 model은 가만히 있을 가능성이 매우 다분하다!\n왜냐하면 단순한 classification을 가정하고 개발됐기 때문이다. 대부분 멈춰 있고 가끔씩 스킬을 사용하는 프로게이머의 로그에서 멈춰 있는 행동 외에는 전부 노이즈 취급할 확률이 매우 높기 때문이다.\nTeam # ![](/assets/images/AI model as Service/01baaabd-315e-4064-b830-3232f0783300-image.png) Requirments를 분석하고 개발하기 위한 팀을 구상해본 도식도이다.\nModel engineer 보통의 모델들은 pytorch로 개발되는데 service에 적합하게 tensorflow로 변환 tensorflow 모델을 모바일에 적합하게 TFLite로 변환 GPU Server에서 serving하기 위해 모델을 TensorRT로 변환 framework 변환 시 존재하지 않는 operation 개발 CUDA Programming Lightweight(경량화) task 빠른 연산을 위해 연산을 C++/C로 변환 Modeler Model을 개발하는 인력 Model을 잘 만드는 역량은 여전히 매우 유효한 능력이지만 자동화가 빠르게 일어나는 분야다(e.g., AutoML) 다른 분야를 조금씩 공부해보자. FE: annotation tool, debugging tool BE: api serving, massive gpu training Model: engineering Model management 전체적인 Model의 품질을 관리 ","date":"23 September 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-09-23-ai-model-as-service/","section":"Posts","summary":"","title":"AI model as Service","type":"posts"},{"content":" MLOps framerwork, tool # 부스트캠프에서 사용해볼만한 MLOps framerwork, tool. 필요한 것만 써보자.\nWandb main logger Sweap hyperparameter tunner Hydra: configuration manager json parser를 직접 구현하는 것보다 통일성, 확장성 좋음 hyperparameter 공유에 용이 DVC: data version control nlp data를 수정할 일이 있으면 사용해볼만 할 듯. KLUE에서는 쓸 일 없을 것 같음 ONNX: model deployment pytorch의 model paramters를 배포하는 것이 아닌, 온전하게 실행 가능한 ONNX model로 model 변환 외부 배포가 필요하다면 고려 가능 Fast API / Uvicorn Asynchronous server가 필요할 경우 docker로 두 프레임워크 사용 ONNX 배포에 유용할 듯 Github actions, Jenkins, Circle CI trained model 후처리에 사용 wandb-action wandb 결과들을 통합해서 csv로 정리 wandb sweeps 사용할거면 굳이 안 써도 될거 같음 Kibana AWS Elastic 사용할거면 필수로 보임. Wandb sweap만으로 해결할거면 불필요 협업 # 사용 가능 환경 # Software engineering이 아닌 MLOps 관점에서 적용할만한 사항들\nCircle CI Build Deploy 보통 Orbs라는 Wrapper package로 관리 ref Github Discussion, issue, action 단일화 가능 별도 세팅 불필요 Issue: 개발 사항 Discussion: Issue 사항 혹은 그 외 사항들 토론 Action: trained model 후처리 Jira Software 별도 세팅 필요, github token으로만 가능 Issue: 개발 사항, 자동화 여지는 github보다 많음 Board: Github Project와 다르게 Board자체가 issue generator. Github은 issue로 따로 변환해야 한다. Build: 별도 build tool 연결해야 함. Circle CI, Jenkins, Travis 모두 jira 지원. 방식 # Github만 사용 Code review, project 관리 모두 github으로 Github은 Code review, Jira는 Project 관리 용도로 사용 https://makeeyaf.postype.com/post/8614009 Code review: Github PR Github는 VCS용도로만 Project 관리: Jira(Issue, Board, Build) Build: Circle CI, Jenkins, Travis, etc.. ","date":"22 September 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-09-22-mlops-%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"MLOps 정리","type":"posts"},{"content":" argmax, multinomial # nlp 모델의 출력에 대해 argmax를 쓰지 않는 이유는 자명하다. 왜냐하면 argmax는 classification과 같이 모델의 출력이 하나의 답을 얻도록 유도하기 때문이다.\n가령, 입력 $X_n$에 대한 모델의 출력에 softmax를 거친 결과물이 [0.2, 0.5, 0.3]이라고 해보자. 그렇다면 argmax를 취했을 때 1번째 index를 답이라 생각하고 나머지 결과들은 무시하게 된다. 이러한 과정은 $X_n$에 대해서 1번째 index만을 출력하도록 모델을 강화하는 효과를 가져온다.\n따라서 argmax를 출력단에 묶어준다면 classification에 아주 적합하다!\n** multinomial을 쓰는 개인적인 생각** 다항분포를 NLP 모델의 출력단에 묶는 이유는 argmax와 같이 출력을 한 가지 경우에 대해서만 강화시키지 않기 위함이라고 이해했다.\n입력 $X_n$에 대한 모델의 출력에 softmax를 거친 결과물이 [0.2, 0.5, 0.3]이라고 해보자. multinomial에서 각각의 index를 뽑을 확률은 해당 값들이 가진 값을 확률로 사용한 경우다. 즉, 한가지 방향으로 출력을 유도하도록 모델을 강화하지 않고 다양한 결과들이 수용될 수 있는 여지를 남겨준다.\n멘토님 말씀으로는 multinomial 외에도 여러 방법을 사용해서 모델의 출력단에서 뽑아지는 결과들을 sampling 한다고 한다. multinomial과 같이 여러 가능성에 대해서 출력이 가능한 여지를 남겨두는 방법론을 선택하면 되기 때문이다. pointer network가 그 예시라고 한다!\nBeam search와 sampling은 다른가? # 얼핏 들었을 때 많이 헷갈렸던 말이다. 근본적으로 전혀 다른 방법론이라고 생각한다.\nBeam search에 대한 이전 포스팅: https://velog.io/@naem1023/Beam-search Beam search는 hypothesis들에 대한 join probabilty를 생성해서 decoding을 하고자 하는 것이다. 즉, decoding을 위한 방법론이다.\nSampling은 모집단에서 표본을 추출하는 행위다. 데이터 추출은 random, multinomial, uniform distribution 등 확률적으로 본인이 원하는 방법론을 사용하면 된다. 모집단의 성질을 잘 반영하도록 추출한다면 더욱 좋을 것이다.\n따라서 bema search는 모집단으로부터 표본을 추출하는 행위가 아니다. sampling 또한 joint probability를 계산해서 최적의 decoding 결과를 생성하는 방법론이 아니다.\nGPT를 $\\sqrt{n}$에 반비례하게 scaling하는 이유 # transformer에서 차원수만큼 scaling을 하여 본래 분산을 복원하는 것과 동일하다. transformer와 다른점은 GPT는 Layer normalization이 Pre-LN 방식을 사용했다. transformer에서는 LN 후 residual block마다 scaling을 해주는 Post-LN이다.\nPre-LN인 GPT는 LN을 선수행 후, residual을 수행한다.\nPre-LN, Post-LN # 또한 Pre-LN, Post-LN에 대한 실험적 증명과 이에 대한 논문이 있다고 한다.\nOn Layer Normalization in the Transformer Architecture\n멘토님의 답변을 정리하면 아래와 같다.\n위 논문의 주된 내용 기존 Post-LN Transformer를 Pre-LN Transformer로 변경하면서 warm-up stage를 제거해도 된다는 것이 주된 목표이고 이에 대한 여러 실험결과였다. 이 때 Post-LN의 경우 Warm-up의 유무가 실험 결과에 굉장히 큰 영향이 있었다.\n그러면 그냥 warmup을 쓰면 되지 않느냐 라는 질문도 당연히 제기되었고 이에 대한 문제는 최종 성능에 영향을 미치고 최적화에도 시간이 오래 걸린다는 문제가 있다. (On the Variance of the Adaptive Learning Rate and Beyond 참고. RAdam 논문인데 이 Optimizer를 사용하면 warm-up을 제거해도 괜찮다 라는 내용의 논문)\n결과적으로는 Pre-LN의 경우 gradient 의 크기 (norm)가 Layer가 깊어져도 그 깊이와 관계 없이 유지되는 것이 확인됐다. Post-LN은 output 쪽에서 미분값이 너무 크고 초반의 layer로 갈수록 이 미분값이 작아진다는 것이 확인됐다. 이 수식은 당연히 이 논문이 엄밀하지만 아래 논문을 확인하는 것이 좋을 것 같다.\nPost-norm과 pre-norm을 비교한 논문: Learning Deep Transformer Models for Machine Translation\n멘토님은 모델의 뒷단에 올수록 high dimentional한 feature가 나타나는데, 이러한 feature에 덜 영향을 받기 위함이라고 하셨는데 근거가 무엇인지 모르겠다. 다시 여쭤보는 중이다.\n","date":"20 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-20-nlp-%ED%97%B7%EA%B0%88%EB%A0%B8%EB%8D%98-%EC%A0%90%EB%93%A4/","section":"Posts","summary":"","title":"NLP 헷갈렸던 점들","type":"posts"},{"content":"","date":"20 September 2021","externalUrl":null,"permalink":"/categories/nlp-theory/","section":"Categories","summary":"","title":"NLP-Theory","type":"categories"},{"content":" Question Anwering # ![](/assets/images/Recent trends of NLP/51679e9c-0eec-4d8b-8950-8b3963c8574a-image.png) BERT, GPT와 같은 self-supervised learning의 가장 큰 수혜자라고 할 수 있는 영역이다.\nQuestion과 context가 주어진다. context는 문맥이라고 이해하면 되는데 사용되는 분야마다 의미가 조금씩 달라진다고 한다. Multiple-choice: Question에 대한 답의 후보군을 여러 개 주고, 그 중 답을 고르게 한다. Span-based: Question에 대한 답을 지문에서 발췌. e.g., 10번째에서 30번째 index에 해당하는 부분이 답을 포함하고 있다. Yes/No: Question에 대한 질의 Generation-based: GPT와 같이 답 자체 또한 language generation task로 보고 수행. Open-Domain Question Answering # ![](/assets/images/Recent trends of NLP/c84b7650-9a71-40b9-b92f-2c5783e5160c-image.png)\nExternal Knowledge인 Knowledgebase, knowledge graph와 같은 구조화된 형태의 DB에서 정보를 발췌하여 QA를 진행. ![](/assets/images/Recent trends of NLP/f9583d5f-92a4-4dca-a04a-2904898adbda-image.png)\nKnowledge tuple: [Hotel], [HasA], [Lobby]와 같이 정보를 가진 node들이 쌍. Knowledge graph: knowledge tuple의 구성체 ![](/assets/images/Recent trends of NLP/29c071a9-8b60-490e-a3ec-889e113bed6f-image.png)\nOpen-Domain Questino answering은 external knowledge에서 정보를 검색한다. 대부분의 NLP task들은 자연어 형태의 sequence data를 기반으로 정보를 얻고 학습을 진행한다. 이러한 모델들을 fine-tuning 할 때, knowledgebase와 같은 구조화된 data를 사용하고자 하는 시도가 최근 trend다.\nOpen-Domain Question answering은 knowledgebase든 Wikipedia같은 자연어 데이터든 External Knowledge를 Retriever가 발췌해서 MRC를 진행하는 것이다.\nRetrieval-augmented Language Model pre-training/fine-tuning # ![](/assets/images/Recent trends of NLP/709cc4be-7d1c-4e11-845b-bc95597783b8-image.png)\n지문에서 답을 찾는 것이 아니라, Pre-training model이 가지고 있는 정보와 external knowldeg만을 활용해 답을 찾는다. 일종의 zero-shot learning이다.\nOpen-domain Chatbot # 아직까지 chatbot을 위한 정형화된 답은 없다. 보통은 Seq2Seq로 구성한다.\nOpen-domain chatbot: 비정형화된 주제라도 대화가 가능하다. closed-domain에 비해서 매우 어려운 task다. Closed-domain chatbot: 특정 주제와 목적만을 위한 chatbot. 사람이 직접 디자인한 모델을 통해 진행되는 경우가 많다. 높은 자유도가 아니다. 보통 classification 위주다. ![](/assets/images/Recent trends of NLP/939fc6a3-a738-473c-8f47-47d25b28f2fd-image.png)\nFacebook에서 공개한 Blender Bot 2.0의 구조이다. 모델이 미리 알고 있는 정보와 인터넷의 정보를 결합해서 질의를 하고자 한다.\nUnsupervised Neural Machine Translation # 보통의 번역 task는 라벨링된 데이터를 활용한다. 이러한 task를 라벨링 되지 않은 데이터에 대해서 활용을 하고자 하는 분야다.\nBack-translation # ![](/assets/images/Recent trends of NLP/b83e7ce2-6962-4c5a-9e6e-dcc68f4016b2-image.png)\nCycleGAN, StarGAN 등에서도 활용되는 기법과 동일하다.\nParallel corpus: 말 그대로 평행한 문서 집합. 가령, (영어, 한국어)의 조합으로 이루어진 corpus. Back-translation의 요지는 가령 영어를 프랑스어로, 다시 프랑스어를 영어로 번역했을 때 원본 문장이 생성되는지를 확인하는 것이다. 모델을 이애 대한 차이를 좁히도록 노력한다. 입력과 출력을 비슷하게 생성한다는 점에서 AutoEncoder와 비슷하지만, AutoEncoder처럼 latent vector에 어떤 것이 발생하는지 신경을 끄지 않는다. 중간 생성물인 프랑스어의 결과가 제대로 나오도록 노력하는 것이 핵심이다.\n물론 중간생성물이 엉망임에도 입력과 출력 문장이 동일할 수 있다. 이러한 모순을 해결하기 위해 Denoising autoencoder를 사용하거나, decoder 결과물의 결과를 체크하는 등의 테크닉을 활용할 수 있다.\nText Style transfer # ![](/assets/images/Recent trends of NLP/db328dbb-5ef1-456b-aeca-c0eb5dbfbd67-image.png)\nsource 문장을 원하는 스타일로 바꾸는 task다. 가령, 문장의 어순을 바꾸거나 캐쥬얼한 문장을 바꾸거나 formal한 문장으로 바꾸는 것과 같은 task다. ![](/assets/images/Recent trends of NLP/f4d012ae-24ff-4225-a188-5878aef63fe6-image.png)\nencoder, decoder 사이에 style 정보를 넣어주거나 transformer에 x와 style에 대한 정보를 같이 주는 형태로 구현된다. conditional model, conditional generator라고 부르기도 하는 형태의 모델이다.\n(a) Disentanglement: context(z)와 style(s)을 구분한다. (b) Entanglement: context와 style을 구분하지 않는다. Quality Esitmation # ![](/assets/images/Recent trends of NLP/83c8fd4e-0f37-41e8-b3e5-2f978689dd78-image.png)\nNLG(Natural language generation)의 지표로 BLEU score가 있다. 하지만 이는 사람이 직접 디자인한 점수일 뿐이고 모델에 대한 복합적인 지표가 아니다. 가령 BLEU score가 50도 되지 않지만 상용 모델로는 과분한 경우도 분명 존재한다. domain에서의 task를 얼만큼 잘 수행했는가에 대한 점수를 직접 디자인하기란 매우 어렵기 때문에 발생하는 문제다.\n따라서 Quality Estimation에서는 정형화되지 않고 문장에 대한 다양한 요소들을 고려한 평가하고자 한다. 출력 문장이 좋은지에 대한 평가가 어렵기 때문에 어려운 분야라고 한다.\nBERTScore # ![](/assets/images/Recent trends of NLP/2c5789c8-dc22-41cf-9565-f3dfa8c552b9-image.png)\nBERT encoding을 사용해서 평가를 수행한다. ground truth와 평가하고자하느 문장을 유사도를 통해 평가한다.\nIn-Context Learning # 모든 NLP task를 자연어 생성 task만으로 처리하고자 하는 분야. GPT가 대표적이다.\n![](/assets/images/Recent trends of NLP/287c17a3-f28f-4ffe-92d5-eeb3d3368bab-image.png)\nCNN의 Few-shot과는 약간 다르다고 생각한다. 말 그대로 모든 task를 자연어 생성 task로 처리하기 때문에 task description, examples, prompt 조차도 자연어이다. 물론 번역과 관련된 데이터는 전혀 학습하지 않은 상태이기 때문에 Few-shot인 것은 CNN과 동일하다.\nPrompt Tuning # 위 도식에서 task description, examples, prompt를 어떻게 썻을 때 원하는 task를 가장 잘 수행하는가를 알아내는 task다.\n![](/assets/images/Recent trends of NLP/ef440bf2-f134-470d-a1e1-19abdedbd69a-image.png)\n즉, 답을 얻고자 하는 질의에 사용되는 모든 문장을 optimize하는 task다. Prompt tuning을 위한 별도의 모델을 구성하지만 본래 모델(GPT)는 전혀 Fine-tuning하지 않는다.\nLanguage Models Trained on Code # ![](/assets/images/Recent trends of NLP/c5dc3fa6-d7d9-4efc-9ddd-9479c8da862f-image.png) Codex: a language model fine-tuning on publicy available python code from Github. In-Context Learning, Prompt Tuning을 code에도 적용하니 코딩을 잘하더라.\nMulti-Modal Models # 다양한 형태의 정보를 혼용하는 모델.\nDall-E # ![](/assets/images/Recent trends of NLP/627b1a77-8cfc-4903-a048-441fc08f2443-image.png) Generating images from text descriptions. Conditional generator다. 이미지를 n개의 patch 단위로 쪼갠 후 patch들을 Embedding vector처럼 취급한다. Dall-E에서는 n개의 patch를 모아서 하나의 sequence처럼 처리하여 생성한다. transformer 기반으로 생성한다.\nCLIP # ![](/assets/images/Recent trends of NLP/50752cfc-b048-4272-8033-3db8d860905b-image.png)\nText와 Image가 Semantic하게 유사하다면 Embedding space에서 비슷한 위치에 있어야 한다는 논리로 생성된 모델.\n학습 데이터는 이미지와 캡션으로 구성된다. 다른 종류의 이미지에서 나온 캡션은 Embedding space에서 거리를 멀게 하고, 같은 종류의 이미지에서 나온 캡션은 거리를 가깝게 하는 방식으로 학습을 진행한다. 이것또한 transformer 기반이다.\n** Pre-trained model로써 굉장히 많이 사용한다! ** CV, NLP 두 분야에서 모두 사용 가능한 굉장히 범용적인 Pre-trained model이다. 텍스트든 이미지든 뭐든지 encoding 가능하다.\nNeRF라고 3D 이미지 생성 모델이 있는데 이게 CLIP을 transfer leraning한 것이라고 한다.\n질의응답 # RNN, LSTM, GRU 등의 기존 RNN 계열의 모델들은 모두 transformer로 대체됐는가? 아니다. 미래 정보 예측의 경우 여전히 RNN을 사용하는 분야가 많다고 한다. Pattern signal을 중점적으로 봐야되는 분야들이 대표적으로 RNN 계열을 사용한다. Transformer는 paramter가 많은 것은 아니지만 보통 많은 layer를 쌓아서 구성하고 중간 생성물인 Q, K, V에 대한 연산들이 많은 메모리를 요구한다. $softmax(QK^T)$가 $sequence^2$에 비례하게 메모리를 요구하기 때문이다. 따라서 경량화를 해야할 때 RNN 계열을 사용한다. Word2Vec, GloVe RNN, LSTM은 점점 사용하지 않는 것 같다. 최근 기술에 집중해서 공부하는 것이 맞을까? 최근에는 Pre-trained model에 별도의 까다로운 embedding을 거치지 않은 데이터를 넣어도 잘 작동하는 경우가 많다. 하지만 어찌보면 Word2Vec, GloVe 또한 Embedding layer에 해당한다고 할 수 있다. 동작 원리를 잘 파악하는 것은 도움이 된다. 모델이 모든 정보를 가지고 있으면 되지 않을까? 예상치 못한 정보에 대한 처리를 위해서 그러한 형태로 모델을 구성하지 않는다. 또한 현실적으로 세상의 모든 정보를 한 모델이 알고 있기란 불가능하다. ","date":"19 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-19-recent-trends-of-nlp/","section":"Posts","summary":"","title":"Recent trends of NLP","type":"posts"},{"content":" GPT-2 # GPT-1과 기본적인 구조는 같다.\nTransformer layer를 보다 더 많이 쌓았다. 다음 단어를 예측하는 task로 학습을 진행. 더 많은 학습 데이터 사용 보다 양질의 데이터 사용 zero-shot setting으로 다뤄질 수 있는 잠재적인 능력을 보여줬다 ref: zero shot learning 한 번도 보지 못한 데이터를 분류 가능하도록 학습하는 것. ![](/assets/images/Advanced Self-supervised Pre-training model/fb607f1a-4fe0-47cc-9754-4f58b7450794-image.png)\nGPT-2의 기본적인 task는 위와 같이 지문이 주어지면 순차적으로 다음에 올 단어들을 에측하는 language model이다. 소설과도 같은 지문이 주어졌을 때, 실제 사람이 쓸 법한 공상적인 이야기를 이어서 썻다.\ndecaNLP, motivation of GPT-2 # 기존의 nlp task는 task별로 다른 모델 구조와 해결 방법을 고안해야했다. 가령, 문장의 긍/부정을 판단하기 위해서는 CLS token을 output layer에 통과시킨 후 이를 binary classification에서 사용한다. 혹은 QnA를 진행하기 위해서 QnA만을 위한 모델 구조를 별도로 고안해야 했다.\nGPT-2로부터 3~4년 전의 decaNLP에서는 모든 NLP task를 질의응답으로 통합해서 구성할 수 있다는 것이 요지다. 즉, 모든 task를 자연어 생성 task로 간주하는 것이다.\n** e.g., 문장의 긍/부정 판단 task를 다음과 같이 구성한다.**\n임의의 문장을 입력 후 \u0026lsquo;What do you think about this document in terms of positive or negative?\u0026lsquo;라는 질문을 추가로 입력 모델이 1번 문장에 대한 긍/부정 판단을 기대한다. 2번의 질의 문장은 자유롭게 바꿀 수 있다. \u0026lsquo;Do you think wheter this sentence is postivie or negative?\u0026lsquo;처럼 말이다.\n문단의 요약을 원한다면 \u0026lsquo;What is the summarization of the above paragraph?\u0026lsquo;를 1번 문장 다음에 추가하면 된다.\nDataset of GPT-2 # 양질의 글으 얻기 위해 아래의 웹 사이트들을 dataset으로 사용했다고 한다.\nReddit 3개 이상의 karam(up-vote)를 받은 discussion이 외부 링크를 포함하고 있다면, 해당 링크의 document가 양질의 데이터를 포함하고 있다고 가정 이러한 방식으로 Reddit의 데이터와 Reddit에서 참조하는 외부 링크들의 document를 dataset으로 활용 45M links 수집 8M removed Wikipedia documents Dragnet(경찰 시리즈물인 것 같다) newspaper Preprocess # BPE(Byte pair encoding) Minimal fragementation of words across multiple vocab tokens Modification of models # Layer normalization sub-block 단위로 수행하거나 기존의 normalization 위치를 변경 Initialization of weight in residual Layer Residual Layer의 index가 커질수록 weight 초기화 값을 $\\sqrt{n}$에 반비례하게 만들었다. Output에 가까워질수록 Layer의 선형변환의 출력들이 0에 가까워지도록 하기 위함이다. 뒤쪽 layer의 역할을 줄여주는 것이 목적. 멘토님 답변: 모델의 뒤로 갈수록 high dimentional한 특징을 학습하는데, 이러한 feature에 영향을 덜 받기 위해서? Question Answering # CoAQ(Conversationi question answering dataset)을 사용.\n해당 데이터셋을 전혀 사용하지 않은채로 테스트할 경우 F1 score = 55% Fine tuned 후 F1 score = 89% zero-shot leraning에 대한 가능성을 GPT-1보다 더욱 크게 보여줬다.\nSummarization # ![](/assets/images/Advanced Self-supervised Pre-training model/54b0c314-4525-41b1-9167-74b8e9a876e5-image.png)\n이 부분은 교수님의 설명이 부족한 것 같아 좀 더 찾아봐야 될 것 같다. 모든 학습 데이터에 대해서 TL;DR; token이 존재할리가 없는데..\nGPT-2의 학습 데이터에는 TL;DR; token의 등장 이후에 한 줄 요약을 하는 데이터가 많다. 따라서 zero-shot learning처럼 별도의 fine tuning 없이 원하는 지문의 끝에 Tl;DR; token을 붙이는 것만으로도 summarization task를 수행할 수 있다고 한다.\nTranslation # Summarization의 TL;DR; token처럼, 번역하고자 하는 문장의 뒤에 \u0026lsquo;in French\u0026rsquo;와 같은 문구를 추가하면 translation task를 수행한다.\nGPT-3 # GPT-2보다 굉장히 많은 parameters, transformer layer, batch size로 학습해보니 GPT-2보다 좋은 모델이 나왔다고 한다.\nFew-shot learning # GPT-2에서는 zero-shot, few-shot learning에 대한 가능성을 보여줬다면 GPT-3는 이에 대한 매우 좋은 성능을 보여줬다.\n![](/assets/images/Advanced Self-supervised Pre-training model/2668f8f5-e35d-47c3-b548-5d41228b28be-image.png)\n모델을 전혀 변경하지 않고 아래의 inference task를 수행했다.\nZero-shot: GPT-3는 번역 데이터를 학습하지 않았지만, 별도의 fine tuning을 전혀 하지 않고도 translation이 가능했다. One-shot: 학습 데이터에 대한 예시를 한 쌍만 보여준다. Few-shot: 학습 데이터에 대한 예시를 여러 쌍을 보여준다. ![](/assets/images/Advanced Self-supervised Pre-training model/68579874-7c27-4d3e-8938-85890f933e74-image.png)\n모델의 크기(parameter의 개수)를 늘릴수록 zero-shot, one-shot, few-shot에 대한 성능은 계속 증가한다. 모델이 클수록 모델의 동적인 학습 능력이 올라갔음을 알 수 있다.\nALBERT # A Lite BERT. BERT, GPT와 같은 거대 모델을 self-supervised learning으로 학습하기 위해서는 많은 메모리, parameters, batch size가 필요하다. 하지만 이러한 자원은 한정적이다. 이를 경량화하면서 오히려 BERT보다 더 좋은 성능을 가진 모델이 ALBERT다.\nObstacles Memory Limitation Training speed Solutions Factoried Embedding Parameterization Cross-layer Parameter Sharing (For performance) Senetence Order Prediction Factorized Embedding Parameterization # ![](/assets/images/Advanced Self-supervised Pre-training model/9c19439d-673b-4846-8cc6-a58de9926c6b-image.png)\n기존 transformer: transformer 내의 residual block 때문에 Embedding의 차원과 출력 차원이 동일해야하기 때문에, 모든 Layer의 입출력 차원들은 같다. ALBERT: Layer별 출력 차원들을 축소시키자. Motivation # Embedding layer: 문맥을 전혀 고려하지 않고 Word만이 가진 정보를 상수로써 표현하는 vector hidden state vector: 문맥이 고려된 semantic한 정보들을 포함하고 있는 vector Embedding layer는 hidden state vector에 비해 담고 있는 정보가 적다. 따라서 Embedding layer를 쪼개서 보다 작은 차원으로 표현해보자.\nImplementation # ![](/assets/images/Advanced Self-supervised Pre-training model/0dee7a07-9347-4262-be6c-1ce1985b6109-image.png) 본래 4차원으로 word들이 Embedding 된다고하자. 그러면 위 도식과 같이 Word embedding layer 또한 4차원이다. 이러한 Embedding layer를 2차원으로 줄이여서 모델에 입력하고자 한다.\n위 도식에서 V x E에 해당하는 layer를 transformer의 입력으로 준다면 transformer의 paramters 수는 이전보다 줄어들 것이다.\nresidual 연산을 위해서는 입력 차원과 동일해야하는데, transformer는 2차원을 출력할 것이다. 따라서 residual 이전에 위 도식의 E x H와 같이 본래 차원으로 되돌려주는 layer를 추가한다. 결과적으로 transformer의 parameter는 줄고 출력은 입력과 동일해진다. 이러한 방식을 Low rank matrix factorization이라고 한다.\nFactorized Embedding Parameterization은 기존의 Word Embedding 결과를 그대로 사용하는 것에 근사하는 결과를 보여주는 것으로 알려져 있다.\nCross-layer Parameter Sharing # ![](/assets/images/Advanced Self-supervised Pre-training model/b90f1049-accb-40d3-9c85-c7cd9e623a3f-image.png)\ntransformer에서 학습이 진행되는 parameters는 self-attention layer별로 가지고 있는 $W_n^Q, W_n^K, W_n^V$와 concatenate된 matrix의 차원을 줄여주는 $W_O$이다. transformer를 쌓을수록 parameter의 수가 늘어나는데, ALBERT에서는 이 parameter들을 self-attetion들이 공유하도록 해봤다.\n![](/assets/images/Advanced Self-supervised Pre-training model/fc8c39c3-3ef7-4f74-ab2a-3801921c84cf-image.png)\nShared-FFN: Only sharing feed-forward network parmeters Shared-Attnetion: Only sharing attention parameters All-shared: Both of them All-shared를 해도 not-shared에 비해서 성능이 나쁘지 않다.\nSentence Order Prediction # BERT는 두 가지 형태로 학습이 진행된다.\nMaksed language modeling: k%를 mask token으로 치환 후 학습 Next sentence prediction: 두 문장을 sep token으로 concat후 학습 BERT의 next sentence prediction 실효성이 없다는 연구결과들이 나았다. next sentence prediction을 학습 과정에서 제외하고 masked language modeling만을 수행해도 모델의 성능이 좋기 때문이다.\n왜냐하면 next sentence prediction에서 negative samples를 판단하는 것은 유사한 단어가 나타나는지 파악만 해도 가능하기 때문이다.\ne.g., 사회면의 기사와 스포츠면의 기사는 내용과 사용되는 단어들이 매우 상이하기에 두 분야에서 sampling한 단어들은 next sentence가 아님을 쉽게 예측 가능하다.\nnext sentence 관계에 있는 문장들은 동일하거나 유사한 단어들이 자주 등장할 것이다. next sentence 판별이 또한 예측이 매우 쉬워진다.\n고차원적인 판단이 아니라 단어의 출현을 기반으로 next sentence prediction을 수행하기 때문에, 이렇게 학습된 모델은 얻게 next sentence prediction으로부터 얻은 정보가 많지 않을 것이다\nALBERT는 next sentence prediction을 변형해서 sentence order prediction으로 학습을 진행했다. 이 방법론은 올바른 순서의 두 문장을 concat해서 모델에 입력하면 올바른 순서라고 인지해야 한다. 반대로 올바르지 않은 순서의 두 문장(Negative smaples)을 concat해서 모델에 입력하면 올바르지 않은 순서라고 인지해야 한다.\noverlapped 되는 단어들이 거의 없도록 두 문장을 뽑기 위해서 동일 문서에서 sampling을 한다. 단어의 출현을 기반으로 학습하지 않고 문맥을 고려하여 Sentence order prediction을 학습하도록 하기 위함이다.\n![](/assets/images/Advanced Self-supervised Pre-training model/0c88484e-052c-4874-a174-560c90c82f97-image.png)\nNSP(Next sentence prediction)과 SOP(Sentence order prediction)의 결과를 나타낸 표이다. NSP는 성능 향상이 미미하고 오히려 떨어지는 case도 있다. SOP는 큰 성능 향상을 이뤘다.\n![](/assets/images/Advanced Self-supervised Pre-training model/f344c656-a105-476d-b5da-0d6bde95cffe-image.png) NLP task를 평가하기 위한 데이터셋인 GLUE에서도 ALBERT가 기존 모델들보다 더 좋은 성능을 보여주는 것을 알 수 있다.\nELECTRA # Efficiently Learning an Encoder that Classifies Token Replacements Accurately. ICLR 2020에서 구글 리서치 팀이 발표한 논문이다.\n![](/assets/images/Advanced Self-supervised Pre-training model/634ddd5e-ec24-487b-ba83-2027e083684c-image.png) 두 모델은 Adversarial(적대적) learning 형태로 학습이 진행된다. 관련 링크: Adversarial learning, Adversarial learning 긴 설명\nGenerator: Maksed language model BERT와 같은 원리로 동작 Maksed 문장을 복원 Discriminator: mask token에 위치한 단어가 원본 단어인지, replaced된 단어인지 추론. Transformer 기반으로 Binary classification 진행. GAN(Generative adversarial network)에서 착안했다. 학습 데이터의 Ground truth와 Discriminator의 결과를 비교하면서 학습한다. ** Pre-trained model로는 Generator가 아니라 Discriminator를 사용한다. **\nPerformance # ![](/assets/images/Advanced Self-supervised Pre-training model/1770209f-2f5e-4ad0-9fd8-4192bf586d95-image.png)\n동일 연산량에서 기존 모델에 비해 더 높은 GLUE score를 기록했다. ALBERT와 함께 많은 downstream task에서 활용되고 있다.\nDownstream Task의 의미\nLight-weight models # BERT, GPT, ELECTRA들은 self-attention block을 매우 많이 쌓아서 성능 향상을 이뤘기 때문에 parameters가 매우 많다. 이러한 parameters를 줄여서 모델의 크기와 학습 속도를 줄이는 경량화 모델이 연구되고 있다.\n클라우드, 고성능컴퓨팅 자원이 아닌 모바일 기기에서 빠르고 저전력을 소모해서 모델을 돌리기 위해 사용된다.\nDistillBERT # HuggingFace가 NeuralPS(뉴립스) 2019에서 발표한 논문.\nteacher, student model로 구성된다.\nteacher: 기존의 거대한 구조를 유지하면서 feature들 학습하고 student model을 학습 시킨다. student: teacher보다 layer, parameters가 적음에도 teacher의 feature를 모사하려고 노력하는 모델. 동작 원리 # ** Teacher: BERT의 Seq2Seq와 동일하다. **\n\u0026lsquo;I go home\u0026rsquo;을 입력으로 했을 때, Seq2Seq의 teacher는 \u0026lsquo;I\u0026rsquo;에 대해서 \u0026lsquo;go\u0026rsquo;를 예측하고자 한다. \u0026lsquo;I\u0026rsquo;에 대한 입력에 대해서 vocabulary size만큼의 vector 생성되고 여기에 softmax를 취한다. 2번의 결과에 대해서 확률값이 가장 큰 값은 \u0026lsquo;go\u0026rsquo;에 해당하는 index일 것이다. ** Studnet: Teacher의 출력인 target distribution을 ground truth로 하여 학습한다. ** 단순히 Teacher의 출력을 모사하도록 한다.\nTinyBERT # Teacher, student로 이루어지는 구조는 DistillBERT와 동일하다. TinyBERT는 teacher의 출력 distribution만을 모사하는 것이 아니라, 중간 생성물인 query, key, value, hidden state들까지도 모사하도록 한다.\nTeacher에 비해서 student는 경량화모델이기 때문에 layer의 dimension들이 teacher에 비해 작다. 모사할 때 이것이 문제가 되는데, teacher의 dimension을 축소시키는 fully connected layer를 추가해서 해결했다고 한다.\nFusing Knowledge Graph into Language model # BERT는 문맥을 파악하거나 단어들간의 유사도를 구하는 task에서는 뛰어났지만 주어진 데이터셋 외의 정보들에 대해서 효과적으로 처리하지 못하기도 한다.\ne.g., 땅을 파는 행위에 대한 데이터로 \u0026lsquo;땅을 팠다.\u0026lsquo;라는 문장만이 존재하는 데이터셋을 가정하자. Question Answering task에서 이 문장에 대해서 \u0026lsquo;어떤 도구를 사용했는가?\u0026lsquo;를 묻는다면 기존의 모델은 답하기 어려울 수 있다. 정보가 없기 때문이다. 사람의 경우 \u0026lsquo;상식\u0026rsquo;이라는 외부 정보를 활용해 다양한 상황에 대한 사용 도구를 추론하고 답할 수 있을 것이다.\n이와 같이 외부 정보를 모델에서 활용하고자 하는 분야다.\n** Knowledge graph: 정보들을 쳬게화해서 정리한 것 **\n대표적인 모델\nERNIE Information fusion layer takes the concatenation of the token embedding and entity embedding KagNET For each pair of question and answer candidate, it retrieves a sub-graph from an external knowledge graph to capture relevant knowledge ","date":"18 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-18-advanced-self-supervised-pre-training-model/","section":"Posts","summary":"","title":"Advanced Self-supervised Pre-training model","type":"posts"},{"content":" Recent trends # transformer, self-attention은 기계번역 외의 분야에서도 쓰이고 있다! transformer 논문에서 제시된 것처럼 6개의 transformer를 쌓지 않고 12개, 24개 혹은 그 이상으로 쌓는 것만으로도 성능 향상이 있는 것이 실험적으로 밝혀졌다. 특별한 기본 모델의 구조 변경 또한 없다! 이러한 모델을 위한 대용량 데이터를 학습하기 위해 Self-supervised learning framework를 사용한다. e.g., BERT, GPT-3, XLNet, ALBERT, RoBERTa, Reformer, T5, ELECTRA 위에 언급된 방식대로 학습 후, 여러 domain과 task에 맞게 transfer leraning하는 것만으로도 해당 분야에 특화된 모델보다 훨씬 좋은 성능은 낸다! 활용분야: Recommender system, drug discovery, computer vision\u0026hellip; 한계점: Greedy decoding에서 벗어나지 못하고 있다. = 왼쪽에서 오른쪽으로 decoding 해가며 해당 step에서 최선의 선택을 하는 형태 GPT-1 # NLP의 여러 task를 통합했다.\n![](/assets/images/Self-supervised Pre-training models/9dcd16aa-dec1-41de-a3cc-bab14982e8c3-image.png) transformer를 12개 쌓았다.\n일반적인 Seq2Seq # 일반적인 seqeunce의 학습 과정은 이전 포스팅에서 다룬 기본적인 transformer와 동일하다. \u0026lsquo;I go home\u0026rsquo;을 출력하기 위해서는 입력으로 \u0026lsquo;[SOS]\u0026lsquo;를 받으며 \u0026lsquo;I\u0026rsquo;를, \u0026lsquo;I\u0026rsquo;를 받으면 \u0026lsquo;go\u0026rsquo;를 출력하도록 한다.\nClassification # ![](/assets/images/Self-supervised Pre-training models/abfa3113-89f0-483c-98e4-060457015953-image.png) Seq2Seq를 수행하면서 Text의 앞뒤에 start, extract token을 넣어서 구현한다.\ntransformer 학습이 수행되면 input과 동일한 형식의 encoding vector가 형설된다. encoding vector에서 extract token에 위치한 값을 활용해 classification을 한다. e.g., 문장의 긍/부정 Seq2Seq는 extract token을 제외한 나머지 encoding vector의 값들을 활용해 지속할 수 있다. Entailment # ![](/assets/images/Self-supervised Pre-training models/6cb1708f-394e-4e05-84bb-799250c9db2d-image.png) 함의라는 뜻이다. Premise(전제)와 Hypothesis(가설)이 논리적 내포 관계인지, 모순관계인지 판단하는 task다. GPT-1에서는 위 도식과 같이 Premise와 hypothesis를 하나의 sequence로 만들어서 task를 해결한다. Extract token이 transformer 상에서 query로 사용되어, 적절한 정보들을 sequence 상의 다른 정보들로부터 추출한다.\n두 문장 사이는 Delim이라는 delimiter를, sequence의 끝에는 Extract 토큰을 넣어준다. Encoding vector의 extract token을 ouput layer에 통과시켜, 논리적 관계를 판단한다. Transfer learning # 특정 task에 적합하게 학습된 GPT-1 모델을 다른 task에서도 사용할 수 있다. 가령, 긍/부정을 판단하는 task를 수행하는 모델을 활용해 주제 분류를 하고자 한다고 해보자.\n![](/assets/images/Self-supervised Pre-training models/abfa3113-89f0-483c-98e4-060457015953-image.png)\n기존의 Output layer는 긍/부정을 판별하기 위한 linear neural network이다. 따라서 해당 layer를 삭제하고, 주제 분류를 위한 linear nueral network를 transformer 뒤에 붙인다.\n마치 CNN에서 기존의 모델에서 마지막 classification layer의 수만을 바꿔줘서 임의의 classification task를 수행하는 것과 마찬가지이다. Pre-trained 전체 네트워크는 유지하고 ouput layer를 initialize해서 학습을 진행한다.\nSelf-supervised learning # GPT는 Pre-trained 모델을 학습할 때 별도의 라벨링이 되지 않은 데이터를 사용해서 Seq2Seq task를 수행하도록 한다. 다음 단어를 예측하는 task이기 때문에 라벨링이 필요하지 않은 것이다. 이 때 self supervised learning이 사용된다고 한다.\n하지만 주제 분류 task는 라벨링이 된 데이터가 필요하다. 보통 라벨링된 데이터는 라벨링 되지 않은 데이터에 비해 매우 소량이기 때문에 모델 학습에 불리하다.\nself-supervised learning으로 pre-trained된 모델이 있기 때문에 대용량 데이터로 대부분의 parameter들이 의미있게 초기화돼있다. 따라서 소량의 라벨링 데이터를 사용해 transfer learning을 하는 것만으로도 유의미한 성능의 모델을 만들 수 있다.\n![](/assets/images/Self-supervised Pre-training models/2ee91262-d36c-41c8-942e-b0a4ad5eac20-image.png)\n위 표는 특정 task만을 위해 고안된 모델과 데이터의 결합을 GPT와 비교한 것이다. 미리 대용량 데이터로 학습 한 후, transfer learning을 한 것이 더 좋은 성능을 보여준다.\nBERT # ![](/assets/images/Self-supervised Pre-training models/449f50bc-7b49-45a2-bb43-29df3b933f7b-image.png)\nBidirectinoal Transformers for Language Understading이라고 한다. 이전에도 LSTM을 활용해 대용량의 데이터를 self-supervised learning을 하도록 시도했지만, BERT가 훨씬 성능이 좋다.\nMotivation # RNN 계열은 한 가지 방향으로만 정보를 취득한다. 전체 맥락을 이해하고 독해를 해야하는 task에서 매우 취약한 방법론이다. 따라서 이를 해결하기 위해 Bi-Directional하게 정보를 취득하고자 하는 것이 BERT의 Maksed language model이다.\nMaksed Language Model(MLM) # 입력 sequence의 단어들을 랜덤하게 mask로 치환한다. 그리고 mask된 단어르 유추하도록 학습을 진행한다.\nhyperparameter $k$: 어느 수준의 확률로 단어들을 mask로 변경할지\n$k$가 너무 높을 때: 정보가 너무 많이 가려져서 mask 데이터를 유추하기 어렵다. $k$가 너무 낮을 때: 학습에 걸리는 시간이 너무 오래 걸리거나, 학습 효율이 떨어진다. 보통 $k=15$를 쓴다.\n** 부작용 ** 15%의 단어들을 mask token으로 치환하고자 했다할지라도 15%에 해당하는 데이터를 모두 mask toekn으로 치환하는 것은 부작용이 발생한다!!\nPre-trained model은 15%의 데이터가 mask token인 것에 익숙하지만, 실제 test data는 그렇지 않을 가능성이 다분하다. 이러한 양상이 transfer learning에서 큰 방해가 된다.\n** 해결 방법 ** $k%$에 해당하는 데이터를 다음과 같이 분류한다.\n80%의 데이터는 mask token으로 치환한다. 10%의 데이터는 random word로 치환한다. 이상한 단어가 입력으로 들어오더라도 제대로 처리하기 위함이라고 한다. 10%의 데이터는 원본을 유지한다. 소신있게 원본이 옳다고 말할 수 있기 위함이라고 한다. Next Sentence Prediction # GPT에서의 task처럼 문장 단위의 task를 처리하기 위해 BERT에서 제안한 방법론이다.\n![](/assets/images/Self-supervised Pre-training models/ff485e36-afd9-4b14-af63-693cb810cbef-image.png)\n마치 GPT에서의 extract, delimiter token과 마찬가지로 CLS, SEP token을 사용했다.\nSEP: 문장과 문장 사이를 구분해주는 token CLS: classification 정보가 담기는 token. 문장의 맨 앞에 온다. MASK: Mask language model에서 쓰인 mask. 위 그림에서 나온 task는 두 문장이 인접한 것이 옳은가를 판별하는 것이다. 즉, CLS token은 binary 데이터를 담게 된다. transformer에는 그림의 모든 정보가 한꺼번에 입력되어 네트워크가 알아서 CLS token에 예측 결과를 출력한다.\nBERT 구조 # Model Architecture # L: Layer H: Attention encoding vector의 dimension A: Attention head per Layer BERT Base: L=12, A=12, H=768 BERT Base: L=24, A=16, H=1024 Input Representation # WordPiece embedding: Word 단위의 Embedding이 아니라 Subword 단위로 Embedding(30,000 WordPiece) Learned positional embedding ![](/assets/images/Self-supervised Pre-training models/7ce8492c-8515-449c-92c3-a0b0daf049c8-image.png) Transformer에서 sin, cos에 미리 정해진 offset을 사용하여 positional embedding에서 사용할 matrix를 계산했다. BERT에서는 이러한 matrix조차도 end-to-end로 학습한다. 마치 Word2Vec에서 embedding vector를 만들기 위해 network를 학습하듯이! Segment Embedding Segment Embedding # ![](/assets/images/Self-supervised Pre-training models/8949e7c7-6900-43fa-a3b9-ccdc797248a8-image.png) Positional embedding에 의해서 순서성이 부여되지만, 문장 간의 결합을 인지하지는 못한다.\nNext sentence prediction 같은 경우 SEP token을 기준으로 \u0026lsquo;he\u0026rsquo;는 첫번째 단어라고 처리를 해야하지만 sequence 단위로는 첫번째가 아니기 때문에 발생하는 문제점이다. 따라서 Segement embedding을 도입한다.\nSEP 단위로 앞 뒤 문장이 다름은 Segment embedding을 통해 계산 후 단순히 더해주기만 하면 된다.\nBidirectional # ![](/assets/images/Self-supervised Pre-training models/4bb1e6ca-012b-47e1-9b14-f19482403798-image.png)\nGPT는 Masked self-attention 사용해서 다음의 정보를 참고자하지 못하도록 했다. 다음 단어를 예측해야 하는데 다음 단어를 보면 안되기 때문이다.\nBERT는 seuquence가 mask로 처리돼있기 때문에 전체 sequence를 본다. 왜냐하면 전체 맥락을 보고 mask를 예측해야하기 때문이다. 그래서 BERT는 일반적인 transformer의 self-attention을 사용한다.\nTransfer learning # ![](/assets/images/Self-supervised Pre-training models/be38af2c-5672-47d8-b674-405d529f9c80-image.png)\nSelf-supervised learning으로 만든 Pre-trained BERT가 있다고 해보자. 이를 활용해서 가능한 task는 아래와 같다. GPT와 유사하다!\nSentence pair classification # ![](/assets/images/Self-supervised Pre-training models/854f091e-26f9-4c20-beff-7e9a0bd29a5a-image.png)\n두 문장을 SEP token 단위로 묶는다. 첫번째 인덱스에는 CLS token을 넣어서 BERT를 통과시킨다. Encoding vector의 첫번재 인덱스는 CLS token인데, 이것을 output layer에 넣어서 class label을 얻는다. Single sentence classification # ![](/assets/images/Self-supervised Pre-training models/d2895a2e-c83c-4d98-9c73-913e2d4b8245-image.png) Sentence pair classification과 동일하다. 한 문장이기 때문에 CLS token만 있다.\nSingle Sentence Tagging # ![](/assets/images/Self-supervised Pre-training models/cf90374c-0ed6-4100-82ed-fceedc0711d6-image.png)\nword 별로 Encoding vector가 존재할 것인데, 이것들 각각을 output layer에 통과시켜서 품사, 형태소 등의 정보를 판별하도록 한다.\nBERT vs GPT-1 # Training size GPT: BookCorpus(800M words) BERT: BookCorpus, Wikipedia(2,500M words) BERT: SEP, CLS token이 있다. Segment embedding을 통해 문장들을 구분한다. Batch size BERT: 128,000 words GPT: 32,000 words 일반적으로 batch size가 크면 학습이 더 안정되고 잘 된다고 한다. gradient descent에서 여러번의 학습을 통해 얻어진 gradient의 평균을 사용하는 것보다, 한번에 계산된 gradient를 사용하는 것이 더 좋다! Task specific fine-tuning GPT: 여러 task에서도 5e-5의 learning rate를 썻다. BERT: task-specific하게 learning rate를 fine tuning했다. MRC(Machine Reading Comprehension), Question Answering # 문장을 독해해서 답을 하는 것. ![](/assets/images/Self-supervised Pre-training models/d9e0adf8-0f25-407f-be4e-f9f278d99288-image.png) 위와 같이 Document에 대해서 주체와 행동에 대한 독해가 제대로 이루어져야지 답을 할 수 있다.\nSQuAD 1.1 # MRC를 활용한 QA model의 성능을 테스트하기 위한 Standford QuA Dataset. 지금은 2.0도 있다고 한다. 테스트셋에 대한 스코어 리더보드도 있다.\nSquAD 1.1 해결 과정 # ![](/assets/images/Self-supervised Pre-training models/5d8ffe38-4dd6-472d-a1af-f22eff6f26b0-image.png) 보통 질문의 답은 지문의 특정 위치에 위치하는데, 이 위치를 찾도록 한다.\n데이터셋의 질문과 질문을 위한 지문을 SEP token을 활용해 concatenate한다. 1번의 데이터에 대한 Encoding vector를 얻는다. 답의 start point을 찾기 위해서 Encoding vector를 scalar로 만들기 위한 fully connected layer를 추가하고, softmax를 수행한다. 답의 end point을 찾기 위해서 Encoding vector를 scalar로 만들기 위한 fully connected layer를 추가하고, softmax를 수행한다. 한 번의 encoding vector에 두 개의 fully connected layer를 사용해서 답의 start, end point를 구한다. SquAD 2.0 해결 과정 # 1.1은 항상 질문에 대한 답이 있지만, 2.0은 질문에 대한 답이 지문에 없는 경우도 포함됐다.\n따라서 답이 있는지 찾는 task가 선행된다. 답이 있다면 SquAD 1.1 해결 과정을 수행하면 된다.\n질문과 지문을 concatenate하고 CLS token을 추가한다. CLS token의 값을 활용해 binary classification을 하는 fully connected layer를 추가한다. Cross entropy를 활용해 분류한다. SWAG # 주어진 문장에 대해서, 다음에 나타날법한 문장을 고르는 task. ![](/assets/images/Self-supervised Pre-training models/3b64218d-f588-4185-9144-e47eadeace3e-image.png)\nPermise 문장과 과 보기에 해당하는 문장들을 각각 concatenate한다. e.g., Permise + 보기1, Permise + 보기2, \u0026hellip; 각각의 concatenate 결과에 대해서 encoding vector를 구한다. 각각의 encoding vector를 output layer에 통과시켜서 scalar를 얻는다. 동일한 output layer를 사요한다. scalar 결과들에 대해 softmax를 취해 가장 높은 확률의 보기를 고른다. BERT: Ablation study # ![](/assets/images/Self-supervised Pre-training models/b2eb659a-3768-4e91-9fc0-5d06324adad9-image.png) BERT는 parameter를 많이 쓸수록 성능이 올랐다.\n","date":"18 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-18-self-supervised-pre-training-models/","section":"Posts","summary":"","title":"Self-supervised Pre-training models","type":"posts"},{"content":" 7주차 학습정리 # 강의 복습 내용 # NLP (10~14번 포스팅) # https://velog.io/@naem1023/series/NLP\n과제 수행 과정 / 결과물 정리 # 멘토링 답변 # ** 피어세션 ** 피어세션에서 정말 많은 질의가 오갔다. 그 중에서 해결이 안되거나 애매모호한 부분들만 모아서 질문을 했고 이에 대한 답변을 정리하면 다음과 같다.\ntransformer에서 d_k로 나누는 이유?\nd_k와 n이 비례하여 증가하기 때문에 gradient exploding을 방지하기 위해서 쓰는거라고 예상했었다. 결론적으로 랜덤 확률 변수를 n으로 나누면 분산을 n^2으로 나누기 때문에, 수학적으로 자명한 사실을 사용했던 것으로 이해했다. positional encoding에서 sin, cos을 쓰는 이유는?\nSin과 Cos 함수는 값이 증가하지 않고 주기성을 가지면서 유니크한 값을 얻을 수 있다. 또한 선형변환을 통해서도 독자적인 값을 가짐을 어느정도 보장할 수 있기 때문에 사용한다. 시간복잡도 (해결)\n이 부분은 약간 이해에 착오가 생기신 것 같습니다. \u0026ldquo;Complexity per Layer\u0026rdquo; 부분이 행렬연산에서 시간복잡도입니다. 여기서 이야기하는 \u0026ldquo;Sequential Operation\u0026quot;은 다른 이야기입니다. RNN을 예로 들면, t번째 time step의 hidden state를 계산하려면, t-1번째 hidden state까지의 연산이 되어있어야만 연산이 가능합니다. 그러니까 Sequence 길이에 비례하게 연산이 진행되어야겠죠. (Sequence 끝까지 연산을 하려면 t=1일 때를 계산하고, 그 다음에 t=2를 계산하고 \u0026hellip; 병렬처리가 될 수가 없죠?) 그래서 두 번째 Recurrent 부분에 O(n) 이라고 표기가 되어있는 것입니다. Sequence 길이에 비례하니까요 ! 반면 Transformer의 경우 전체 Sequence에 대한 모든 attention을 한 번에 계산합니다. Transformer 부분 강의자료를 다시 살펴보시면, Query 행렬 Q를 연산할 때, 입력행렬의 차원수가 (n * d)인걸 보시면, sequence내 전체 token에 대해 연산을 한번에 진행하는 것을 알 수 있죠. 그래서 Sequence 길이와 관계 없는 O(1)이 됩니다. GPU 성능\u0026hellip; 이야기는 어디서 나온건지 말 모르겠네요 ! Transformer를 이해하는데 가장 좋은 글인 것 같습니다. 찾아보면 번역본이 있어요 왜 sqrt(d)로 나누는가? (해결)\n이는 Attention is all you need! 논문을 다시 한 번 꼼꼼히 살펴보시면 이해할 수 있을 겁니다. 이를 나눠주지 않게 되면, Softmax 내에 있는 값이 행렬곱이 벡터 내적연산 결과다보니 값이 너무 커서 나눠주게 됩니다. 첨부 이미지 참고해주세요 왜 Sin, Cos 인가?\n너무 다양한 이유가 있지만 단순 정수의 나열은 뒤로 갈수록 값이 너무 커지는 문제가 있고, 정수는 우리가 임베딩에 포함된 값들에 비해 그 수가 너무 크기 때문에 아주 \u0026ldquo;작은\u0026rdquo; 위치 정보만을 임베딩에 추가해야하는 원래 취지를 벗어나게 됩니다. sin, cos은 그 값을 sequence 길이에 따라 그 주기성을 이용해 조절할 수 있고, 수학적으로도 안정적 (?) 이기 때문에 사용한다는 데 이는 제가 보증할 수 없습니다 (ㅎㅎ) Positional encoding을 이해하는데 도움이 되는 링크는 아주 많습니다. 몇 가지 첨부해드릴게요 What is the positional encoding in the transformer model? Why does the transformer positional encoding use both sine and cosine? Transformer Architecture: The Positional Encoding 왜 Post-Layer Normalization이 문제 되는가와 warm-up\n지금 써주신 두 가지 화두가 같은 문제입니다. LN을 후에 걸어주게 되면, 값의 안정화가 후에 일어나기 때문에 이 부근 미분값이 초기에 크기 때문에 learning rate에 민감해진다는 이슈입니다. 그래서 warm-up이 필요한 것이구요. (정말 간단하게만 설명했습니다) 자세한건 캠퍼분들께서 아래 논문을 리뷰해보시면 좋을 것 같습니다. On Layer Normalization in the Transformer Architecture Transformer에서 Gradient Vanishing\n음 이 문제는 이슈가 되지 않는 것이 1번이랑 관련이 있습니다. Vanishing이라는 게 결국에 sequence가 길어지면서 뒤의 sequence에서 생긴 gradient가 앞의 sequence까지 전달되는 과정에서 값이 작아지기 때문인데, Transformer의 경우 전체 sequence를 동시에 보기 때문에 그런 이슈에 대한 언급이 많이 없는 것 같습니다. 또한 Skip-connection이 Transformer에서도 사용되는데, 이 부분이 어느 정도 Vanishing을 해결해주지만, 결정적이라고 생각하지는 않습니다. RNN과 구조상 차이인 \u0026lsquo;모든 sequence를 한 번에 본다\u0026rsquo;가 더 이유라고 보는게 맞을 것 같네요 ! 피어세션 정리 # 위에서 언급한 질문들에 대해서 논의하고, 멘토님의 답변을 다시 우리들만의 언어로 이해하고 정리하기 위해 노력했다.\n학습 회고 # 21/09/06: transformer 1강 공부 21/09/07: transformer 2강 공부 21/09/08: BERT 공부 21/09/09: 나머지 강의들 공부. transformer 복습 및 정리. 21/09/10: 과제 리뷰 및 정리.\n","date":"17 September 2021","externalUrl":null,"permalink":"/posts/records/2021-09-17-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-7%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 7주차 학습정리","type":"posts"},{"content":"\u0026lsquo;Attention is all you need\u0026rsquo;(2017) 논문 이전의 attention은 LSTM, GRU에 add-on처럼 쓰일 뿐이었다. 해당 논문에서는 기존의 RNN 모델을 걷어내고 attention을 사용한 새로운 Seq2Seq 모델을 제시했다.\n기존의 RNN # ![](/assets/images/Transformer 도입/6db7e06a-32fe-4da2-9829-fe67bf7b4796-image.png)\nSequence를 token 단위로 매 step마다 입력으로 받는다. step마다 hidden state를 encoding하여 hidden state를 출력한다. 또한 attention을 사용하여 이전 step의 정보들에 대한 가중치를 부여하는 방식으로 이전 정보들을 현재 step에서 얼만큼 사용할지 정해주기도 했다.\n하지만 Sequence의 길이에 비례해서 학습 과정이 길어지는 RNN 특성 때문에 long term dependency, gradient vanishing/exploding과 같은 근본적인 문제점들을 완벽히 해결할 수 없다.\nBi-Directinoal RNNs # ![](/assets/images/Transformer 도입/c77a3d16-d4ae-4354-8c2e-d1813138d51b-image.png) 기존의 RNN은 다음 step에 대한 정보를 이전 step에서 활용할 수 없다. 위 그림에서 Forward RNN의 경우 \u0026lsquo;I\u0026rsquo;는 \u0026lsquo;go\u0026rsquo;, \u0026lsquo;home\u0026rsquo;에 대한 정보를 알 수 없다. 하지만 순서 상 뒤에 오는 정보를 활용해야 하는 문맥이 분명 존재하기 때문에 Backward RNN이라는 것을 고려할 수 있다.\n즉, 기존의 순서를 뒤집이서 병렬적으로 학습을 진행하는 것이다. 위 그림을 에시로 들면 \u0026lsquo;go\u0026rsquo;에 대해서 $h_2^f$, $h_2^b$라는 두 개의 hidden state를 얻을 수 있다. 이를 단순히 concatenate해서 기존의 hidden state의 2배에 해당하는 크기의 새로운 hidden state를 얻어서 활용할 수 있다.\nTransformer # Self-attention # ![](/assets/images/Transformer 도입/d2a126d4-f92e-494d-9566-8ed18f4eaad6-image.png) (그림1)\ntransformer에서 input vector가 위와 같이 존재한다고 하자. transformer에서는 sequence의 내용들을 잘 반영하고 input vector의 형식으로 encoding vector를 생성한다.\n위 그림을 decoder라고 보자. 그러면 I, go, home이라는 입력을 특정 time step에서 발생하는 decoder의 hidden state라고 볼 수 있다. 동시에 I, go, home을 encoder hidden state vector의 세트라고 볼 수 있다.\n![](/assets/images/Transformer 도입/1c87ed9e-bc78-4f1c-a9ca-87c34dd1379f-image.png) 이전 attention 포스팅에서 attention을 구할 때 위 그림처럼 encoding의 hidden state과 decoder의 hidden state vector 간의 내적으로 유사도를 구했다.\n이러한 과정을 하나의 vector 세트 내에서 수행해서 (그림1)을 self-attentino module이라고 부른다.\n문제점 # 하나의 vector 세트에서 유사도를 측정하기 때문에 자기 자신과의 유사도가 다른 유사도에 비해 매우 크게 나타날 것이다. 즉, output vector는 자기 자신에 대한 정보만을 담고 있는 vector로 구성될 것이다.\n우리가 의도하고자 하는 것은 Seq2Seq에서 의도하는 것과 같이 유의미하게 Sequence를 해석해서 결과를 얻어내는 것이지, 자가복제가 아니다.\n해결방법 # 이를 위해서 아래의 세 벡터를 고안한다.\nQuery: 사용하고자 하는 vector set 중에서 어떤 vector를 선별적으로 사용할지 결정해주는 vector다. Key: Query와 어떠한 벡터를 내적해서 유사도를 구해야하는데 이 때 사용되는 재료 벡터. Values: Query와 Key의 내적으로 구해진 가중치가 적용된 벡터. ** Self attention의 구조 ** Self-attention 답게 입력으로 들어온 vector set의 원형을 query, key, values vector로 바꾸는 것이다. 이는 (그림1)에서 보이는 것과 같이 $W^Q$, $W^K$, $W^V$와 같이 서로 다른 linear transform matrix를 통해 각각 선형변환된다.\n** Encoding 과정 ** \u0026lsquo;I\u0026rsquo;를 encoding한다고 해보자.\nQ, K, V 연산 $W^Q$를 통해서 Query vector를 만들고, $W^K$를 통해서 Key vector를, $W^V$를 통해서 Values vector를 만든다. 이 때, $k_n$에 의해서 생성된 유사도는 $v_n$에 순서대로 1대1 대응된다.\nAttention 가중치 구하기 Q와 K에 대한 유사도를 구하고 이것에 softmax를 취해준다. (그림1)과 같이 자기 자신과의 유사도가 다른 벡터와의 유사도보다 낮음을 볼 수 있다. 자기 자신과의 유사도가 매우 높은 경향으로 높게 측정되지 않는 객관적인 유사도를 얻었다!\nValue vector에 Attention 가중치를 적용 \u0026lsquo;I\u0026rsquo;에 대해 원하고자 하는 encoding vector를 얻는다.\n다음 Seuence에 대해서 동일한 과정 수행 Query만 $W^Q$에 의해 새롭게 연산되고 나머지 Key, Value는 기존에 연산해놨던 결과를 그대로 사용한다.\n행렬로 병렬화 # ![](/assets/images/Transformer 도입/4c2a7006-2f55-4aba-bfa9-bd71c4693eea-image.png)\n\u0026lsquo;Thinking Machines\u0026rsquo;라는 문장이 있다고 해보자. 이 문장에 대해서 Self attention을 수행하고자 한다면 Q, K, V를 계산해야 한다. 위에서 공부한대로 $X_1$에 해당하는 \u0026lsquo;Thinking\u0026rsquo;에 대해서 Q, K, V를 구한 후 output vector를 계산한 후 동일 연산을 $X_2$에 적용해도 무방하다.\n하지만 행렬로 이를 병렬화해보자.\n$X$라는 행렬을 $X_1, X_2$의 concatenate 결과라고 해보자. $X$에 $W^Q$를 dot product하면 이 결과인 $Q$는 $Q_1, Q_2$를 concatenate한 결과와 동일하다. 이는 $K, V$에 대해서도 마찬가지이다.\nRNN의 한계를 개선 # transformer는 sequence의 길이나, 순서에 무관하게 모든 sequence의 내용들이 Q, K, V로 변환되어 각각의 input vector에 맞는 encoding vector를 만든다. 즉, RNN과 같이 sequnece의 길이가 길어짐에 따라 이전의 정보들을 담기 힘들어지는 hidden state의 근본적인 구조를 탈피했기에 long term dependency의 문제를 상당히 많이 개선했다.\nScaled Dot-Product Attention # Input: query $q$, a set of key-value $(k,v)$ Q, K, V, Ouput is all vector! Output: weigted sum of values Weigted vector: inner product of query and corresponding key Queries and keys have same dimensionality $d_k$. Dimensionality of value is $d_v$. $d_k$와 $d_v$는 같지 않아도 무방하다! 어차피 values에 대한 가중치 적용은 차원수와 무관하게 벡터 전체에 대한 scalar 곱이다. 일반화 # ![](/assets/images/Transformer 도입/d2839f00-d508-44d2-b13d-a0c73b9f3e4c-image.png)\ni: 몇번째 입력 vector인지 j: 모든 입력 vector에 대한 q와 k의 내적을 구하기 위한 index ![](/assets/images/Transformer 도입/eadc7d76-82cd-4c03-9371-6fbae7532ad8-image.png)\n첫번째로 나온 수식은 하나의 query에 대한 self-attention 결과를 나타낸 수식이다. 이를 확장시켜서 Query matrix에 대한 수식을 써보자. 이는 각각의 query를 row로 가지고 있는 matirx다.\n![](/assets/images/Transformer 도입/a096577f-69ad-431b-bad1-37c4346ebad8-image.png)\nQ와 K 행렬의 곱은 모든 query에 대한 key를 내적해서 softmax를 취하기 이전의 vector들을 row로 가진 matrix를 만들어준다.\nQ와 K의 내적 결과에 softmax를 건다. softmax가 matrix에 걸릴 때는, row-wise softmax가 걸리는 것이 기본적이다. 말 그대로 matrix의 row에 대해서 softmax를 취하는 것이다. 즉, 2번의 결과 matrix에는 vector마다 합이 1이 되는 가중치 vector들이 담겨있다.\n2번의 결과와 values matrix를 내적한다. 이 연산에서 의도한 것은 앞서 계속 해왔던 가중치 vector와 values vector의 element-wise 곱 연산을 수행하는 것이다. inner product이기 때문애 가중치들은 column을 따라서 곱해지고 sum이 된다.\n여기서 많이 헷갈렸는데 개념을 설명하기 위해서 앞서 설명했던 그림과는 다소 다른 순서로 연산이 진행되어서 그렇다.\n![](/assets/images/Transformer 도입/b67e0323-8b47-4c86-bc61-ffc314f4a5ac-image.png)\n여기서는 0.2라는 가중치는 $v_1$에 scalar 곱이 되고, 0.1은 $v_2$에, 0.7은 $v_3$에 scalar 곱이 되어서 새롭게 변형된 $v_1, v_2, v_3$이 만들어진다. 그 후 이 3개의 벡터들에 대해서 단순 더하기를 수행해 3차원의 vector를 형성한다.\n![](/assets/images/Transformer 도입/37b48225-c5a8-4330-b896-6d7e9badb784-image.png)\n실제 연산에서 사용되는 수식에서는 이미 모든 query에 대한 가중치들이 계산되어 row마다 저장돼있다. 또한 values들도 row마다 하나의 value 벡터를 구성하고 있다. 즉, 단순히 두 matrix를 곱해서 row마다 가중치의 column이 일괄적으로 곱해지고 합해진다.\n개념적으로 설명했던 내용들은 행렬을 통해 병렬적으로 계산하고 있다는 이야기다.\ntransformer 정리 # ![](/assets/images/Transformer 도입/3d4dbbf7-ce2a-443f-a640-78a9db4d9d03-image.png) scaling 부분만 달라졌을 뿐, 위에서 설명했던 내용들을 모두 정리한 내용이다.\n앞서 정리했던 연산 순서를 간략화해보자.\nQ, K, V 연산 위 수식대로 Attention 연산 수행 Encoding vector 얻는다! 이를 도식화하면 아래와 같다. ![](/assets/images/Transformer 도입/abe2b34e-7c3a-41d0-84a1-bfcec1de9fd3-image.png)\n$\\sqrt{d_k}$로 scaling하는 이유 # 조원들끼리 gradient exploding을 방지하기 위해서라고만 이해했다. 물론 이도 어느정도 일리는 있지만 주재걸 교수님 강의에서 논리적인 정답을 찾을 수 있었다.\n** 분산 ** 다음과 같이 평균은 0이고 분산이 1인 두 vector를 가정하자.\n(a, b), (x, y)\n두 vector의 곱은 (x,y)를 transpose해서 $ax+by$로 구할 수 있다.\n** 수학적 증명 사실 **\n수학적으로 평균이 같은 random variable끼리 곱하면 평균과 분산이 유지된다고 한다. 두 random variable을 더하면 분산 또한 더해진다는 것이 증명됐다고 한다. random variable을 $\\sqrt{n}$으로 나누면 random varaible의 분산은 $n$으로 나뉜다. 즉, $ax+by$의 분산은 2가 된다. 여기서는 2차원 vector끼리의 곱을 상정했기 때문에 큰 차이가 나지 않지만, 가령 100차원이었다면 분산은 100이 된다.\n분산이 2라면 표준편차는 $\\sqrt{2}$이고 가령 이 때의 곱 결과를 $[1.1, -0.8, -1.7]$이라고 하자. 이 값들이 가령 분산이 100이 되버린다면 $[8, -11, 7]$이 되버릴 수도 있다. 정확한 값 계산은 아니다..\nsoftmax에서 표준편차가 클수록 원래 값들 중에서 큰 값에 몰리도록 확률분포가 계산된다. 표준편차가 작을수록 확률분포는 표준편차가 클 때에 비해서 보다 고르게 나타난다.\n즉, dimension의 크기가 커질수록 본래 의도하고자 했던 확률분포가 아니라 전혀 다른 확률분포가 나올 가능성이 매우 다분하다는 것이다. softmax의 값들 중 특정 확률분포가 본래 의도보다 너무 크거나 작으면 gradient vanishing이 발생할 위험이 발생한다.\n따라서 앞서 밝혔던 수학적 사실을 활용해 분산을 원래대로 복구해주자. 즉, $ax+by$에 $\\sqrt{2}$를 나눠서 분산에 2를 나누는 효과를 줌으로써 분산이 1이 되도록 한다.\n이러한 이유 때문에 transformer에서는 $d_k$로 scaling을 한다. 왜냐하면 $d_k = d_q$다.\n","date":"13 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-13-transformer-%EB%8F%84%EC%9E%85/","section":"Posts","summary":"","title":"Transformer 도입","type":"posts"},{"content":" 6주차 학습정리 # 강의 복습 내용 # NLP (1~9번 포스팅) # https://velog.io/@naem1023/series/NLP\n과제 수행 과정 / 결과물 정리 # bucketing # ![](/assets/images/부스트캠프 AI Tech 2기 6주차 학습정리/89d8a5d1-3a51-4a52-8c89-fd8cfa8a7fcb-image.png) 데이터의 길이에 따라서 batch를 재구성하는 기법이다. 과제 4에서 나왔는데 기법 자체에 대한 이해는 쉬웠지만, 코드에 대한 이해가 매우 어려웠다.\n피어세션에서 해결했는데 코드에서 하고자 하는 바는 다음과 같았다.\n데이터의 길이를 max_pad_len 단위로 바라본다. 가령, max_pad_len=5으로 잡고 이에 대한 몫을 활용한다고 하자. 그러면 데이터의 길이가 5 ~ 9인 데이터들은 하나의 batch로 재구성이 되도록 하고자 한다. 데이터 자체를 옮기면서 재구성하지 않고 데이터의 인덱스를 따로 저장하는 방식으로 batch를 재구성한다. 인덱스를 기준으로 batch를 재구성하고, 데이터의 길이가 같은 그룹으로 구성되는 데이터는 리스트에 인접하게 위치하도록 구성한다. e.g., 데이터의 길이가 5 ~ 9인 데이터들은 리스트에서 인접하도록 위치한다. 위의 과정이 끝나면 인덱스 리스트를 순서대로 불러오기만해도 새롭게 batch를 재구성할 수 있다. 이러한 방식이 과제 4의 bucketing 코드에 담겨있었다.\n피어세션 정리 # 위에서 언급한 bucketing 문제에 대해서 많은 논의를 했다.\n팀 구성에 대해서도 후기를 공유했다. CV와 달리 NLP는 팀구성에 굉장히 다들 의욕적이어서 구인 속도가 매우 빨랐다.\n학습 회고 # 21/09/06: Word2Vec 공부, 과제1 해결 21/09/07: RNN, LSTM 공부, 과제2, 3 해결 21/09/08: attention 공부, 과제4 해결 21/09/09: 팀 구성에 많은 힘을 쓴 시간.. 21/09/10: 과제4 리뷰\n","date":"10 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-10-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-6%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 6주차 학습정리","type":"posts"},{"content":"일반적인 precision이나 recall을 계산하면 Seq2Seq에서는 모든 지표가 0에 가까울 것이다. 왜냐하면 step별로 비교하면 대부분 일치하지 않을 확률이 매우 높기 때문이다. 즉, 아래처럼 굉장히 유사한 문장의 지표가 0에 가깝게 나올 수도 있다. 그래서 이러한 맥락을 지표에 반영할 필요가 있다.\nPrecision, Recall # ** Precision(정밀도) **\n예측한 결과에 대해서 corrected words가 몇개인지 나타낸다. 예측한 결과를 기준으로 ground truth와 겹치는 단어 수 사용자에게 노출된 예측결과가 얼마나 정확한지 나타내는 지표. e.g., 사용자에게 노출된 검색결과들 중 제대로 찾은 검색 결과가 몇개인지 ** Recall(재현율) **\nGround truth에 대해서 corrected words가 몇개인지 나타낸다. Ground truth를 기준으로 예측한 결과와 겹치는 단어 수 실제로 찾아야하는 검색결과들 중에 검색엔진이 얼마나 찾아냈는지 사용자에게 노출되지 않은 정보들 중에서 사용자가 원하는 정보가 있을 수도 있다. 스타1의 리콜을 생각하면 편하다. 실제로 소환하고자 했던 유닛들 중에서 몇 개의 유닛이 소환됐는가? F1 score # Precision과 recall에 대한 통계량을 표현하고 싶다면 두 지표의 평균을 구하면 된다. 이 때 평균들의 대소 관계는 아래와 같다.\n산술평균 \u0026gt;= 기하평균 \u0026gt;= 조화평균\nF1 score는 조화평균을 사용하는데, Precision과 recall 중에서 보다 작은 지표에 집중한다. 내 생각에는 마치 Big O 표기법과 같이 최악의 상황을 가정하는 쪽으로 지향하는 것이 보다 정확한 지표이기 때문에 조화평균을 사용한 것 같다.\n기계번역에서 기존 방식과 같이 f1 score를 연산하면 문법 고려, 어순 등 여러가지 요소들이 무시된다. 따라서 새로운 지표가 필요하다.\nBLEU score # BiLingual Evaluation Understudy. 블루라고 발음하시더라. 기존처럼 한개의 단어에 대한 overlap만을 계산하지 않고 N-gram overlap을 계산한다. 보통 1 ~ 4 gram 사용. 뒷 항에서는 Precision만 고려하고 recall은 고려하지 않는다. 기계번역에서 기존 문장을 빠짐없이 얼마나 재현했는가가 중요하지 않기 때문이다. 예측 결과가 기존 문장과 얼마나 겹치는가가 더 중요하다. 1 ~ 4 gram의 precision에 대한 기하평균을 계산한다. f1 score와 같이 지표들 중 작은 값에 치중하기 위함이다. 조화평균은 지나치게 작은 지표들에 가중치가 부여되기 때문에 사용하지 않는다. Brevity penalty # 너무 작은 번역 결과에 대한 가중치를 계산하기 위한 항이다. BLEU 수식에서 min()에 해당한다.\n예측한 결과가 기존 문장보다 짧다면 이 값이 1보다 작아진다.\n이를 통해 뒷 항에서는 고려하지 못한 recall을 어느정도 고려해준다. 왜냐하면 재현률의 최댓값이 1보다 커지는 상황이 발생할 수 있는데 이를 최대 1로 억제시켜주기 때문이다. 또한 기존 문장보다 예측문장이 짧을 경우 이에 대한 수치를 곱해주는 역할 또한 한다.\n","date":"10 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-10-bleu/","section":"Posts","summary":"","title":"BLEU","type":"posts"},{"content":" Greedy decoding # 이전 포스팅의 attention이나 LSTM들은 특정 step에서 다음 단어를 예측할 때, 가장 확률이 높은 하나의 단어를 선택한다. 이러한 방법을 greedy decoding이라고 한다.\n전체적인 맥락에서 예측하는 것이 아니라 근시안적으로 가장 좋은 방법을 택하기 때문이다.\n예를 들면 아래와 같다.\ninput: {어려운 프랑스어}, answer: he hit me with a pie\n이러한 상황에서 decoder가 \u0026lsquo;he hit a\u0026rsquo;까지만 예측했다고 해보자. 분명 틀린 문장이 되버렸지만 greedy decoding에서는 돌이킬 수 있는 방법이 없다.\nExhaustive search # ![](/assets/images/Beam search/5e071974-4a49-4fed-baad-01a0420124b7-image.png) Greddy decoding에서 주어진 문장 x에 대한 출력 y는 위와 같은 joint probability로 표현이 가능하다. joint probability의 첫번째 항은 x가 주어졌을 때 y1을 출력할 확률이다. 2번째 항은 y1과 x가 주어졌을 때 y2를 출력할 확률이다. 즉, 이 모든 값들에 대한 곱은 Seq2Seq에서 출력 y에 대한 확률을 모든 token에 대해서 고려한 동시사건들에 대한 확률이다.\n목적 $P(y|x)$를 최대화하는 것이다. 가장 자연스러운 y를 찾는 것이 목적이기 때문이다.\n문제점 greedy decoding은 목적을 이루지 못할 수도 있다. $P(y_1|x)$를 최대화하는 선택을 근시안적으로 수행하는 것이 문제다. 이러한 선택 때문에 $P(y_1|x)$ 이후에 등장하는 항들의 값이 작아지며 전체적인 값이 작아질 가능성이 존재하기 때문이다.\n해결방법 t 시점에서의 확률값이 작아지더라도 전체적인 값을 올릴 수 있는 선택을 하면된다. t 시점에서 선택 가능한 모든 경우의 수를 계산하면 이를 구현할 수 있을 것이다. decoder의 t 시점에서 선택 가능한 경우의 수, 즉 단어의 수를 $V^t$라고 한다면 Complexity는 $O(V^t)$일 것이다. 너무 큰 complexity다.\nBeam search # Greedy search와 모든 경로를 탐색하는 방법론의 절충안이다. $V^t$만큼 탐색하는게 아니라 Vocabulary의 수인 V를 k개로 사용자가 설정해서 탐색해보는 것이다.\n![](/assets/images/Beam search/56127164-40e8-4cbe-9617-f8c4ef530d47-image.png) k: beam size(in practice around 5 to 10) 하나의 step에서 k개의 hypothesis를 탐색하는 형태다.\n기존의 joint probability에 log를 씌워둬서 단조증가함수로 바꿔준다. 즉, 정의역이 커지면 치역도 커지는데 이러한 성질 덕분에 joint probability가 가장 클 때는 log를 취한 score값도 가장 클 것이다. 즉, log 씌워도 무방!\nglobally optimal solution을 보장해주지는 못한다. exhaustive search보다는 효율적이다! ![](/assets/images/Beam search/f9a523c0-1e1e-4138-a217-778334300d44-image.png) hypothesis가 4개로 분할된 양상이다. start에서 k개만큼 hypothesis를 생성한다. 다음 step에서는 각각의 k개에 대해서도 k개의 hypothesis를 분화시킨다. 여기서는 k=2, step=2이므로 hypothesis=$k^n$은 $2^2=4$만큼 분화된다.\n![](/assets/images/Beam search/361ca7c2-e54c-439a-84a2-7bbc0aaa2589-image.png) 하지만 다음 step에서는 k개에 대해서 k개의 hypothesis를 만들지 않는다. 적당히 greedy하게 k개의 hypothesis들 중 확률 값이 가장 큰 것들을 k(2)개 선택한다. 그리고 선택된 hypothesis에 대해서 k개의 hypothesis를 만든다.\n이러한 방식을 사용하기 때문에 모든 path를 탐색하는 것보다 훨씬 적은 complextiyr가 소요된다.\nHypothesis 종료조건 # Decoder가 token을 생성했을 때. 따라서, 다른 hypothesis에 비해서 먼저 끝나는 hypothesis가 존재하는데 이러한 결과들은 따로 저장하고, 끝나지 않은 것들은 원래 시나리오대로 진행시킨다.\nBeam Search 종료조건 # 미리 정해둔 timestep T에 도달했을 때 적어도 n개의 hypothesis가 종료됐을 때 최종 평가 # 확률값이 0과 1사이의 값이기 때문에 joint probability의 길이가 길어질수록 score는 줄어든느 것이 자명하다. log의 값이 음수가 되기 때문이다. 따라서 공평하게 score를 계산하기 위해서 score를 길이로 나눈 값을 score로 사용한다.\n","date":"8 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-08-beam-search/","section":"Posts","summary":"","title":"Beam search","type":"posts"},{"content":" Seq2Seq with attention # Seq2Seq with LSTM # Seq2Seq는 RNN의 구조 중 many to many에 해당한다. 즉, 입출력 모두 sequenece인 word 단위의 문장.\n위 그림은 Dialog system(e.g., chat bot)이다. 입력 문장을 받아들이는 부분이 encoder, 출력 문장을 생성하는 부분이 decoder다. 채택한 RNN 모델은 LSTM이다. Encoder의 가장 마지막 단에서 출력된 hidden state는 decoder의 입력 hidden state로 사용된다.\nSoS(Start of Sentence) 생성되는 문장의 첫번째 토큰을 의미한다. SoS는 Vocabulary에서 따로 관리되는데, 학습 시 decoder의 첫번째 입력으로 넣어준다.\nEos(End of Senternce) 생성되는 문장의 마지막 토큰을 의미. 언제까지 문장을 생성할지 지정해준다.\n문제점 # hidden state의 고정된 dimension에 encoder의 모든 정보를 저장해야 한다. 따라서 LSTM이 아무리 long term dependency를 해결했다고 할지라도, sequence가 길어질수록 이전 정보들은 소실되거나 변질될 가능성이 다분하다.\n가령, \u0026lsquo;I go home\u0026rsquo;과 같은 문장에서 주어를 먼저 인식해야한다. 하지만 주어는 문장의 가장 앞에 보통 위치하기 때문에 해당 정보가 뒤로 갈수록 변질되어서 주어 부분을 decoder에서 제대로 생성하지 못하는 문제가 발생할 수도 있다.\n차선책 문장의 순서를 뒤집는다. \u0026lsquo;I go home\u0026rsquo;이면 \u0026lsquo;home go I\u0026rsquo;와 같이 중요한 정보를 뒷단에 배치하는 식으로 순서를 바꾸는 방식이다. 근본적인 해결책은 아니다.\n해결책 step 별로 발생하는 hidden state를 모두 활용하자.\nSeq2Seq with Attention # 프랑스어 문장을 영어로 번역하는 task다.\nencoder는 기존 Seq2Seq와 같이 각각의 step에서 hidden state가 발생한다. encoder의 마지막 step에서 발생하는 hidden state는 첫번째 decoder step의 입력 hidden state가 된다.\nencoder의 hidden state들 중 어떤 것을 필요로 하는지 정하기 위해서, encoder의 $h_n^{(e)}$과 decoder의 $h_1^{(d)}$ 각각에 대해서 내적을 구한다. 즉, 위 그림에서는 4개의 내적 결과가 개별적으로 산출될 것이다. 내적 결과들은 hidden state들 간의 유사도로 생각할 수 있다.\n내적 결과들에 logit처럼 생각해서 softmax를 적용하면 확률을 구할 수 있다. 구해진 확률들은 $h_n^{(e)}$에 부여되는 가중치로 사용된다.\n** Attention vector ** 이렇게 생성된 합이 1인 가중치 vector를 attention vector라고 부른다.\n가중치를 사용해 $h_n^{(e)}$들 간의 가중치가 적용된 평균을 구해서(가중평균) 하나의 attention output vecotr를 생성한다. 이러한 결과물을 context vector라고도 한다.\n정리하면 decoder의 hidden state가 필요로 하는 정보들이 선택되서 새로운 encoder hidden state를의 결합물을 구한 것이다.\n** Attention module ** 위 그림에서 초록선으로 묶은 부분을 attention module이라고 한다. 입력으로 encoder의 hidden state를 받고 출력으로 하나의 attention output을 연산하는 모듈이다.\ndecoder의 hidden state과 context vecotr(attention ouput)이 concatenate가 되어 output layer의 input이 된다. 이렇게 다음 단어를 예측한다.\ndecoder의 2번째 step도 동일한 과정을 반복한다. decoder는 $h_1^{(d)}$를 입력 hidden state로 받고, \u0026rsquo;the\u0026rsquo;를 입력으로 받아 $h_2^{(d)}$를 출력한다.\n반복 작업은 출력으로 end token(EoS)가 나올 때까지 수행된다.\nDecoder의 hidden state # decoder의 hidden state vector는 두 가지 역할을 해야 한다.\nencoder에서 어떤 hidden state를 중점적으로 가져올지 정해야한다. = attention vector를 만드는 정보를 가지고 있어야 한다. ouput layer에 input이 되어 결과를 예측하는데 사용된다. decoder의 학습은 이 두가지 역할을 동시에 수행할 수 있도록 진행된다.\n따라서 backpropagation은 위 그림의 보라색 선과 같은 경로로 진행된다.\nTeacher forcing # Teacher forcing 방식에서 학습을 진행할 때 decoder 단의 입력은 ground truth가 된다. 즉, 학습을 진행하면서 모델이 다음 단어를 잘못 예측하더라도 ground truth를 통해 바로잡는 역할을 기대할 수 있다. 이러한 방식을 사용하면\n유사도 측정 # 단순히 내적을 통해서 구할 수도 있지만 아래와 같이 여러 방법으로 유사도를 구할수도 있다. $score$: 유사도를 구하는 함수 $h_t$: decoder의 hidden state $\\bar h_s$: encoder의 hidden state $genral$ 내적을 수행할 때 $W_a$를 가중치로 두어 사용한다. 행렬의 곱셈에서 각각의 곱셈 요소에 가중치를 부여할 수 있는 권한을 준다고 생각하면 된다.\n$\\begin{pmatrix} a \u0026amp; b \\ c \u0026amp; d \\end{pmatrix}\\begin{pmatrix} x \u0026amp; y \\ z \u0026amp; v \\end{pmatrix}$ 가령 위 식에서 행렬곱은 $ax+bz$, $ay+bv$ 등으로 구성된다.\n이 때 하나 하나의 요소에 가중치를 부여해서 $w_0(ax+bz)$, $w_1(ay+bv)$과 같이 행렬곱 요소에 가중치를 조절할 수 있는 변수를 곱해주는 것이다. 딥러닝에서는 학습 시 조절 가능한 parameter가 발생한 것이다.\n$concat$ $[h_t;\\bar h_s]$에서 ;는 행렬간 concatenate를 의미한다. 수식을 보면 tanh로 묶인 항은 마치 neural netowrk와 동일한데 맞다! $h_t=[1,3]$. $\\bar h_s=[2, -5]$라 하면 위와 같이 네트워크를 구성해보는 것이다. W1, W2는 fully connected로 구성된 네트워크를 의미한다.\n그런데 수식을 보면 W2는 $v_a$로 표기된다. 이는 마지막 네트워크 단이 scalar로 나오기 위해서는 W2가 vector 형태가 되어야 하기 때문이다. 위 네트워크를 보면 3 dimension짜리 vector를 scalar로 만들기 위해서 W2 또한 3 dimension짜리 vector가 되어야 함을 알 수 있다.\n유사도 측정의 방법을 다변화 하는 이유 모델 학습 시 단순히 내적을 하는 것에 비해서, 조절할 수 있는 parameter가 늘어난다. 또한 늘어난 parameter는 attention vector를 구하는 것에 크게 관여한다.\n즉, 유사도 측정에 변수를 고려해서 모델이 atten vector를 구하는 과정 또한 학습할 수 있도록 유도할 수 있다.\nAttention의 장점 # Mahcine translation의 성능을 정말 많이 올렸다. 이전의 Seq2Seq와 달리 decoder가 특정 정보에 집중할 수 있는 환경을 만들어줬다. 긴 문장이 제대로 번역되지 않던 문제를 해결. Attention sovles the bottleneck problem. 하나의 hidden state에 이전의 모든 정보를 담기 때문에 발생했던 문제들 해결. decoder가 소스의 정보를 바로 볼 수 있다! Attention solves gradient vanishing. 이전에는 decoder와 encoder를 통해서 순차적으로 backpropagation이 되며 loss가 전파됐다. (위 그림에서 빨간색 경로) 따라서 bottleneck 현상이 여기서도 발생한다. 특히나 encoder의 앞단에 위치한 step의 hidden state를 바꾸고 싶다면 backpropagation이 매우 깊게 발생해야 한다. attention을 사용하면 이러한 전파 과정이 간소화된다. (위 그림에서 파란색 경로). attention ouput을 통해서 마치 지름길과 같은 path가 backpropagation에서 발생한다. Atten provides some interpretability(해석능력). 특정 입력에 대한 attention vector의 분포를 알면, decoder가 어떤 정보에 focusing하는지 알 수 있다. Attention 사례 # attention을 활용해 프랑스어를 영어로 번역한 사례이다. 순서대로 잘 번역하다가 어순이 뒤바뀌는 구절에 대해서는 atttention이 이러한 어순의 변화를 알아서 감지하고 알아서 번역해줬다. end-to-end 방식으로 알아서 번역을 수행했다!\n","date":"8 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-08-attention/","section":"Posts","summary":"","title":"Attention","type":"posts"},{"content":"","date":"8 September 2021","externalUrl":null,"permalink":"/tags/attention/","section":"Tags","summary":"","title":"Attention","type":"tags"},{"content":" LSTM # Long short-term memory.\nRNN이 가진 Long term dependency를 해결한 모델. 먼 time step의 정보를 잘 전달하기 위해 만들어졌다.\nhidden state를 마치 단기 기억 소자처럼 보고, 단기 기억 소자가 보다 긴 시간 동안 생존할 수 있도록 고안했기 때문에 붙여진 이름이라고 한다.\n기존 RNN $h_t=f_w(x_t, h_{t-1})$\n![](/assets/images/LSTM, GRU/7ea559fc-a17b-4a5f-aee0-84c7217836fd-image.png)\nLSTM cell state($C_t$): 이전의 모든 정보들이 담겨 있는 state. hidden state($h_t$): 해당 step에서만 표출해야 되는 정보가 담겨 있는 state. $C_t, h_t=LSTM(x_t, C_{t-1}, h_{t-1})$\n![](/assets/images/LSTM, GRU/ac01efbd-be4a-468e-bf62-51229c235589-image.png)\n$x_t$와 $h_{t-1}$을 선형변환한 결과에 각각의 activation function에 통과시켜서 input gate, forget gate, output gate, gate gate(?)로 쓴다.\nh를 $x_t$와 hidden state의 dimension이라고 할 때, W는 (4h, 2h)이다. 왜냐하면 x와 hidden state를 선형변환하기 위해서 column은 2h여야 한다. 또한 i, f, o, g로 바로 쓸 수 있도록 row를 4h로 구성한다.\nsigmoid를 통과시켜서 얻은 확률들은 hidden state에 element wise하게 곱해져서 마치 weight를 부여하는 역할을 한다.\nForget gate # ![](/assets/images/LSTM, GRU/bdc68075-8b6c-4f1e-b6dd-901317b552d2-image.png) ![](/assets/images/LSTM, GRU/91210f85-3b3f-4580-80a3-3cfcada828ef-image.png)\n$h_{t-1}$과 $x_t$을 결합하고 이를 W와 선형결합해서 얻은 결과에 sigmoid를 씌웠다. 이를 cell state와 곱해서 cell state의 값들을 얼만큼 보존할지 결정한다. 다시 말하면 얼만큼 정보를 잊을지 결정한다.\nGate gate # ![](/assets/images/LSTM, GRU/e03ef103-303b-42e0-bf51-7d5aa1c05d9e-image.png) ![](/assets/images/LSTM, GRU/ca1652db-356f-44c7-9a1c-f6b3eb1af438-image.png) $\\tilde{C_t}$가 gate gate다. 새로운 정보를 만들어내는 게이트다.\n$i_t$는 input gate이다. forget gate와 동일한 형태로 sigmoid를 통과한 값을 가진다. $\\tilde{C_t}$를 얼만큼 $C_t$에 적용할지 결정해준다. ![](/assets/images/LSTM, GRU/5720ab0d-448f-40f0-9989-a387670a3c75-image.png) cell state를 새롭게 갱신한다. 앞쪽에 있는 항은 앞서 봤던 forget gate와 이전 cell state를 곱한 matrix다. 여기에 input gate와 gate gate 곱을 더해준다.\n굳이 input gate를 생성해서 gate gate에 곱하는 이유는 한번의 선형변환만으로 원하는 결과를 만들기 어렵기 때문이라고 한다. 즉, input gate와 gate gate를 통해 더하고자 하는 정보에 대한 조작을 더 용이하게 하고자 하기 위함이다.\nOutput gate # ![](/assets/images/LSTM, GRU/3a97d88c-cda9-482d-8450-0eccf4581c42-image.png) ![](/assets/images/LSTM, GRU/8dd2d712-9343-47cb-b421-8315284df0cc-image.png)\n$h_t$를 생성하기 위해 output gate를 먼저 구한다. cell state의 각 dimension을 적절한 비율만큼 줄이기 위해서 output gate를 사용한다고 한다. LSTM에서의 $h_t$는 현재 시점의 output에 직접적으로 사용되는 값이다. 즉, $C_t$가 가지고 있는 많은 정보들 중에서 현재 시점 t에 대한 정보만을 필터링한 정보를 가지고 있다고 생각하면 된다.\n가령, hello를 학습데이터로 하는 모델이 있고 학습이 끝난 후 inference를 한다고 해보자. h를 모델에 넣으면 $h_t$에 $W_y$를 선형결합한 결과가 e가 되고 이것이 다음 step에서의 input이 된다.\nBackpropagation # RNN과 달리 LSTM은 아래와 같이 덧셈의 형태로 정보를 결합한다. ![](/assets/images/LSTM, GRU/5720ab0d-448f-40f0-9989-a387670a3c75-image.png)\n즉, seuquence data가 길어져도 거듭제곱의 형태로 인해서 gradient vanishing/exploding이 발생하지 않는다.\nGRU(Gated Recurrent Unit) # LSTM보다 적은 메모리만을 사용하기 위한 네트워크. LSTM과 성능이 비슷하거나 오히려 좋은 경우도 있어서 많이 사용된다.\n![](/assets/images/LSTM, GRU/40e9cc2e-7594-48b6-ba86-31a1742e8c9d-image.png) ![](/assets/images/LSTM, GRU/9b14177c-2e51-4987-9e77-2e45045300b1-image.png)\nLSTM에서 forget gate와 input gate를 통해 정보 삭제량, 정보 생성량을 조절했다. GRU에서는 $z_t$를 한번만 계산해 $1-z_t$를 마치 forget gate처럼, $z_t$를 input gate처럼 사용한다.\n또한 LSTM의 cell state와 hidden state을 하나의 hidden state만으로 구현한다. 즉, GRU의 hidden state는 이전의 모든 정보들을 가지고 있으면서 현재 step의 ouput에 직접적으로 관여하는 값이 된다.\n","date":"7 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-07-lstm-gru/","section":"Posts","summary":"","title":"LSTM, GRU","type":"posts"},{"content":"","date":"7 September 2021","externalUrl":null,"permalink":"/tags/rnn/","section":"Tags","summary":"","title":"RNN","type":"tags"},{"content":" RNN # sequence data가 입출력으로 주어진 상태에서 t에서의 입력 $x_t$와 이전 hidden state인 $h_{t-1}$을 입력으로 받고 $h_t$를 출력하는 네트워크.\n중요한 것은 매 time stamp마다 새로운 model이 등장하는 것이 아니라, 하나의 parameter set인 A가 모든 time stamp에 걸쳐서 사용된다.\n왼쪽처럼 압축해서 표현한 것을 rolled diagram, 왼쪽처럼 time stamp를 표시한 것ㅇ unrolled diagram이다.\n도식화하면 위와 같다.\n$h_t$: new hidden state vector $f_w$: RNN function with parameters W. W: linear transform matrix $y_t$: output vector at time step t. $h_t$를 활용해서 계산한다. time step마다 계산할 수도 있고, 마지막에 한번만 계산할 수도 있고 맘대로다. e.g., 품사 예측이라면 매 step마다 해야될 것이고, 문장의 긍정/부정을 판단하는 것이라면 마지막에 한번만 할 것이다. $f_w$는 위 수식처럼 non linear function으로 정의한다. 이 때, $W_{hh}$와 $W_{xh}$로 W를 분리해서 수식에서 적었는데 이는 아래 그림처럼 하나의 W matrix에서 유도된 것으로 생각해볼 수 있다.\n$h_t$의 차원은 hyperparamter이기 때문에 2로 정의해보자.\n입력으로 $x_t$와 $h_{t-1}$을 받아서 $h_t$을 출력으로 내보내기 위해서 W는 (2, 5)의 크기를 가져야한다. 그래야만 $x_t$와 $h_{t-1}$의 concatenate 결과를 W와 내적했을 때 (2,1)이 나오기 때문이다. 이 때, 굳이 W를 (2, 5)로 하지않고 위 그림에서 빨간색과 초록색 동그라미로 분리된 부분으로 분리해볼 수 있다. 즉, $x_t$와 $h_{t-1}$가 각자의 W를 가지도록 한 후, 내적 결과를 더해주면 $h_t$가 알아서 생성된다는 것이다.\n즉, $W_{hh}$는 $h_{t-1}$을 $h_t$로 변환해주고 $W_{xh}$는 $x_t$를 $h_t$로 변환해주는 역할을 한다.\n같은 논리로 $W_{hy}$는 $h_t$를 $y_t$로 변환해주는 역할을 한다.\n만약 binary classification이라면 $y_t$는 1차원 벡터, 즉 scalar 값을 가질 것이다. 여기에 sigmoid를 씌워서 결과값을 예측되는 확률로 사용할 것이다. Multi class라면 class의 개수가 $y_t$의 dimension일 것이고 추가적으로 softmax를 사용해서 확률분포를 얻는다.\nType of RNN # RNN은 입출력 중 한 가지가 sequence이거나 모두 sequence data인 경우를 통합적으로 다룰 수 있다. ref: http://karpathy.github.io/2015/05/21/rnn-effectiveness/\none to one(standrad neural network) 입출력 모두 sequence data가 아니고, time step도 하나인 형태다. 일반적으로 알고 있는 DNN과 동일한 구조다. one to many 입력은 sequence data가 아니지만, 여러 time step에 걸쳐서 sequence data를 출력한다. 처음에만 실제로 입력이 발생하기 때문에, 나머지 time step에서는 모든 값이 0인 tensor를 입력으로 준다. e.g., Image captioning many to one 매 time step에 맞춰서 입력이 발생하고 마지막에 한 번의 출력이 발생. e.g., Sentiment classification(감정 분석) many to many sequence data를 time step별로 입력한 후에 time step 별로 sequence data를 출력한다. e.g., Mahcine translation 모든 time step마다 입력, 출력 발생 e.g., Video classification on frame level Character-level language model # language model(언어 모델)은 주어진 문자열, 단어들의 순서를 바탕으로 다음 단어가 무엇인지 맞추는 task. word, character level에서 모두 수행 가능하다.\nCharacter-level language model의 구축은 다음과 같은 순서로 진행된다.\nExmaple of training sequence: \u0026ldquo;hello\u0026rdquo;\nCharacter 단위로 unique vocabulary를 만든다. [h, e, l, o] vocabulary의 character들은 word embedding에서 했던 것처럼 one-hot vector로 표현한다. h = [1,0,0,0] 아래 수식에 맞게 \u0026ldquo;hell\u0026quot;을 순서대로 RNN에 넣어서 계산한다. 중요한 점은 매 time step마다 다음 character를 예측해줘야 하는 task라는 것이다. 즉, many to many의 형태로 RNN을 구성해야 한다. 이는 아래 그림과 같다. output은 아래 수식과 같이 계산할 수 있다.\nlogit이라고 쓰는 이유는 multi class classification을 위해서 softmax를 사용하기 때문이다.\nInference # RNN이기 때문에 매 time step의 출력을 다시 다음 time step의 입력으로 사용할 수 있다. 즉, 첫번째 character인 \u0026lsquo;h\u0026rsquo;만 입력으로 주고 나머지는 알아서 출력하게 한다.\nTraining Shakespeare\u0026rsquo;s plays # character 단위에서 사용했던 방법론을 글에서도 사용할 수 있다. 단어 단위로 Vocabulary를 만들되, punctuation도 모두 vocabulary에 포함시킨다. 쉼표,\u0026rsquo;\\n\u0026rsquo;, 공백 등 모두 포함시킨다. 이렇게 해서 rnn으로 간단한 language model을 구성해볼 수 있다.\n학습을 진행하면 할 수록, 첫번째 character를 주어졌을 때 완성되는 나머지 문장들이 더욱 자연스러워진다.\n다른 예제들 # 희곡에서 인물과 대사를 구분해서 학습할 수 있다. LaTeX로 쓰여진 논문을 학습해서 새로운 논문을 inference할 수 있다. C언어를 학습해서 코드를 생성할 수 있다. BPTT(Backpropagation through time) # 모든 loss를 활용해서 학습한다면 좋겠지만, 보통 sequence의 길이가 매우 길어서 모든 loss를 활용할 수 없다. 따라서, 학습에는 모든 데이터를 사용하되 loss는 일부 구간에서만 가져와서 Backpropagation을 한다.\nHow RNN works # RNN이 어떻게 학습을 진행하는지 추적해볼 수 있다. hidden state에 t 시점 이전의 정보들에 대한 모든 정보들이 담겨 있다. 즉, hidden state가 초기에 비해서 어떻게 변화하는지 추적해본다면 RNN이 어떻게 학습하는지 알 수 있다.\n아래 결과들은 Vanila RNN은 아니고 LSTM, GRU를 사용했을 때의 hidden state 변화를 나타낸 것이다.\n빨간색은 hidden state의 특정 셀이 음수로 커지는 것이고, 파란색은 양수로 커지는 것을 나타낸다. Quote Detection을 담당하는 cell을 hidden state에서 추적해보니 위와 같은 결과가 나왔다. if statement를 담당하는 cell의 hidden state는 위와 같이 변화했다.\nVanishing/Exploding gradient in RNN # RNN 자체는 훌륭하나 backprogation에서 문제가 발생한다. RNN은 $W_h$를 지속적으로 곱하고 activation function을 통과시킨 형태로 수식이 짜여진다. $W_h$이 반복적으로 곱해지는 형태는 backpropataion에서 gradient 값이 1보다 크다면 매우 커지거나 1보다 작다면 매우 작아지는 현상을 초래한다.\n간단하게 이해하기 위해 W를 scalar로 생각해서 예시를 들면 위와 같다.\nbackpropagation에서 gradient를 구하기 위해 h3를 미분한다. 이 때, h1에 대한 gradient를 구하기 위해서는 합성함수의 미분을 3번이나 풀어줘야하는데 이 때 $w_{hh}$인 3이 3번이나 곱해지면서 gradient가 된다. h3이 아니라 더 커다란 seuquence였다면 gradient는 더 큰 3의 거듭제곱에 비례할 것이다. $w_{hh}$이 1보다 작은 값이었다면 값은 매우 작아지는 형태로 나타날 것이다.\n결과적으로 h3에서 발생한 값을 h1까지 잘 전달할 수 있는 형태가 되야하는데, 그렇지 못하고 되려 무한대나 0으로 gradient가 수렴하는 것이다.\n","date":"7 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-07-rnns/","section":"Posts","summary":"","title":"RNNs","type":"posts"},{"content":" pathlib # ref: https://brownbears.tistory.com/415\n그 동안 os.path.join이나 os.sep을 사용해서 일일히 경로 연산을 했다. 문제는 없지만 매우 귀찮고 번거럽고 코드도 더러워지면서 관리도 힘들다. 다행히도 python built in으로 pathlib라는 것을 제공해준다.\n요점은 path를 객체처럼 관리하자는 것이다. 또한 경로 연산에서 연산자를 재정의할 수 있어서 \u0026lsquo;/\u0026lsquo;를 나누기가 아니라 경로 구분자로 즉시 사용할 수 있다.\n","date":"7 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-07-pathlib/","section":"Posts","summary":"","title":"pathlib","type":"posts"},{"content":"","date":"7 September 2021","externalUrl":null,"permalink":"/categories/pytorch/","section":"Categories","summary":"","title":"Pytorch","type":"categories"},{"content":" 불용어(stopword) # ref: https://bkshin.tistory.com/entry/NLP-3-%EB%B6%88%EC%9A%A9%EC%96%B4Stop-word-%EC%A0%9C%EA%B1%B0 분석에 큰 의미가 없는 단어들. a, an, the와 같은 관사나 I, my 같은 대명사들이 해당된다.\nspacy는 nlp객체의 token에서 is_stop(boolean)을 제공해준다. nlkt는 불용어 사전을 제공해준다. import nltk nltk.download(\u0026#39;stopwords\u0026#39;) print(\u0026#39;영어 불용어 갯수:\u0026#39;,len(nltk.corpus.stopwords.words(\u0026#39;english\u0026#39;))) Lemmatization # ref: https://wikidocs.net/21707\n단어는 어간과 접사가 있다.\n어간(stem): 단어의 의미를 담고 있는 부분 접사(affix): 단어에 추가적인 의미를 주는 부분 어간을 추출하는 작업이 lemmatization이다.\nPunctuation # ref: https://www.delftstack.com/ko/howto/python/how-to-strip-punctuation-from-a-string-in-python/#%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%97%90%EC%84%9C-%EB%AC%B8%EC%9E%90%EC%97%B4%EC%97%90%EC%84%9C-%EA%B5%AC%EB%91%90%EC%A0%90%EC%9D%84-%EC%A0%9C%EA%B1%B0%ED%95%98%EA%B8%B0-%EC%9C%84%ED%95%B4-string-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%A9%94%EC%84%9C%EB%93%9C-%EC%82%AC%EC%9A%A9\nPunctuation(구두점) 제거는 가장 흔하게 쓰이는 text normalization.\nRegex로 제거하기 text = re.sub(r\u0026quot;[^a-zA-Z0-9]\u0026quot;, \u0026quot; \u0026ldquo;, text) 알파벳, 숫자 외는 모두 공백으로 변경. 보통 공백으로 치환해서 문장의 구조를 최대한 유지해준다. spacy의 token에서 is_punct를 호출하면 puncutaion인지 알 수 있다. python built in function을 써도 된다. punctuation list인 string.punctuation를 사용. ","date":"6 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-06-nlp-%EC%A0%84%EC%B2%98%EB%A6%AC/","section":"Posts","summary":"","title":"NLP 전처리","type":"posts"},{"content":" Word Embedding # 문장의 단어들을 벡터 공간 상의 점으로 표현하기 위해, 단어들을 벡터로 변환하는 방법.\nWord Embedding 자체가 딥러닝, 머신러닝 기술이다. 학습 데이터, 사전에 정의한 벡터 공간의 차원 수를 통해 학습을 진행한다. 학습이 완료되면 학습 데이터, 즉 특정 단어에 대한 최적의 벡터를 출력해준다.\nWord Embedding의 기본 아이디어 # 비슷한 의미를 가지는 단어들이 벡터 공간에서 비슷한 위치에 맵핑되게 하여 유사도를 가지게 한다. 이를 통해 다른 자연어 처리에서 더욱 쉽게 task를 처리하도록 환경을 제공한다.\nWord2Vec # Word Embedding의 대표적인 예시이다.\n하나의 문장에서 인접한 단어들은 비슷한 의미를 가질 것이라는 가정이 이 알고리즘의 기본 아이디어다. 즉, _특정 단어는 주변의 단어들을 통해 해당 단어의 의미를 알 수 있다_는 논리가 존재하는 알고리즘이다.\nThe cat purrs. This cat hunts mice.\n위 두 문장에서 cat 주변에는 The, purrs, this, hunts, mice가 있다. 그렇다면 이 단어들은 cat과 유사한 의미를 가질 것이라는 가정이 존재하는 것이다.\n예측 방법 # 학습데이터를 바탕으로 target 단어(여기서는 cat) 주변 단어들(w)의 확률 분포를 예측한다.\n![](/assets/images/Word Embedding/86cb5366-3ede-449e-ad8a-7cabfc970947-image.png) 만약 cat이 입력으로 주어졌다면, 주변 단어를 숨기고 $P(w|cat)$의 학습을 진행한다.\n학습 방법 # 주어진 문장을 word로 분리하는 Tokenization을 수행. Unique words로 vocabulary 구축. Vocabulary의 각 단어들은 Vocabulary의 사이즈만큼의 dimension을 가지는 One-hot vector로 나타낸다. Sliding windows를 통해 학습 데이터의 입출력 쌍을 구성한다. 가령 Sliding windows의 크기가 3이라고 해보자.\nI study math.\n해당 문장에서 I에 Sliding windows를 적용한다면, I를 기준으로 앞뒤로 1개의 단어들을 살펴본다. 앞에는 아무 단어가 없고, 뒤에 study가 있으므로 I를 기준으로는 (I, study)라는 입출력 쌍이 구성된다.\nstudy를 기준으로 한다면 앞뒤로 I와 math가 있다. 그러므로 (study, I), (study, math)라는 입출력 쌍이 구성된다.\n간단한 neural network를 구성해서 준비된 쌍들을 학습한다. ![](/assets/images/Word Embedding/a55dc671-1420-4c7f-bd74-a6232bf7aa48-image.png)\nInput Layer: 입력 one-hot vecotor의 차원 수만큼의 node를 가진다. Output Layer: 출력 one-hot vecotor의 차원 수만큼의 node를 가진다. Hiddne Layer: Word embedding을 수행하는 좌표 공간의 차원수와 동일하게 node를 구성. 사용자가 설정하는 hyperparameter.\n![](/assets/images/Word Embedding/f3a03d57-9baa-4a28-aa9d-7c3cf21eea57-image.png)\n위 neural network를 벡터로 도식화하면 위와 같다.\n$W_2(W_1x)$의 형태로 곱할 것이기 때문에 W1은 (2,3,), W2는 (3,2)로 구성한다. 그 후 softmax를 통과시켜서 3차원 벡터가 확률분포를 가지도록 바꿔준다. 이렇게 구한 출력 값과 y 벡터 간의 거리가 가장 가까워지도록 neural network를 학습하기 위해 softmax loss를 사용한다.\n내적 계산 # 일반적인 행렬곱을 계산해도 되지만 one-hot vector의 특성 상, 한 개 성분만이 1을 가지기 때문에 특정 index의 값만 취하게 된다.\n예를 들면 위 그림에서 W1와 x를 곱할 때, x의 2번째 성분만이 1이기 때문에 W1에서 2번째 column만을 취하게 된다.\n이러한 성질을 활용해서, one-hot vector를 곱할 때는 행렬곱을 수행하지 않고 특정 index의 값만을 취하는 형태로 연산이 발생한다.\nW2는 Vocabulary의 수만큼 row vector를 가질 것이다. 실제로 3개의 row를 가지고 있다. row의 차원은 W1과 내적이 가능하도록 2가 될 것이다.\nGround truth 실제 값을 의미한다. 기상학에서 쓰이던 용어인데, 인공위성과 같이 멀리석 관측한 데이터가 아니라 실제 지상에서 관측한 값을 지칭할 때 쓰던 용어다. 기계 학습에서는 y hat이 아니라 학습 데이터로 주어지는 실제 y 값이라고 생각하면 된다.\nlogits sigmoid와 역함수 관계에 있는 함수. 출력이 −∞ ~ +∞다. ref: https://velog.io/@gwkoo/logit-sigmoid-softmax%EC%9D%98-%EA%B4%80%EA%B3%84\n$W_2(W_1x)$의 값이 ground truth와 일치하게 하려면 ground truth가 1인 3번째 index에서 logits 값이 무한대고 나머지에서는 logits 값이 -무한대여야할 것이다.\n이해 못 한 점 # W1와 W2에 대한 연산이 벡터 간의 유사도를 측정하는 것과 같다고도 하셨는데 무슨 말인지 모르겠다..\nProperty of Word2Vec # Word2Vect는 단어들간의 의미론적 관계를 vector들 간의 관계에 잘 학습해준다. ![](/assets/images/Word Embedding/1e262f45-bd01-472b-964c-601113b61db0-image.png)\n위 그림은 Word2Vec을 통해 학습된 단어들의 vector다. 서로 비슷한 관계에 있는 벡터들 간의 관계(벡터들 간의 차이)는 같은 방향성을 가지는 것을 알 수 있다.\nWord2Vec 한글 # https://word2vec.kr/search/?query=%ED%95%9C%EA%B5%AD-%EC%84%9C%EC%9A%B8%2B%EB%8F%84%EC%BF%84\n![](/assets/images/Word Embedding/a94e074e-e8f6-428f-906b-529fdb58091c-image.png)\nWord2Vec을 한글에서 구현한 예제이다. 쿼리문은 다음과 같이 사용된다. 한국-서울은 나라와 수도 간의 관계를 나타내도록 해준다. 그리고 이것에 도쿄를 더하면 나라와 수도 간의 관계를 도쿄에 적용시켜주고 그에 대한 결과를 보여준다.\nIntrusion Detection # 여러 단어가 주어졌을 때, 나머지 단어와 가장 의미가 상이한 단어를 찾는 task. Word2Vec을 통해 Word Embedding 결과를 구해서 해결할 수 있다.\n특정 단어에 대한 나머지 단어들의 Euclidian distance를 구하고 평균을 낸다. 이러한 과정을 모든 단어에 대해 반복하고 거리의 평균이 가장 큰 단어를 구하면 된다.\nApplication of Word2Vec # 본래 단어의 의미를 찾기 위한 방법론이지만, Word Embedding 결과를 쉽게 뽑아주는 task이다. 따라서 단어를 벡터로 변환해야 되는 다른 nlp 방법론에서도 유용하게 많이 사용한다.\nWord simliarity Machine translation 서로 다른 언어에서 같은 의미를 가지는 단어들이 쉽게 align될 수 있게 해준다. PoS tagging NER Sentiment analysis 단어들의 긍정, 부정을 쉽게 표현할 수 있게 해준다. Clustering Semnatic lexicon building Image captioning GloVe # Word2Vec과 함께 많이 사용되는 Word Embedding 방법론.\nWord2Vec과의 가장 큰 차이점은, 학습 데이터 내에서 한 단어 쌍이 하나의 windows에서 동시에 출현한 빈도 수를 미리 모두 계산한다. 이것을 $P_{ij}$라고 하자.\n![](/assets/images/Word Embedding/42dd8894-fed3-494d-a1eb-feab9605c113-image.png)\nGloVe의 object(loss) function은 위와 같다.\n$u_i$ = input word embedding vector $v_j$ = output word embedding vector $P_{ij}$ = i, j 두 단어가 한 window 내에서 동시에 몇 번 존재하는가. 선형대수적인 관점에서, 추천시스템 알고리즘인 co-occurrence low rank matrix factorization로도 이해할 수 있다.\n장점 # 중복되는 계산을 줄일 수 있다. 가령, study와 math가 동시에 많이 존재한다면 Word2Vec은 두 단어 간의 관계에 대해 그냥 많이 학습한다. 하지만 GloVe는 미리 두 단어의 동시 존재성에 대해서 알고 있다.\n따라서 위 수식에서 study와 math의 경우 내적 값에 큰 값을 빼주면서 학습을 더 빨리 할 수 있게 해준다.\n또한 더 적은 데이터에 대해서도 잘 작동한다.\n예시 # ![](/assets/images/Word Embedding/83e4d75b-78c7-483c-87e8-0044e3074d79-image.png)\n성별만 다르고 의미가 같은 단어에 대해 GloVe를 수행한 결괄르 PCA로 찍어본 결과다. 성별의 차이가 일정한 크기와 방향을 가진다는 것을 알 수 있다.\n![](/assets/images/Word Embedding/41d2c1e0-3829-49f8-89d0-fc03c6e756c6-image.png)\n형용사의 comparative, superlative 또한 크기와 방향성을 학습한다!\nPre trained model # https://nlp.stanford.edu/projects/glove/\nwikipedia, crawling, twitter에서 수집한 단어들에 대한 pre trained 모델을 배포해준다. ![](/assets/images/Word Embedding/7216a42a-d6d0-4828-aecd-83a9aa09bd6f-image.png)\nuncased: 대소문자가 구별된 단어라도 같은 단어로 취급 cased: 대소문자가 구별된 단어를 다른 단어로 취급 dimesion: 입출력 word 벡터의 dimension ","date":"6 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-06-word-embedding/","section":"Posts","summary":"","title":"Word Embedding","type":"posts"},{"content":"","date":"6 September 2021","externalUrl":null,"permalink":"/tags/ai-competition/","section":"Tags","summary":"","title":"Ai Competition","type":"tags"},{"content":"2주간의 짧은 시간이었지만, 계속 밤을 샜던지라 4주와도 같았던 시간이었다. 그 동안 시도했던 내용들, 다른 사람들이 사용했던 방법들을 정리해봤다.\n최종 결과물 # 실험을 위해 구현했던 개인 코드: https://github.com/naem1023/boostcamp-pstage-image\n내가 만든 모델을 통해 5위를 한 것도 너무 좋았고 그 과정에서 팀원들과 밤새면서 함께 코딩하고 실험했던 경험들이 소중했다. 교외에서 한번도 협업하면서 함께한다는 기분을 느낀 적이 없었는데, 비즈니스적인 협업 외의 끈끈함을 느낀 것은 이번이 처음이었다.\npublic data로 채점했을 때는 10위라 졌잘싸 분위기였다. private data까지 포함된 최종 순위에서 확 올라서 우리 조는 축제 분위기였다.\npublic data와 private data 간의 f1 score의 변화가 없었던 것을 보면 robust한 모델을 구현하는 것에 신경 쓴 것이 효과가 있었던 것 같아 좋았다.\n대회 개요 # 18개 class는 mask(3개 class), gener(2개 class), age(3개 class)로 구성된다. 학습 데이터를 기준으로 18개 class에 대한 classifier를 만들고 이를 submission.csv로 제출해서 채점한다.\n학습 계획 # 첫번째 학습계획: https://velog.io/@naem1023/TIL-train-%EA%B3%84%ED%9A%8D-%EC%A0%95%EB%A6%AC-2021.08.24 첫번째 학습계획: https://velog.io/@naem1023/TIL-train-%EA%B3%84%ED%9A%8D-%EC%A0%95%EB%A6%AC-2021.08.24-xuou0hx5\n처음에는 단순하고 모호하게 Ensemble을 하고자 생각했는데 어떤 방식으로 해야될지 계획조차 짜지 못했다. 팀원들에게 피드백을 받아서 정돈된 모델 학습 계획을 세우고 해당 방법론이 후의 모든 학습 방법에 대한 토대가 됐다.\n학습 파이프라인 # ref: https://github.com/victoresque/pytorch-template 해당 템플릿을 활용해서 코드를 구성해보려 했다. 추상화를 완벽하게 해서 가령 BaseTrainer를 상속받아 팀원들이 각자 새로운 Trainer를 만드는 형태로 프로젝트를 구성하려고 했다. 물론 이렇게는 안되고 OOP를 사용한다는 것 자체에 의의를 두게 됐다.\nconfig # config.py에 관련 설정들을 해두고 train.py를 실행시키면 알아서 학습하도록 구성해보려고 했다. 본래 config.json을 구성하고 이에 맞춘 parser를 구성하는 것이 맞다. 하지만 parser를 만드는 수고를 할 시간에 빨리 모델을 돌려보며 여러 실험을 하고 싶었다. config.py에 변수 형태로 여러 configuration 값들을 설정하고 python 문법을 활용해서 configuration을 구성해두는 것으로 간단하게 해결했다.\nBackbone model # 여러 시도를 했고 그에 대한 결과들을 다음과 같다.\n실험을 위한 모델(Resnet18, efficientnet-b2) Augmentation, cutmix 등의 여러 방법들을 실험할 때 빠르게 결과를 내기 위해서 사용했다. 재밌는 점은 cutmix 적용 이후부터는 resnet18에서 성능이 너무 별로였다. f1 score 기준으로 대체로 0.6후반 0.7초반이 형성이 됐는데 여러 cutmix 기법을 사용했을 때의 차이점을 알기가 어려웠다. 가령, cutmix를 기존 방식대로 vertical하게 잘라보되 lambda의 비율은 random으로 줄지말지를 결정했을 때의 차이가 resnet18에서는 드러나지 않았다. 더 큰 모델에서는 cutmix 기법에 따라서 결과의 차이가 제대로 나타나서 대회 2주차부터는 efficientnet-b2를 통해 실험했다. 검증을 위한 모델(Efficientnet-b7) Efficientnet-b4를 사용하고자했으나 소수점 3자리 차이로 b7이 더 좋은 결과들을 내서 b7 위주로 검증을 했다. 1, 2위 조의 사용모델들은 efficientnet-b0, resnet152이었다. Augmentation # Albumentation을 시도: https://velog.io/@naem1023/Preprocessing Transformation 정리:\n속도도 매력적이었지만 다양한 transformation을 제공해준다는 점에서 Albumentation을 사용했다. 매우 좋았다.\n주의해야될 점은 pytorch의 transformer와는 다르게 반환형이 dictionary다.\n전처리 # Face crop # 마스크, 나이, 성별에 관한 학습이기 때문에 배경을 날려버리는 것이 중요하다는게 우리 조의 정론이었다. 따라서, 얼굴만 자른 데이터셋을 만들어보자는 의견이 있었고 해당 데이터셋을 조원분께서 구해주셔서 사용했다.\n1, 2위는 오히려 Face crop을 안했다고 했다. ???\n내 생각에는 모델이 사진에서 사람의 형태를 찾는 것은 다소 쉬운 task에 해당하기 때문에, 굳이 배경을 날려버릴 필요가 없는 것 같다.\ntransformation # https://velog.io/@naem1023/TransformationAlbumentation 따로 정리한 글이 있는데, 이대로 했다. 요점은 강한 augmentation을 통해서 되도록 robust한 모델을 구성하고자 한 것이다. 데이터의 수가 많지 않았기 때문이다.\nLabel 기준 변경 # age에 대해서 데이터들의 기준을 바꿨다. 대회는 30세, 60세를 기준으로 age를 3 그룹으로 나눠서 최종적인 18개의 class를 도출하는 것이 기본 가이드였다. 하지만 데이터 자체가 매우 불균형해서 해당 기준대로 데이터를 사용하면 학습이 거의 안된다.\n![](/assets/images/첫번째 Ai Competition 마무리/398ac5b3-a8f7-4d62-9441-37e7f968c8dd-image.png)\n3060세 사이에서 30세 부근에는 매우 적은 사람만이 30세60세 그룹에 들어간다. 60세 이상의 그룹은 다른 그룹에 비해서 사람이 너무 적었다. 이러한 문제점을 age class의 기준점을 바꾸는 것으로 해결해보려했다.\n30세~60세 그룹의 데이터 수를 조금이나마 늘리기 위해 30세가 아닌 29세를 기준점으로 삼았다. 60세 이상에 해당하는 그룹의 데이터 수를 늘리기 위해 60세가 아닌 59세를 기준점으로 삼았다. Feature 분할 # 다행히 우리 조는 대회 첫날부터 이 이슈가 나왔다. 대회는 mask, age, gender라는 feature를 사용해서 임의의 이미지를 18개의 class로 분류하는 문제다. 하지만 feature들 간에 상관관계도 없거니와 인과관계도 없다는 의견이 나왔다. 내가 생각해도 정론이었다.\n우리 조에서는 내가 feature를 분할해서 학습을 진행하고 다른 분들이 모든 feature를 한꺼번에 학습시켜봤다. 결과적으로 feature를 분할해서 학습시킨 것이 소수점 2자리에서 미세하게나마 성능이 좋았다.\n물론 대회이기 때문에 이 차이는 매우 컷지만, 인과관계가 없는 feature를 분할하는 것이 큰 효과가 있다고 말할 수는 없었다. 내 생각에는 모델의 크기가 보통 크기때문에 mask, gender, age의 feature를 모델이 알아서 한꺼번에 학습했던 것 같다. 즉, 인과관계가 없지만 하나의 모델이 parallel하게 여러 feature를 동시에 학습한 것이라고 생각한다.\nLabel smoothing # 우리조는 해당 방법론을 적용하니 되려 성능이 나빠졌다. 1, 2위조는 사용했다고 하니 적용 방법에 문제가 있었던 것 같다.\nValidation set 구성 # 우리조에서 약간 등외시한 사항이다. 어차피 대회 웹사이트에 csv를 제출하면 채점이 되서 확인 가능하니 굳이 할 필요가 있냐는 것이다.\n지금 생각해보면 정말 정말 잘못된 생각이다. 왜냐하면 여러 학습 기법들은 validation set에 대한 평가지표를 활용해서 판단을 하기 때문이다. 즉, 올바르게 구성되지 않은 validation set은 쓰레기다.\n물론 validation set 자체는 model paramter update 과정에 관여하지 않기 때문에 validation set을 잘못 구성한 것이 학습이 잘 안된다는 것과 동일한 의미는 아니다. 하지만 앞서 말한 이유들 때문에 반드시 제대로 구성해야된다.\n구성 방법 # train, test set에 동일한 사람이 존재하지 않도록 한다. 대회에서 제공해주는 파일들은 mask 5장, normal 1장, incorrect 1장으로 구성된다. 이 때, 7장의 사진이 train, test set에 산개한다면 같은 사람이 train과 test에 2번 이상 등장한 것이다. 즉, 해당 사람에 대해서는 당연히 validation 점수가 높을 것이고 이는 평가지표에 방해요인이 된다. train, test set에 동일한 class 분포를 형성해줘야 한다. 대수의 법칙에 따라 매우 많은 데이터를 임의로 분리하면 두 데이터 set의 분포는 동일한 분포를 가질 것이다. 하지만 대회의 학습 데이터는 그 정도로 충분히 많지 않기 때문에 동일한 class 분포를 가지도록 조정해야 할 것이다. Loss function # 더 적은 데이터 분포에 더 많은 가중치를 부여해주는 focal loss를 썻고, 이는 다른 조들도 매우 일관되게 공통되는 사항이었다.\n다른 점은 1, 2위는 f1 loss를 사용했다. loss.backward()를 하고 optimizer.step()으로 parameter를 업데이트하기 전에, loss 값에 f1 score를 더해주는 것이다. 즉, loss function이 f1 score를 줄이는 방향으로 움직이도록 유도하는 것이다.\nCutMix # 화두의 그 CuiMix다. 오히려 1, 2위는 cutmix를 안 썻더라. cutout을 썻다.\n우리 조의 CutMix 적용기는 아래 링크에 있다. https://velog.io/@naem1023/CutMix https://velog.io/@naem1023/CutMix-vertical\nDatasampler # Dataloader에서는 sampler를 지정할 수 있다. 나는 train, test set을 분리할 때, RandomSampler를 사용했다.\n다른 조는 pytorch 공식 제공 라이브러리 외에 imbalancedDatasetSampler를 사용해서 불균형 데이터에 대해서 어느 정도 균등하게 만들어주는 sampler를 사용했다고 한다.\nOne hot vector? # 대부분 One hot vector를 통해 multi label classification을 진행했지만, 2위조는 사용하지 않았다고 한다. 모든 class에 대한 출력단의 독자적인 확률을 가지도록 출력단에 sigmoid를 통과시켜서 모델을 구성했다고 한다. 물론 이 중 가장 높은 확률을 가지는 하나의 class를 활용하는 것은 One hot vector를 구성해나가는 과정과 같이 torch.argmax를 썻다.\nPsuedo labeling # 화두의 그 방법이다. 우리조는 Dacon 등 여러 국내 대회에서 test set의 사용 자체를 금지한다는 것을 알았기 때문에 고려도 안했던 방법이다. 나중에 부스트캠프 대회 규정을 살펴보니 test set을 활용해도 무방하다고 했더라. 몰랐다\u0026hellip;\ntest set data나 인터넷에서 크롤링한 이미지와 같이 unlabeled data를 학습 데이터로 활용하기 위한 방법론이다. 모델이 학습을 진행해서 inference을 할 수 있으면, 해당 모델을 통해 unlabeled data를 labeling해서 학습 데이터로 사용하는 방법이다. 즉, 기존 모델의 학습 방향을 강하게 강화시키는 역할을 한다.\n물론 도박에 가까운 방법론이다. 왜냐하면 기존의 모델 학습 방법이 옳다는 보장이 없기 때문이다. 물론 이번 대회와 같이 대회 막바지에 상위권을 향하는 모델이 만들어진다면 매우 유용한 방법이다.\n1, 2위와의 f1 score이 0.01차이였는데 수도 라벨링의 유무가 제일 컷던 것 같다.\nTTA # 대회 마감 하루 전에, 내가 제출했던 최고 모델에 대해서 0.7665를 0.7666으로 만든 방법론이다. 결과처럼 획기적인 성능 향상 방법은 아니고 약간 점수 굳히기 용도로 사용했다.\n후기 # 생각보다 놓친 방법론이 너무 많았다. 특히 validation set 구성은 치명적이었다.. 결과는 좋았지만 회사에서는 결코 하지 않을 방법들이다. 성적이 잘 나와서 좋은 점도 있지만, 그 만큼 놓쳤던 많은 방법들을 remind할 수 있는 기회였다.\n","date":"6 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-06-%EC%B2%AB%EB%B2%88%EC%A7%B8-ai-competition-%EB%A7%88%EB%AC%B4%EB%A6%AC/","section":"Posts","summary":"","title":"첫번째 Ai Competition 마무리","type":"posts"},{"content":"","date":"6 September 2021","externalUrl":null,"permalink":"/tags/cnn/","section":"Tags","summary":"","title":"CNN","type":"tags"},{"content":"","date":"6 September 2021","externalUrl":null,"permalink":"/categories/computer-vision/","section":"Categories","summary":"","title":"Computer-Vision","type":"categories"},{"content":"","date":"6 September 2021","externalUrl":null,"permalink":"/tags/pytorch/","section":"Tags","summary":"","title":"PyTorch","type":"tags"},{"content":" Albumentation # 속도, 다양성 면에서 pytorch 내장 transformation보다 좋길래 사용했다.\nTransformation 구성 # 가령, 아래와 같은 transformation을 학습에서 사용했다고 해보자.\ntransformation = A.Compose( [ A.Resize(224, 224), A.CenterCrop(100, 100), A.HorizontalFlip(p=0.5), A.OneOf( [ A.MotionBlur(p=0.2), A.MedianBlur(blur_limit=3, p=0.2), A.Blur(blur_limit=3, p=0.2), ], p=1, ), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ), albumentations.pytorch.transforms.ToTensorV2(), ] ) 그러면 inference에서도 아래와 같이 동일 구성의 크기 조절, crop, normalization을 해줘야 한다.\ntransformation = A.Compose( [ A.Resize(224, 224), A.CenterCrop(100, 100), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ), albumentations.pytorch.transforms.ToTensorV2(), ] ) Transformation 적용시점 # 나는 dataset을 생성할 때 parameter로 transformation을 받고 getitem에서 해당 transformation을 적용시켰다.\n사용한 transformation # 학습 # transformation = A.Compose( [ A.Resize(224, 224), A.HorizontalFlip(p=0.5), A.OneOf([A.GaussNoise()], p=0.4), A.OneOf( [ A.MotionBlur(p=0.2), A.MedianBlur(blur_limit=3, p=0.2), A.Blur(blur_limit=3, p=0.2), ], p=1, ), A.OneOf( [ A.HueSaturationValue(p=0.5), A.RGBShift(p=0.5), A.ChannelShuffle(p=0.5), ], p=1, ), A.ShiftScaleRotate( shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.4, ), A.CoarseDropout(p=0.5), A.ColorJitter(p=0.3), A.RandomBrightnessContrast(p=0.7), # A.Rotate(limit=(-10, 10), p=0.4), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], ), albumentations.pytorch.transforms.ToTensorV2(), ] ) TTA # A.GaussNoise(var_limit=(20.0, 60.0),p=1), A.MedianBlur(blur_limit=9, p=1), A.Blur(blur_limit=9, p=1), A.HueSaturationValue(hue_shift_limit=40, sat_shift_limit=40, val_shift_limit=40,p=1), A.RGBShift(r_shift_limit=50, g_shift_limit=50, b_shift_limit=50,p=1), A.ChannelDropout(p=1), A.ChannelShuffle(p=1), A.CoarseDropout(p=1), A.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.5,p=1), A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5,p=1), A.ShiftScaleRotate( shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=1, ), ","date":"6 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-06-transformationalbumentation/","section":"Posts","summary":"","title":"Transformation(Albumentation)","type":"posts"},{"content":" Bag-of-Words # 딥러닝 이전에 단어를 숫자로 나타내는 기법.\nBag-of-Words Representation # 1. Constructing the vocabulary conatining unique words. # 여러 문장에 걸쳐 중복되게 사용된 단어라도 Vocabulary에서는 한번만 표현된다.\n2. Encoding unique words to one-hot vectors. # Vocabulary에 존재하는 단어들을 일종의 categorical data로 볼 수 있어서 one-hot vecotr로 표기해보는 것. 가령, Vocabulary에 8개의 단어가 있다면 8차원의 one-hot vector를 구성하는 것이다. For any pair of words, the Euclid distance is $\\surd2$. For any pair of words, the cosine similarity is 0. 모든 내적의 조합이 0이니까.\n즉, 단어의 의미에 관계없이 모든 단어가 동일한 관계를 가진 형태로 벡터를 표현한다.\nBag-of-words vector # One-hot vector로 단어들을 표현했다면 문장을 one-hot vecotr로도 표현 가능하다. 즉, 문장을 모든 단어들의 one-hot vector의 합으로 나타내는 것인데 이것이 Bag-of-Words vector다.\nNaiveBayes Classifier # 문장이나 문서를 Bag-of-words vector로 표현했을 때, 해당 vector를 특정 category로 분류하는 방법론.\nd: document c: class P of c, given d. MAP: Maximum a posteriori = Most likely class. $P(c|d)$에서 가장 높은 확률을 가지는 class c를 선택하는 방법. Bayes rule에 의해서 두번째 수식으로 변경 가능. $P(d)$는 특정 document가 뽑힐 확률로 상수 취급 가능하다. 따라서 무시하면 세번째 수식을 도출할 수 있다. $P(d|c)$: category c가 고정됐을 때 문서 d가 나타날 확률. d는 w1, \u0026hellip;, wn까지의 word가 동시에 나타날 사건으로 볼 수 있다. 따라서 가장 왼쪽처럼 수식을 변화시킬 수 있다. 즉, $P(c)$와 $P(w_i|c)$를 추정할 수 있다면 NaiveBayes classifier에서 원하는 parameter들을 모두 추정할 수 있다.\n적용 # $P(c)$와 $P(w_i|c)$를 모든 경우의 수에 대해서 구해준다. 새로운 입력에 대해서 1번에서 구한 데이터를 활용해 category별로 $P(c)$와 $P(w_i|c)$를 구한다. argmax를 구한다. 문제점 # 학습 데이터 상에 존재하지 않는 입력이 들어올 경우, 해당 단어에 대해서는 $P(w_i|c)이 0이 된다. 즉, 다른 단어들이 특정 class와 아주 밀접한 관련이 있음에도 모든 클래스에 대한 확률이 0이 될 수 있다.\n=\u0026gt; regularization을 통해 해결한다고 한다.\n실제 계산 # $P(c)$와 $P(w_i|c)$은 단순히 개수를 세는 것처럼 구할 수도 있지만 실제로는 MLE 등을 활용해 구한다고 한다.\n","date":"6 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-06-bag-of-words/","section":"Posts","summary":"","title":"Bag-of-Words","type":"posts"},{"content":" NLP(Natural language processing) # NLU와 NLG로 나뉜다.\nNLU(Natural language understanding): 언어에서 의도한 바를 이해하는 것\nNLG(Natural language generation): machine이 nl을 어떻게 생성할지 가르치는 영역.\nMajor conference: ACL, EMNLP, NAACL\nLow-level parsing # 의미 추출을 위한 low level task들은 다음과 같다.\nToeknization # ref: https://bkshin.tistory.com/entry/NLP-2-%ED%85%8D%EC%8A%A4%ED%8A%B8-%ED%86%A0%ED%81%B0%ED%99%94Text-Tokenization\nToken: 문법적으로 나눌 수 없는 언어요소. Corpus: 말뭉치. 텍스트 표본. Toeknization: corpus로부터 token을 분리하는 작업. 즉, 문장을 token으로 이루어진 sequence로 이해한다.\nStemming # Stem: 어간 Stemming: 어간 추출 영어든 한글이든 단어의 어간, 말미에는 여러 변화형이 붙을 수 있다. 이러한 변화형들을 제거해서 단어의 본래 의미만을 추출하는 작업이다.\nWord and pharse level # NER(Named entity recognition) # 여러 단어로 이루어진 개체명(고유명사 등)을 인식하는 과정. 사람 이름, 시간, 회사 등을 인식하는 것.\nPOS(Part of speeching) tagging # 단어의 품사를 알아내는 작업.\nSentence level # Sentiment analysis # 문장의 감정 분석. 긍정적/부정적 등을 평가할 수 있는 과정.\nMachine translation # 기계번역. target 언어의 문법과 어순을 잘 고려해서 수행.\nMulti-sentence and paragraph level # Entailment prediction # 두 문장 간의 논리적인 모순 관계 예측.\nQuestion answering # 문장의 의미를 독해해서 사용자가 원하는 답을 해주는 것.\nDialog system # Chat bot과 같은 대화를 처리하는 task.\nSummarization # 주어진 문서를 요약하는 task.\nText Mining # Major conference: KDD, The WebConf(formerly, WWW), WSDM, CIKM, ICWSM\nExtract useful information and insights from text and document data\ne.g., 특정 인물의 시대별 이미지를 분석, 특정 키워드의 빈도수를 분석해 대중의 반응 분석 Document clustering(topic modeling)\n서로 다른 의미를 가지지만 같은 group으로 처리하는 task e.g., 가성비, 내구성, A/S 등의 키워드를 통해 특정 제품의 반응을 탐색한다. Highly related to computational social science\ne.g., social media data를 분석해 사회적 insight를 발견. Information retrieval # 검색 관련 기술.\n구글, 네이버와 같은 곳에서 사용되는 검색 기술은 정말 많이 발전해와서 현재는 발전 속도가 더디다. 가장 활발한 연구 분야는 추천시스템. e.g., 검색엔진이 사용자가 찾아볼법한 내용들을 선제시해준다. Trends of NLP # 2~3년 전의 CV 발전\n새로운 Convolution layer를 쌓는 방법 개발, GAN 활용하며 굉장히 빠르게 발전 NLP\nTransformer의 등장 전까지 CV에 비해 비교적 느리게 발전. LSTM, GRU오 같은 RNN 계열의 모델들이 주로 사용돼왔음. 2017년, \u0026lsquo;Attention is all you need\u0026rsquo;라는 논문 발표 이후로 거의 모든 NLP 모델들은 self-attention 기반의 transformer를 사용한다. Transformer # 본래 기계 번역을 위해 고안된 모델. 딥러닝 이전의 기계번역은 언어의 전반적인 rule을 전문가가 모두 정의하고 대응시켰다.\n딥러닝의 등장 후에는 RNN의 입력과 출력으로 같은 의미를 가지는 서로 다른 언어를 학습시켜서 기계 번역을 수행했다. 많은 기법들 덕분에 RNN 계열의 기계번역은 성능이 오를대로 올랐다.\n이러한 RNN보다 기계번역에서 더 좋은 성능을 보여준 것이 transformer다. 등장 이후에는 영상처리, 시계열예측, 신약개발, 신물질개발 등에서도 활용된다.\n본래 각자 도메인과 상황에 맞게 특화된 모델을 개발해서 사용하기 마련이다. 하지만, transformer 등장 이후에는 self-attention을 쌓아올려서 만든 거대한 모델을 자가지도학습(supervised learning)으로 학습해서 범용적 task를 처리할 수 있도록 만들었다. 이러한 모델은 후에 큰 구조적 변화가 없이 여러 분야에 transfer learning 형태로 적용하면, 해당 분야에 특화된 모델보다 더욱 뛰어난 성능을 보여주는 것이 증명됐다.\nNLP에서의 supervised learning # 빈 칸 채우기와 유사하다. 가령 \u0026lsquo;I study math\u0026rsquo;에서 study를 빈 칸으로 만들고 빈 칸에 들어갈 단어를 유추하도록 학습시킨다. 예시에서는 study에 해당하는 빈 칸에는 동사가 들어갈 자리이고, 문맥상 목적어로 math를 가질 수 있는 동사들을 후보군으로 가지고 학습을 진행할 것이다.\n이런 식으로 학습된 모델들이 BERT, GPT 등이 존재한다.\n특정한 task만을 처리하는 인공지능에서 보편적인 AI(Artificial Intelligence)에현대의 기술이 발전하는 것으로 볼 수 있다.\n하지만 이러한 자가지도학습을 위해서는 대규모 데이터, gpu 리소스가 필요하다. 테슬라도 모델 학습을 위한 전기세만 수십억원을 소모했다고한다..\nWord Embedding # 문장을 vector로 만들어 벡터 공간에서 sequence로 표현하는 것.\n","date":"6 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-06-nlp-%EA%B0%9C%EC%9A%94/","section":"Posts","summary":"","title":"NLP 개요","type":"posts"},{"content":"","date":"3 September 2021","externalUrl":null,"permalink":"/categories/rl/","section":"Categories","summary":"","title":"RL","type":"categories"},{"content":"","date":"3 September 2021","externalUrl":null,"permalink":"/tags/rl/","section":"Tags","summary":"","title":"RL","type":"tags"},{"content":"멘토님께서 찾아봐주신 강화학습 자료들이다. 정말 감사합니다..\n김성훈 교수 모두를 위한 머신러닝/딥러닝 - 강화학습 : https://hunkim.github.io/ml/ Stanford대 Richard sutton 교수의 Reinforcement Learning - Stanford University pdf (국내 번역본도 있음) [책] 강화학습 첫걸음 : http://www.yes24.com/Product/Goods/57617908?OzSrank=12 [책] 파이썬과 케라스로 배우는 강화학습 http://www.yes24.com/Product/Goods/89784478?OzSrank=5 [강의] 네이버 커넥트재단 : https://www.edwith.org/move37 [국내 유튜브] https://www.youtube.com/watch?v=wYgyiCEkwC8 [블로그] 강화학습 공부 자료 정리\n","date":"3 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-03-%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B3%B5%EB%B6%80-%EC%9E%90%EB%A3%8C/","section":"Posts","summary":"","title":"강화학습 공부 자료","type":"posts"},{"content":"https://programmers.co.kr/learn/courses/30/lessons/43163\n풀이 # ref: https://moseory20.tistory.com/31 처음에 bfs로 풀었는데 4번째 테스팅을 통과 못해서 결국 다른 분의 풀이를 봤다.\n내가 풀었던 것과 주요 포인트는 같았다.\n한글자만 달라지는 것을 감지 한글자만 달라졌다면 검사 시행 검사 시행 결과를 저장 다만 나는 재귀함수로 더 복잡하게 풀었고 이 분은 반복문으로 깔끔하게 풀었다.\nbfs, dfs를 재귀함수로 짜면 function의 parameter를 통해 새로운 subset문제를 정의한다. 이러한 과정을 반복문에서 변수들을 통해 세팅해 줄 수 있다.\n이해가 바로 가지 않았던 점은 bfs를 반복문에서 표현한 것인데 answer에 대한 증감 비교가 없는 점이었다. 왜냐하면 순서대로 탐색을 할 경우 분명 answer의 값이 다르게 나올 가능성이 있을거라 생각했기 때문이다.\n아래 코드를 보면 알 수 있지만, 순차적인 비교가 아니라 한꺼번에 비교하기 때문에 이렇게 비교해도 되는 것이었다.\ndiff_word에 한꺼번에 모든 word를 쌓아두고 비교하기 때문에 코드가 얻은 answer는 항상 minimize된 값이다.\n코드 # ref: https://moseory20.tistory.com/31\ndef solution(begin, target, words): if target not in words: return 0 answer = 0 word_len = len(begin) word_list = [begin] diff_word = list() while(1): for wl in word_list: diff_word.clear() for word in words: diff = 0 for idx in range (0, word_len): if wl[idx] != word[idx]: diff += 1 if diff \u0026gt; 1: break if diff==1: # 1글자 차이 diff_word.append(word) words.remove(word) answer += 1 if target in diff_word: return answer else: word_list = diff_word return answer ","date":"2 September 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-09-02-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%8B%A8%EC%96%B4-%EB%B3%80%ED%99%98/","section":"Posts","summary":"","title":"[프로그래머스] 단어 변환","type":"posts"},{"content":"https://www.acmicpc.net/problem/17297\n풀이 # ref: https://mountrivers.github.io/boj17297/\nF(m) 아래와 같이 m번째 문자에 대해서 피보나치 수열로 표현할 수 있는 문자열이다.\nF(m) = F(m - 1) + F(m - 2)\n다행히도 문자열은 Messi와 Gimossi만 존재해서 문자열의 수를 예측하는 것 자체는 쉽다. 관건은 F(m)에서 마치 binary search를 하는 것 마냥 F(m)을 탐색할 수 있는지를 떠올려야 한다.\n공백을 무시하고, 문제를 재현한 값들은 아니지만 다음의 상황을 가정해보자.\nN = 100 F(51) = 130 F(50) = 80 F(49) = 50 F(48) = 30 그럼 F(51)에 해당하는 문자열에는 최소한 N번째 문자가 있음을 예상할 수 있다. F(50)이 80글자이기 때문이다.\nF(51) = F(50) + F(49)의 순서로 문자열이 구성된다. 이 순서가 매우 중요하다. 왜냐하면 F(50)의 글자 수에 따라서 100번재 문자가 F(50에 있는지, 혹은 F(49)에 있는지 결정되기 때문이다.\n여기서는 F(50) = 80이므로 F(50)에는 100번째 문자열이 올 수가 없다.\n따라서 F(49)에서 100번째 문자열을 찾는 과정을 반복해야한다. 즉, 100 - 80 = 20번재 문자열을 F(49)에서 찾는 문제로 바뀐 것이다.\n만약 F(50)이 100보다 컷다면 F(50)에서 100번째 문자열을 찾는 문제로 문제를 바꾸면 된다.\n공백을 고려하면 공백을 즉시 탐색할 수도 있다. F(50)과 F(49) 사이의 인덱스가 100이라면 100번째 문자열은 공백인 것을 O(1)에 알 수 있다.\n코드 # 이 문제를 해설해주신 블로거께서는 C++로 짜셨고, 해당 코드를 python으로 옮기면 아래와 같다.\nimport sys N = int(sys.stdin.readline()) pibo = [] b = \u0026#39;Messi Gimossi\u0026#39; q = 5 w = 13 pibo.append(q) pibo.append(w) while w \u0026lt; 1073741824: e = w w = w + q + 1 q = e pibo.append(w) i = 0 while pibo[i] \u0026lt; N: i += 1 \u0026#39;\u0026#39;\u0026#39; String Order F(M) = F(M-1) + F(M-2) \u0026#39;\u0026#39;\u0026#39; while i \u0026gt;= 2: # Detect space, exit immediately if N == pibo[i - 1] + 1: N = -1 break # If target is located on F(M - 2) elif N \u0026gt; pibo[i - 1]: # Decrease counter i -= 2 # Decrease N with F(M - 1) N -= pibo[i + 1] + 1 # If target is located on F(M - 1) else: i -= 1 if N == -1 or N == 6: print(\u0026#39;Messi Messi Gimossi\u0026#39;) else: print(b[N - 1]) ","date":"2 September 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-09-02-%EB%B0%B1%EC%A4%80-messi-gimossi/","section":"Posts","summary":"","title":"[백준] Messi Gimossi","type":"posts"},{"content":"","date":"2 September 2021","externalUrl":null,"permalink":"/tags/pyton/","section":"Tags","summary":"","title":"Pyton","type":"tags"},{"content":"대회 막바지에 점수를 올리기 위해 사용했던 기법들이다. 극적인 성능 향상은 아니고 점수 굳히기 느낌이었다.\nTTA(Test time augmentation) # ref: https://chacha95.github.io/2021-06-26-data-augmentation2/\n확정된 모델이 존재할 때 사용할 수 있는 방법이다.\n확정된 모델에 대해 각종 augmentation이 적용된 이미지를 개별적으로 넣어서 나온 출력을 ensemble하는 방법론이다. ![](/assets/images/마지막 점수 올리기/4738e4db-bdab-4b73-9942-f3efd86cc2f3-image.png)\n여러 출력을 ensemble하는 방법은 자유롭게 정한다. 보통은 soft voting을 사용한다고 한다. 여러 방법을 사용 가능한데 굳이 soft voting을 사용하지 않을 이유가 없다. soft voting이 hard voting에 비해 overfitting을 방지하면서 성능향상을 노리기 더 좋기 때문이다.\n하지만 꼭 soft voting이 만능해결법은 아니다.\nref: https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting\n어떤 class에 대한 probability 가 일정 값보다 큰 모델들만 따로 모아서 이들에 대해서만 Hard Voting 을 진행한다.\n위의 예시가 hard voting을 사용하는 전형적인 예라고 한다.\nSoft voting 구현 # 다양하게 구현할 수 있는데 내가 본 예시는 다음의 두 가지다. 모듈을 굳이 찾아보진 않았는데 모듈을 쓰는 것보다 구현하는게 더 빠를 것 같았다.\nvalidation 단계에서 batch image에 대해 n개의 augmentation을 적용해서 나온 각각의 출력들이 $Output_n$이라고 하자. 그러면 단순하게 n개의 tensor들을 더해주고 n으로 나눠준다. transoformation을 정하고, 모델과 dataset, dataloader를 만들어 validation을 하는 과정을 하나의 파이프라인으로 만들어져 있다고 하자. 이러한 파이프라인에 대해 동적요인으로 transformation을 줘서 여러 파이프라인에 대한 출력을 구하고 class index에 대해 평균을 구한다. 난 2번째 방법을 택했는데, 구현의 편의성이나 간결성을 고려하면 첫번째가 압도적으로 편하다. 구조상 어쩔 수 없이 두번째를 택했다.\nHalf precision # 이건 적용이 된지 모르겠다. batch size가 2배정도 늘어나야 정상인데 40에서 50으로밖에 안 늘어난다거나 이전과 동일한 경우가 허다하다. 속도는 그대로인거 같고.. 뭔가 docs에서 하라는대로 적용은 했는데 하나도 안된다. nightly에서만 제대로 적용되나 싶었서 nightly로 설치해봐도 안되더라.\n적용만 제대로 된다면, 학습의 초창기부터 적용해도 좋을 기법이다. 부동소수점은 16비트로 써서 학습속도와 batch size에서 2배 이상의 이득을 볼 수 있다고 한다.\n대회 막바지에 알게되서 이 포스팅에 넣었다.\n","date":"2 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-02-%EB%A7%88%EC%A7%80%EB%A7%89-%EC%A0%90%EC%88%98-%EC%98%AC%EB%A6%AC%EA%B8%B0/","section":"Posts","summary":"","title":"마지막 점수 올리기","type":"posts"},{"content":"","date":"1 September 2021","externalUrl":null,"permalink":"/tags/numpy/","section":"Tags","summary":"","title":"Numpy","type":"tags"},{"content":" dtype=object, when compelx list # list를 numpy로 변환하는 과정에서 str과 numerical 데이터를 혼용하면 numpy의 dtype이 obejct로 고정된다. 이 때는 해당 numpy를 index slicing해도 dtype이 바뀌지 않고, astype으로 바꾸려하면 오류가 뜬다.\na = {somthing compelx Nd list} b = np.array(a) only_numerical = b[ {some slicing selecting only numerical data} ] only_numerical.astype(np.float16) -\u0026gt; error!! 이 때 np.stack을 쓰니 한방에 해결됐다..\nnp.stack(only_numercial) np.mean # soft voting을 구현하던 중에 np.mean을 사용해야하는 상황이 있었다.\n\u0026gt;\u0026gt;\u0026gt; pred.shape (10, 12800, 3) 10은 augmentation의 수, 12800은 데이터의 수, 3은 class의 수이다. 원하는 것은 각각의 augmentation 결과들이 가진 class 예측결과들의 평균을 구하는 것이다. 마지막 행의 평균을 구하는 것이니 다음과 같이 예측했었다.\nnp.mean(pred, axis=-1) 하지만 생각해보니 axis=0을 주는 것이 의도한 것이었다. 포인트는 다음과 같다.\nnp.mean의 axis는 사라진다. 가령 axis=2면 2번째 axis가 사라진다. 행과 열로 생각하면 Nd에서 사고 자체가 안되니, 편하게 axis 번째의 데이터에 대한 연산을 한다고 생각하자. 가령 [[1,1,1,],[2,2,2,]]에서 axis=1의 평균을 구하는 것은 1, 1, 1의 평균을 구하는 것이다. 즉 이러한 포인트를 감안하면 axis=-1이 아니라 axis=0을 줘야한다. agumentation을 기준으로 평균을 구해야하기 때문이다.\n\u0026gt;\u0026gt;\u0026gt; pred.shape (10, 12800, 3) \u0026gt;\u0026gt;\u0026gt; np.mean(pred, axis=0).shape (12800, 3) ","date":"1 September 2021","externalUrl":null,"permalink":"/posts/ml/2021-09-01-%ED%97%B7%EA%B0%88%EB%A6%AC%EB%8A%94-numpy/","section":"Posts","summary":"","title":"헷갈리는 numpy","type":"posts"},{"content":"","date":"31 August 2021","externalUrl":null,"permalink":"/tags/dl/","section":"Tags","summary":"","title":"DL","type":"tags"},{"content":"필드에서는 앙상블을 시도하기 위한 노력을 모델과 학습 파이프라인을 최적화시키는데 사용한다고 한다. 하지만 competition에서는 소수점 한자리 이하의 싸움이 있기 때문에 앙상블을 활용해서 점수를 올리는 것이 중요하다.\nEnsemble # 대부분의 모델들을 학습시켜보면 overfitting이 빈번하기 발생한다. 물론 데이터가 너무 작고 편향돼서 underfitting이 발생할 수도 있지만 흔한 경우는 아니다.\n아래 그림을 보면 이해가 편할 것이다. ref: https://bywords.tistory.com/entry/%EB%B2%88%EC%97%AD-%EC%9C%A0%EC%B9%98%EC%9B%90%EC%83%9D%EB%8F%84-%EC%9D%B4%ED%95%B4%ED%95%A0-%EC%88%98-%EC%9E%88%EB%8A%94-biasvariance-tradeoff\nVoting # ref: https://devkor.tistory.com/entry/Soft-Voting-%EA%B3%BC-Hard-Voting\nHard voting: majority class를 선발 Soft voting: class 간의 평균을 출력 Weight voting: model의 출력에 각각의 weight를 곱해주고 weight의 합으로 나눠준다. ","date":"31 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-31-ensemble/","section":"Posts","summary":"","title":"Ensemble","type":"posts"},{"content":"","date":"31 August 2021","externalUrl":null,"permalink":"/tags/ensemble/","section":"Tags","summary":"","title":"Ensemble","type":"tags"},{"content":" Gradient Accumulation # gpu가 좋은 상황이 아닐 경우 사용할만한 방법이다.\nnum_accum = 2 optimizer.zero_grad() for epoch in range(10): running_loss = 0.0 for i, data in enumerate(train_loader, 0): inputs, labels = data outputs = net(inputs) loss = criterion(outputs, labels) / num_accum loss.backward() if i % num_accum == 0: optimizer.step() optimizer.zero_grad() num_accum만큼의 epoch을 돌아야지 model의 parameters를 업데이트. critertion의 결과물에 num_accum을 나눠주는 이유는 일반화때문이라고 한다. 뇌피셜: num_accum동안의 loss를 한번의 step만에 반영해줘야하기 때문에 개별적인 loss값에 일정한 가중치를 두어 균등화하는 효과를 가지는 것 같다. ","date":"30 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-30-training-proecss/","section":"Posts","summary":"","title":"Training proecss","type":"posts"},{"content":" amp # nvidia에서 torch에 넣어준 그 \u0026lsquo;amp\u0026rsquo;다. FP16으로 계산하게 해준다 해서 사용할 계획.\nhttps://pytorch.org/docs/stable/notes/amp_examples.html\nautocast에서 자동으로 해주는 첫번째 방법 사용. 다만 극적으로 성능이 향상되지는 않는다. nvidia benchamark에서도 일의자리 한자릿수 정도에서 성능향상이 이루어지더라.\nlabel smoothing (loss) # model의 결과에 softmax를 거쳐서 생긴 결과를 사용해왔다. 그러지말고 model의 결과를 그대로 사용하면서 라벨을 실수로 표현하기 위해 사용한다.\n가령\n[0,1,0,0]\n이러한 모델의 출력을\n[0.025, 0.925,0.025,0.025]\n이렇게 바꿔서 loss를 구한다. 실수로 바꿀 때 방법마다의 차이가 있겠지만 그냥 비율을 계산해서 구하는 것으로 알고 있다.\nArcFaceLoss # 개념: https://aimaster.tistory.com/93 구현: https://www.kaggle.com/underwearfitting/pytorch-densenet-arcface-validation-training\n좀 더 알아봐야겠다..\nClass pivot 변경 # 현재 class의 age feature는 30세, 60세를 기준으로 3개의 그룹을 형성한다. class의 분포를 살펴보면 유독 30세 부근과 60세 이상에 사람의 수가 적다. 클래스 불균형이 있어서 임의로 데이터의 class를 조작해서 학습해보니 성능이 좀 더 잘 나왔다.\n분석 # gender, mask 착용 여부와 같은 feature들처럼 이산적으로 명확하게 구별되는 feature들에 비해 age는 애매한 면이 많다. 사회 경험이 많은 사람도 30세와 29세를 구분하지 못하는 경우가 분명 많을 것이다.\n따라서 class간의 pivot에 조작을 가하더라도, age를 제대로 학습하고자하는 의도에서 멀어지지 않을 것이라고 생각했다.\n결과 # 30세와 60세인 기준점을 29세와 59세로 바꾸니 f1 score 기준으로 0.05 정도의 성능향상이 꾸준히 발생했다. 다만 29세와 58세로 기준점을 변경하면 되려 성능이 하락했다.\nwandb # trainer가 한번의 step당 다음의 정보들을 업데이트하도록 변경했다.\nacc, loss, val_acc, val_loss, f1_score, val_f1_score, learning_rate\n하나의 workspace가 여러 tag를 가지고 있도록 구성해서 나중에 필터링하기 편하도록 구성했다. augmentation중 핵심적인 기법들 같이 학습에 구별되서 사용된 기법들을 tag에 더 넣기로 했다. ![](/assets/images/추가적인 학습 기법들/675f679a-b603-46cc-bd23-5aa7f0eafb36-image.png)\n","date":"30 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-30-%EC%B6%94%EA%B0%80%EC%A0%81%EC%9D%B8-%ED%95%99%EC%8A%B5-%EA%B8%B0%EB%B2%95%EB%93%A4/","section":"Posts","summary":"","title":"추가적인 학습 기법들","type":"posts"},{"content":"그 동안 헷갈리고 모호하게 알고 있던 내용들을 정리했다.\nTraining, validation 순서 # def train(): for epoch in range(epcoh): training() validate() 이 순서가 맞다. 아래처럼 해도 모델이 input data에 대해서 학습을 하긴한다.\ndef train(): for epoch in range(epcohs): training() for epoch in range(epcohs): validate() 문제는 validate 시점이 모든 trainig이 끝난 시점이라는 것이다. 즉, 최종적으로 학습이 끝난 모델을 다시 epoch만큼 validation하는 것이다. 자원만 잡아먹는 쓸모없는 작업이다.\n제대로된 순서로 진행해야 한번의 step에 대한 학습 결과를 validate해서 평가에 반영할 수 있다.\nK fold cross validation # 말 그대로 validation 기법이다. 따라서 아래처럼 학습에 사용하면 안된다.\ndef train(): for epoch in range(epcohs): training() for epoch in range(epcohs): validate() def kfoldvalidate() # do something... train() kfoldvalidatie() 내 생각에는 학습에 사용해도 무방하지만 사용한다면 다음처럼 앙상블 러닝과 같이 사용할 것 같다.(뇌피셜\u0026hellip;)\ndef train(): model_list = MakeManyModel() for idx, train_set, validate_set in enumerate(kfold(dataset)): for epoch in range(epcohs): training(mode_list[idx]) for epoch in range(epcohs): validate(model_list[idx]) return model_list def kfoldvalidate(model_list) SelectBestModel(model_list) train() for i in range(k): kfoldvalidatie() 하나의 fold에 대해서 하나의 모델이 존재하고 k개의 모델 중에서 가장 좋은 모델을 사용하는 것이다. 굳이 제일 좋은 모델을 선택하지 않고 실제 앙상블러닝처럼 Voting을 활용해도 되겠다. 자원이 너무 많이 소모될 것 같아 시도하진 않았다..\n","date":"29 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-29-%ED%97%B7%EA%B0%88%EB%A0%B8%EB%8D%98-training-%EB%B0%A9%EB%B2%95/","section":"Posts","summary":"","title":"헷갈렸던 training 방법","type":"posts"},{"content":"","date":"28 August 2021","externalUrl":null,"permalink":"/tags/cutmix/","section":"Tags","summary":"","title":"Cutmix","type":"tags"},{"content":" CutMix # 본래 cutmix는 랜덤하게 이미지 패치를 샘플링한다. 해당 방식이 마스크 이미지에선느 썩 효과적이지 못할 수도 있다. 마스크를 착용여부, 성별, 나이를 알기 위해서는 얼굴만 detection해서 patch를 하는것이 가장 효과적일 것이다. 따라서 랜덤하게 패치하고자한다면, 얼굴 영역 내에서 해야한다.\n하지만 얼굴 detection을 하기 위해서는 또다른 수고가 들어간다\u0026hellip; 막막하다. 찾아보니 다른 분들은 cutmix를 vertical하게 줘서 성능향상이 있었다고 한다.\n구현 # https://github.com/naem1023/boostcamp-pstage-image/blob/main/loss_set/cut_mix.py 이전 포스팅에 있던 pytorch implementation 코드와 다른 분이 공유해주신 vertical cutmix 코드를 합친 구현물이다.\ncutmix는 구현 자체가 어렵다기보다는, 이를 학습에서 어떻게 반영하고 평가지표를 산출할지가 너무 어려웠다.\n","date":"28 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-28-cutmix-vertical/","section":"Posts","summary":"","title":"CutMix vertical","type":"posts"},{"content":" CutMix # https://hongl.tistory.com/223 random crop보다 효과 있다는 CutMix를 사용하기로 했다. 가령, crouput은 개의 얼굴을 그대로 날려버린다. CutMix는 개의 얼굴부분에 고양이를 붙여서 학습하겠다는 것이다.\n목적: $(x_A, y_A)$를 $(x_B, y_B)$와 결합하여 robust하고 성능이 좋은 모델 생성. 결합 방법: $\\lambda$만큼의 결합비율을 사용한다. 결합비율은 아래와 같이 사용된다. $M \\in {0,1}^{W\\times H}$, normalized했기 때문이다. 구현 # 공식 레포: https://github.com/clovaai/CutMix-PyTorch pytorch implement 레포: https://github.com/hysts/pytorch_cutmix\nimplement repo의 cutmix.py와 train.py를 보면 감이 잡힌다.\nloss # cutmix를 dataloader에서 collate_fn으로 사용하면 dataloader의 라벨 출력은 두 tensor가 나온다. 첫번째는 원래 라벨이고 두번째는 random shuffle되어 원래 이미지에 patch된 이미지의 라벨이다.\ncutmix를 거친 이미지 tensor와 두 라벨 tensor에 대해서 각각 loss값을 구한 후 lambda와 1-lambda만큼 가중치를 부여주해주면된다.\npytorch implementation을 보면 cross entropy를 쓰고 있다. 자유롭게 criterion을 변경할 수 있게 돼있으니 바꾸고 싶은대로 바꾸자. 나는 데이터에서 class 불균형이 심해서 focal loss를 쓰고 있다.\n평가지표 # accuracy, loss # accuracy는 위 수식에서 lambda를 사용해 y hat을 구한 것과 같이 구하면 된다. 실제로 그만큼의 비율이 사용됐기 때문이다.\nloss도 마찬가지로 crossentropy를 통과시킨 값에 대해서 위와 같은 수식을 적용시켜주며된다. 이뉴는 accuracy와 동일.\nf1 score # 보통 sklearn에 y와 predicted_y를 함께 넘겨줘서 계산한다. predicted y는 이미 준비돼있지만 y의 경우가 문제다.\nCutMix를 Dataloader의 collate function으로 구현됐는데, 원본 y와 shuffled y를 각각 1개씩 받도록 설정돼있다. 즉, dataloader를 통해 받은 y는 2개의 y set으로 구성돼있고 다른 지표들(accuracy, loss)은 lambda와 (1-lamda)를 두 결과에 각각 곱하고 더해서 하나의 scalar 지표를 얻는다.\nf1 score도 batch 단위로 새롭게 구해지는 lambda를 활용해 f1 score를 구하자.\n(origin f1 score) * lambda + (random shuffle f1 score) * (1 - lambda)\nbatch 별로 구하고 하나의 epoch에 대해서는 batch f1_score들의 평균을 구해 사용하자.\n","date":"27 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-27-cutmix/","section":"Posts","summary":"","title":"CutMix","type":"posts"},{"content":" 4주차 학습정리 # 강의 복습 내용 # ai competition (1~6번 포스팅) # https://velog.io/@naem1023/series/Ai-competition\n과제 수행 과정 / 결과물 정리 # 학습 계획 수립: https://velog.io/@naem1023/TIL-train-%EA%B3%84%ED%9A%8D-%EC%A0%95%EB%A6%AC-2021.08.24\n과제 수행 과정 정리: https://velog.io/@naem1023/TIL-%EC%BD%94%EB%94%A9-%EC%A0%95%EB%A6%AC-2021.08.2527\n피어세션 정리 # 대회가 있다보니 학습을 위한 방법들을 공유했다. 가령, hyper parameter는 어떻게 줄 것인지. 어떤 모델을 쓰는 것이 과연 의미가 있는지.\n내부적인 결론은 어떤 cnn 모델을 쓰든 크게 결과는 달라지지 않은 것 같다라고 결정지었다. mobilenet, resnet, efficientnet이든 소수점 아래에서 유의미한 차이가 있긴하지만 결정적인 차이를 보여주진 않았다.\n일단 다양한 방법을 시도하는 것이 좋다고 의견이 모였다. 배운 것도 많을뿐더러 배운 것들을 실제로 적용해봐야하기 때문이다.\n학습 회고 # 21/08/23: jupyter 노트북으로 여러 방법들 실험해 대회에 제출 21/08/24: jupyter 노트북으로 실험한 내용들을 pipeline으로 만들어서 프로젝트 구성. 21/08/25: efficientnet-b7, VOLO, BiT, CaiT를 시도 21/08/26: efficientnet-b4, resnet18로 간단하게 테스팅하면서 프로젝트 개선 21/08/27: cut mix 구현\n","date":"27 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-27-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-4%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 4주차 학습정리","type":"posts"},{"content":"","date":"26 August 2021","externalUrl":null,"permalink":"/tags/ai-comeptition/","section":"Tags","summary":"","title":"Ai Comeptition","type":"tags"},{"content":"","date":"26 August 2021","externalUrl":null,"permalink":"/tags/til/","section":"Tags","summary":"","title":"TIL","type":"tags"},{"content":"코딩, 검증에 정신없어서 기록을 하나도 못했다. 몰아서 정리해야겠다.\nconda, pip # conda가 dependency conflicts 측면에서는 압도적으로 좋다. 다만, 네이버 서버에서는 매우 느렸다. 대부분 conflicts가 발생했고 설치된 모든 파이썬 모듈에 대해서 conda가 conflicts를 해결하기 위해 5분 이상이 걸렸다. 특히 윈도우에서는 더욱 오래 걸렸다.\nconda-forge를 사용하면 설치하지 못하는 모듈이 없지만, 작은 모듈들은 pip로 설치했다.\nwandb # tensorboard에 비해 매우 좋았다.\ntensorboard: 서버사이드의 log파일에 남은 파일을 서버가 읽어준다. wandb: python script 상에서 개발자가 직접 update할 요소, update 시기, update 객체를 지정 wandb 객체 # 하나의 wandb의 run 객체가 wandb 상에서 하나의 name을 가지는 지표 모음집이 된다. ![](/assets/images/TIL 코딩 정리 2021.08.25~27/4ecb11b3-9b07-4405-bd98-f669fec6ae26-image.png)\n이 때, 하나의 run 객체에서 train과 validation을 한번에 update해주면 step을 표시에서 문제가 발생한다. ![](/assets/images/TIL 코딩 정리 2021.08.25~27/952e8ca1-65f4-4fff-8b42-e627f2e0ccaf-image.png)\n개발자가 의도한 것은 train, validation 지표들이 분리되서 진행되는 것이다. 하지만 동일 run 객체에 update하게 되면 train, validation 지표가 번갈아가면서 업데이트된다.\ne.g., 110 step은 train이 업데이트되고, 1115는 validation이 업데이트.\n해결책 # train과 validation 진행시 서로 다른 wandb run 객체를 불러와서 update해야한다. 코드를 다 뜯어고쳐야겠다..\nFine tuning # class num을 조절할 수 있게 해주는 pre trained model들만 찾다보니 선택지가 별로 없었다. 생각해보면 pytorch의 특성을 살려서 사용하고 싶은 아무 모델이나 쓰면 된다.\nPre trained model 선정 print로 model 출력 출력단 layer 이름 확인 출력단 layer의 이름을 model의 attribute로 호출해서 layer output 변경. 보통 linear neural network이니 xavier로 초기화. 개발 / 테스트 # 무조건 작은 네트워크로 프로그램을 테스트 후, 결과가 필요할 때 무거운 모델로 학습. 무식하게 efficientnet-b7으로 테스트하지말고 resnet-18이나 더 작은 모델로 테스트한다.\n다만, 이전처럼 서버에서 모든 것을 해결하지 않는다. 데스크탑에서 os dependency가 없는 코드를 구현해 테스트하고, 이를 서버에서 사용하는 방식을 사용 중이다. 즉, 서버는 계속 train만 하고 개발은 데스크탑에서만 한다. 다행히 rtx3070 덕분에 colab을 안 써도 괜찮다.\n경로 구분자 # 습관적으로 리눅스의 경로 구분자인 \u0026lsquo;/\u0026lsquo;를 사용하게 되는데, 의식해서 os.sep이나 os.path.join을 사용해서 os 종속성을 없애자.\nensemble learning # 내가 생각했던 ensemble learning module들은 임의의 model을 넣으면 알아서 본인만의 방법론으로 최적의 model을 만들어주는 줄 알았다. 실제로 pytorch 전용으로 그러한 모듈이 있긴하다.\nhttps://ensemble-pytorch.readthedocs.io/en/stable/quick_start.html\n하지만, custom 여지도 없고 불편한데다 효과도 없었다. XGBoost, LightBGM같은 방법론도 찾아봤는데 CNN과는 다른 성질의 모델 자체였다.\n마스크 구분 문제를 CNN으로 푸는 것이 취지인 대회이니 아예 voting을 구현하는 것이 제일 좋은 것 같다.\nmodel save name # 이전 회사에서도 날짜를 활용해 디렉터리를 생성 후, 해당 디렉터리에 plt 출력 파일, 그래프 이미지 파일, 텐서보드 로그 파일, 프로그램 로그 파일 등등 온갖 부산물을 다 넣었다.\n그 때와 유사하게 이번에도 날짜별로 디렉터리를 만들고 model의 이름, feature 이름, 평가 지표 등을 파일 이름에 넣기로 했다.\nearly stopping # 이전 회사에선느 이걸 휴리스틱하게 했는데, 생각해보면 기계적으로 깔끔하게 처리하는 것을 구현하는데 100줄도 안되는 코드로 구현이 된다. 왜 지금까지 생각을 못했을까\u0026hellip;\n일정 step만큼 특정 지표가 상승되지 않다면 학습을 멈추게 해주는 클래스를 만들어서 쓰기로 했다.\ndelta # https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py github에 미리 구현된 early stopping을 보면 delta라는 것을 비교 수식에 더해준다.\nearly stopping은 지표가 수렴할 때 멈추고자 하는 것인데, 수렴의 정확한 의미를 수치 상의 단순 비교로 처리하기 어려운 경우가 많다.\n가령, 6번의 step 동안 평가 지표가 상승하지 않아야지 early stopping을 한다고 해보자. 이 때, 지표가 아주 미세하고 상승하고 다시 하락하는 행위가 반복된다면 early stopping을 할 수 없다. 즉, 지표가 특정 수치 부근에서 수렴하는 듯할 때 학습이 멈추길 바랬지만 단순 수치 상의 비교를 하게 되면 지표가 조금이라도 변할 때 early stopping counting을 멈춘다.\n따라서 지표가 진동하는 듯한 그 범위를 delta로 잡아주고 비교 수식에 더해준다. 빼주지 않는 이유는 아래 수식은 loss를 기반으로 하고 있기 때문에 loss가 상승하는 것을 count하지 않기 위함이다.\nelif score \u0026lt; self.best_score + self.delta: self.counter += 1 project sturcture # 수업에서 알려준 python template을 많이 참고했다. https://github.com/victoresque/pytorch-template\n현재 사용 중인 프로젝트 구조는 다음과 같다. ![](/assets/images/TIL 코딩 정리 2021.08.25~27/fa5294a5-736c-4735-a8d0-5f919ceddf39-image.png)\ngit # config 추적 # 데이터 파일들의 경로, 프로그램 설정 값 등을 config.json이나 config.py로 관리하게 된다. 이 때 git으로 아래처럼 관리하자.\nconfig file 추적을 할 경우 os 별로 서로 다른 config file을 사용하도록 설정 config file 추적을 안 할 경우 신경쓸거없다. 배포 시에 따로 config 파일을 배포할 방법을 찾아야한다. 윈도우와 리눅스를 번갈아가면서 사용하고 있어서 config 파일을 추적하지 않지만, 제대로 배포할거면 config 파일을 추적해서 os 별로 다르게 사용하도록 하자.\ngit 추적 중지 # 사용할 일이 없어서 몰랐는데 알아두자. https://kamang-it.tistory.com/entry/TipGit-%EC%82%AC%EC%9A%A9%EC%8B%9C-%ED%8A%B9%EC%A0%95-%ED%8C%8C%EC%9D%BC%EC%9D%84-%ED%8A%B8%EB%9E%98%ED%82%B9%EC%9D%84-%ED%95%98%EA%B8%B0-%EC%8B%AB%EC%9D%84-%EA%B2%BD%EC%9A%B0\nconfig 파일, csv 부산물들을 추적 중지할 때 사용했다.\ngit rebase # 전 회사에서 merge만 쓰다가, rebase로 다시 merge하라고 했을 때 문제 해결하려고 안간힘 썻던 기억이 난다.. 깔끔하게 브랜치 관리가 편한 rebase를 되로고 쓰자. 사용법을 까먹어서 docs를 보고 참고했다. https://git-scm.com/book/ko/v2/Git-%EB%B8%8C%EB%9E%9C%EC%B9%98-Rebase-%ED%95%98%EA%B8%B0\ngit default branch # local에서 default branch를 변경하는 것은 command로 해결 가능하다. https://stevenmortimer.com/5-steps-to-change-github-default-branch-from-master-to-main/\n하지만 remote repository의 default branch를 변경하는 것은 반드시 repository 관리자가 github 웹사이트에서 행해야하더라.\n","date":"26 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-26-til-%EC%BD%94%EB%94%A9-%EC%A0%95%EB%A6%AC-2021.08.25~27/","section":"Posts","summary":"","title":"TIL 코딩 정리 2021.08.25~27","type":"posts"},{"content":"","date":"24 August 2021","externalUrl":null,"permalink":"/tags/ai-competiton/","section":"Tags","summary":"","title":"Ai Competiton","type":"tags"},{"content":" 정리 # 학습 계획은 이전 TIL에 적은 그대로이긴하다. 다만 ensemble learning으로 학습할 방도가 떠오르지않아 일단은 단순 if문으로 처리했다.\n3개의 feature 조건에 따라서 18개 클래스가 결정된다. 일일히 if문을 18개 넣어도되지만, python itertools의 product를 써서 해결했다.\nmask = [0, 1, 2] gender = [0, 1] age = [0, 1, 2] label_number = list(product(mask, gender, age)) 전제조건은 3개의 featuer와 class 번호가 오름차순이어야한다는 것이다. 다행히 그랬다. 3개의 model에서 나온 출력을 label_number에 대조시켜서 최종적인 class를 얻으면된다.\n그림으로 표현하면 다음과 같다.!\nundefined\nEnsemble learning # ref: https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-11-%EC%95%99%EC%83%81%EB%B8%94-%ED%95%99%EC%8A%B5-Ensemble-Learning-%EB%B0%B0%EA%B9%85Bagging%EA%B3%BC-%EB%B6%80%EC%8A%A4%ED%8C%85Boosting\nBagging(parallel): 동일한 출력을 가진 모델을 사용한다고 해보자. 이 모델과 동일한 구조를 가진 여러 모델이 동일 데이터 셋에서 반복 추출하여 학습하는 행위(Bootstrap Aggregation)를 여러번 반복한다. 같은 데이터셋이라도 표본을 여러번 뽑으면 학습 효과가 증가한다고 한다. Boosting(sequential): 서로 다른 모델을 활용한다고 한다. 이전 모델의 결과를 다음 모델 학습 시, 데이터에 가중치를 부여하는 방식으로 재활용하여 사용한다고 한다. 두 방법의 사용은 왕도가 없다고 한다. 도메인과 문제에 적합한 방법을 사용하자.\nBoosting은 Bagging에 비해 error 적다. Boosting은 학습 속도가 느리고 overfitting될 가능성이 높다. 모델의 낮은 성능이 문제다 -\u0026gt; Boostring으로 해결해보자. Overfitting이 문제다 -\u0026gt; Bagging으로 해결해보자. 모델 변경 # https://paperswithcode.com/sota/image-classification-on-imagenet 참고한 cnn 랭킹.\n기존: resnet18, 학습속도가 매우 빨라 테스트 용도로 용이 efficientnetb7: epoch=5, kfoldsplit=2로도 9위를 찍었다! 하지만 매우 느리다. 안전하게 사용할 수 있는 최저선이라고 생각하자. volo: 성능이 매우 좋다고 했지만 model의 output이 이상하게 나와서 못 써먹었다. 랭크표에서 성능이 매우 좋았는데 아쉽다. CaTi: ViT의 일종이라고하는데 pretrained 모델이 없어서 accuracy가 38%로 나왔다..;; https://github.com/facebookresearch/deit https://github.com/facebookresearch/deit/blob/main/README_cait.md https://paperswithcode.com/paper/going-deeper-with-image-transformers#code BiT: Google에서 transformer로 만든 Image Classification이다. 이것도 순위권에 있길래 시도 중이다. https://rwightman.github.io/pytorch-image-models/models/big-transfer/ 계획 # 일단 efficientnet을 epoch, kfoldsplit을 늘려서 학습해보자.. 8시간이 날라갔다 ㅠ\n","date":"24 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-24-til-train-%EA%B3%84%ED%9A%8D-%EC%A0%95%EB%A6%AC-2021.08.24/","section":"Posts","summary":"","title":"TIL train 계획 정리 2021.08.24","type":"posts"},{"content":" Data Generation # Data feeding # Data feeding을 잘해보자.\n![](/assets/images/Data Generation/ba02679f-71ab-49da-800f-2b178061cab7-image.png)\n가령, 위처럼 코딩해놨다고 하자. 두 코드 모두 비효율적이다. 첫번째는 generator의 속도가 model보다 느리기 때문에 model이 제 성능을 내지 못한다. 두번째는 model의 속도가 generator보다 느리기 때문에 generator가 제 성능을 내지 못한다.\n다만, 보통 model의 성능이라하면 gpu의 성능을 의미하게 된다. 즉, 둘 중 한 가지 상황을 선택해야한다면 model의 성능을 극대화시키는 두번째 방향을 선택하는 것이 좋겠다. 물론 이 또한 상황마다 모두 다르기 때문에 적절하게 선택하자.\ntransforms # ToTensor() RandomRotation([-8, +8]) Resize((1024, 1024)) 위 3개 transforms에 대한 성능은 순서에 의존된다. 만약 이미지의 크기가 1024보다 작다면 resize를 마지막에 하는 것이 가장 성능이 좋을 것이다. 당연하다. 100x100에 대해서 tensor로 변환하고 rotation 변환을 하는 것이 1024x1024 이미지에 대한 연산보다 빠를 것이다.\nalbumentations # pytorch의 transpose보다 빠르고 기능이 많다고 한다. 새로운거 배우는 김에 써보기로 했다.\nimport albumentations as A import albumentations.pytorch transformation = A.Compose( [ A.Resize(224, 224), A.HorizontalFlip(p=0.5), A.OneOf([A.GaussNoise()], p=0.2), A.OneOf( [ A.MotionBlur(p=0.2), A.MedianBlur(blur_limit=3, p=0.1), A.Blur(blur_limit=3, p=0.1), ], p=0.2, ), A.OneOf( [ A.CLAHE(clip_limit=2), A.Sharpen(), A.Emboss(), A.HueSaturationValue(), A.RGBShift(), A.ChannelShuffle(), ], p=0.3, ), A.ShiftScaleRotate( shift_limit=0.2, scale_limit=0.2, rotate_limit=10, border_mode=0, p=0.5, ), A.RandomBrightnessContrast(p=0.2), A.Rotate(limit=(-30, 30), p=0.2), A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225],), albumentations.pytorch.transforms.ToTensorV2(), ] ) 대부분의 기능들은 메서드 명 그대로이다. p = probability ShiftScaleRoate는 image전체를 rotate하는데, 이미지에 빈 공간이 생기기 rotate도 해준다. Normalize에서 rgb의 mean, std를 직접 지정할 수도 있다. OneOf: OneOf의 구성요소 중 한가지를 선택해준다. 이것에 대해서도 p를 지정할 수 있다. ","date":"24 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-24-data-generation/","section":"Posts","summary":"","title":"Some tips","type":"posts"},{"content":" Preprocessing # bounding box # 필요 이상의 정보를 거르자. 문제는 보통 그냥 이미지만 덜렁 주어진다는 것이다. 개발자가 알아서 적절한 방법을 찾아야한다. 수업 때 배웠던 YOLO를 써도되고, 대부분 이미지 중앙에 마스크 사진이 있으니 중앙crop만 해도 되고\u0026hellip; 이것저것 해봐야겠다.\nResize # 원본 크기로 계산하면 좋겠지만 이미지의 w, h, channel을 고려하면 정보의 양은 매우 크다. 즉, 정보의 손실과 계산의 효율성 사이에서 적절하게 균형을 찾아야한다. 오히려 계산량을 조금 줄여서 여러번 학습하는게 효율적일 수도 있다.\n도메인 지식 활용 # Kaggle에서 안구 관련 데이터 이미지를 가져왔다. 원본 이미지를 그대로 사용하지 않고 약간의 전처리를 거친 것이다. 밝기를 올리고 채도를 살짝 낮춘 것 같다. 이런식으로 도메인에서 활용이 적절하다고 생각되면 바로 활용해보자.\nData Augmentation # Bias, Variance # 벌써 2년 사이에 4번이나 배우는 내용이다. 하지만 관점이 약간 다르다. 그 동안은 generalization이 잘 되는 모델을 공부하기 위해 위의 내용들을 봤다.\n하지만 지금은 데이터의 노이즈 관점에서 생각할 수 있다. 즉, 우리가 원하는 아주 이상적인 데이터만 존재하는 것은 불가능하고 현실 세계의 문제 또한 노이즈가 매우 많다. 따라서 해당 노이즈에 대해서도 잘 처리할 수 있는 모델을 학습하기 위해, 노이즈에 대한 전처리, augemntation(증대)가 필요하다.\nTrain, validation # 작년에 pytorch를 처음보고 굉장히 궁금했던 내용인데, 나는 validation set의 결과가 train 결과에 피드백이 되서 weight가 조절되는 줄 알았다. 하지만 막상 pytorch 코드를 보니 그냥 validation의 결과만 보고 학습이 끝나길래 뭔가 싶었다.'\n이렇게 validation set을 굳이 나누는 이유는 학습에 활용되지 않은 데이터 분포가 필요하기 때문이다. 즉, 그냥 학습을 다 돌려버리면 test set에 model을 돌려보지 않는 이상 학습이 잘 됐는지 알 수 없다. 내 코드는 그저 train set에 fitting된 모델일 뿐이니까 당연하다.\n따라서 굳이 train set에서 validation set을 만들어주는 것이다. 내가 진행한 학습이 제대로 되고 있는지 사람이 확인할 수도 있고, hyper parameter tuning을 위한 지표로도 사용 가능하다.\nTest set은 절대 건드리지 않는다!! 보는 것은 cheating일 뿐더러 generalization도 잘 안된다.\nData Augmentation # 데이터를 일반화하는 과정. 주어진 데이터가 가질 수 있는 case(경우), state(상태)를 다양하게 변경하여 일반화하는 것이다.\n가령, 자동차 사진이 있다고 하자. 지금까지처럼 이 사진만으로도 학습을 할 수 있지만, 이미지의 상태와 경우는 매우 다양하다. 사진과 같이 밝지 않은 상태를 가정할 수도 있고, 비가 오는 상황을 가정할 수도 있다.\n그리고 실제로 이러한 다양한 상황에서도 모델이 동작하는 것이 목적이다. 따라서, 데이터에 이러한 노이즈를 추가해서 데이터의 variance를 늘려서 robust한 model을 만들 수 있다.\ntorchvision.transforms # https://www.cse.iitb.ac.in/~vkaushal/talk/auto-augment/\n그림처럼 사진을 다양하게 변형시켜서 variance를 늘리는 방법이다. 다만, 중요한 것은 실제로 현실세계에 있을법한 variance를 고려해야한다.\n가령, 이번 이미지 대회는 마스크 사진 검출이다. 따라서 매장 앞에서 손님들을 찍은 사진에 대한 검출이 목적이다. 손님이 천장에 매달리지 않는 이상 상하반전 사진은 기대할 수 없다는 뜻이다. 굳이 transform에서 상하반전을 할 필요는 없을 것이다.\n이런 식으로 도메인 지식을 적극적으로 활용하자.\nAlbumentations # pytorch의 transforms보다 빠르고 다양하다고 한다. 사용해보자.\n정리 # 무조건 해야되고, 무조건 좋은 method는 결코 없다. 가정과 실험을 통해 검증하면서 사용하자.\n","date":"24 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-24-preprocessing/","section":"Posts","summary":"","title":"Preprocessing","type":"posts"},{"content":"TIL 자체를 체계적으로 정리하지않고 카테고리별로 포스팅 후, 주간학습 정리에서 TIL을 간략하게 적었다.\n지식을 배울 때는 쓸모있는 방식인데 매일매일 해커톤과 같이 계속 코딩할 때는 매우 비효율적인 느낌이다. 대회가 진행되는 P stage에서는 TIL을 직접 적어야겠다.\nTrain 계획 # feature는 여러개가 있지만, feature 간의 상관관계, 인과관계가 있다고 장담할 수 없다. 그리고 없는게 당연하다. 데이터를 보아하니 그럴 수 있는 데이터가 아니다\u0026hellip; 그래서 아래와 같이 앞으로 어떻게 학습할지 정리해봤다.\nundefined\nundefined\nundefined\nundefined\n요약 # 글씨가 개판이다.. 정리하면 feature 별로 모델을 생성 후, Ensemble learning을 할 것이다. 각각의 모델을은 각각 동일한 images를 입력으로 받고, feature에 대한 classification을 하는 모델이다.\nJupyter vs py # 회사에서 jupyter를 안 쓰고 cli python으로만 코딩한게 1년이 넘어가니까 아무래도 후자의 방식이 훨씬 편하고 정리하기도 좋다. jupyter로 쓰면 노트북 하나에 그 많은 코드를 다 때려박아야되니 가시성이 끔찍하게 안 좋다.\n다만, 이번에는 실험용으로 jupyter를 활용했다. 데이터 셋, pandas 사용법 등 여러 class, function들을 미리 jupyter에서 만들어보고 이를 .py에 적용했다.\n다만 이렇게 해도 불편한 점들은 여전히 존재한다.\nmodule 관리할 수 있도록 따로 추가 코딩 필요. global variable을 자주 활용하는 jupyter 특성 상, .py에 그대로 적용이 어려워 추가 코딩 필요. 결국은 .py에 그대로 코딩 중이다.\npycharm ssh # education 버전으로 pycharm을 쓰고 있어서 ssh, sftp를 써서 서버사이드 코딩이 되는 것을 알고 있었다. 다만, 내가 해봤을 때는 vscode의 remote-ssh보다 불편한 점들이 한두개가 아니라서 안썼다.\n몰랐는데 오늘 피어세션에서 다 된다는 정보를 들었다. 시간이 없으니 주말에나 도전해봐야겠다. ide의 완성도가 pycharm이 한참 좋아서 갈아타는게 무조건 이득이다.. 다만 대회에서 포트를 전부 open 안해놔서 pycharm으로는 디버깅이 안된다고 한다.\n","date":"23 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-23-til-train-%EA%B3%84%ED%9A%8D-%EC%A0%95%EB%A6%AC-2021.08.23/","section":"Posts","summary":"","title":"TIL train 계획 정리 2021.08.23","type":"posts"},{"content":" 접근방법 # 대회 시작하자마자 데이터 열어보지말고 overview check. EDA(Exploratory Data Analysis) # ![](/assets/images/ai competition/26691a6c-56b0-43d9-9f13-bebacd91e628-image.png) 탐색적 데이터 탐색.\ninput이 될 X에 대한 분석 target이 될 y에 대한 분석 X, y관계를 확인할 수 있는 분석 EDA in image classification # input이 될 X에 대한 분석 X는 Image가 됩니다. X에 대한 특성(feature)은 어떤 것이 있을까요?? 이미지 사이즈 분석 대상이 되는 객체의 위치 RGB 채널별 통계 값 이미지에서 r,g,b 중 어떤 것이 유독 강한가? target이 될 y에 대한 분석 y는 저희가 맞추고자 하는 값이며 y값에 대한 특성은 어떤 것이 있을까요?? y값에 독립적 분포 확인 class의 개수를 확인해보자. ex) y_1의 분포는? y값 들간의 관계 분포 확인 class 간의 개수가 확연하게 차이가 나는가? ex) y_1, y_2 정보를 섞은 분포는? X, y 관계를 확인할 수 있는 분석 X특성과 y의 특성 간의 분포 차이는 어떻게 있을까요?? 이미지 사이즈와 y 특성의 관계 이미지를 키웠을 때 학습이 잘 되는 경우도 있다. 이미지의 사이즈를 바꿨을 때 학습이 잘 되는 경우를 찾아보자. RGB 통계값과 y 특성의 관계 rgb channel shift: r,g,b의 순서를 섞어서 채널에 종속되게 학습하지 않도록. 객체의 위치와 y 특성의 관계 데이터의 노이즈 확인 ex) y 값이 잘못 부여된것이 있을까?? ","date":"23 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-23-ai-competition/","section":"Posts","summary":"","title":"ai competition","type":"posts"},{"content":"","date":"23 August 2021","externalUrl":null,"permalink":"/tags/competition/","section":"Tags","summary":"","title":"Competition","type":"tags"},{"content":"","date":"22 August 2021","externalUrl":null,"permalink":"/tags/data-viz/","section":"Tags","summary":"","title":"Data Viz","type":"tags"},{"content":" Facet # 분할. 즉, 하나의 데이터 셋에 대해서 서로 다른 방법으로 시각화를 하는 것.\n서로 다른 인코딩을 통한 다른 인사이트 같은 방법으로 동시에 여러 feature 보기 큰 관점, 작은 관점 등\u0026hellip; matplotlib에서의 facet은 여러개의 figure, ax를 통해 표현할 수 있다.\nGrid spec # 말 그대로 ax를 마치 grid처럼 보는 것이다. grid처럼 사용하는 방법은 두가지가 있다.\nnumpy와 같은 slicing x, y, dx, dy 사용 ax 내부에 subplot 추가 # 미니맵과 같은 형태로 추가된다.\n","date":"22 August 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-08-22-facet/","section":"Posts","summary":"","title":"facet","type":"posts"},{"content":"","date":"22 August 2021","externalUrl":null,"permalink":"/categories/visualization/","section":"Categories","summary":"","title":"Visualization","type":"categories"},{"content":"실습에 사용된 코드가 너무 많아, 코드 관련된 내용은 쥬피터 노트북에만 정리해놨다.\n연속형(sequential) # sequential data에 적합 연속적 색상으로 표현 발산형(divere) # 중앙을 기준으로 발산 상반된 값(기온), 서로 다른 2개(지지율) 데이터에 적합 보통 양 끝으로 갈수록 짙은 색 중앙의 색은 양쪽의 점에 편향되지 않은 색 예시 # 대한민국 평균 기온 데이터\nHSI # matplotlib의 color api는 hsi를 사용한다고 한다. 대학교 컴퓨터 비전 시간에 배웠던 \u0026lsquo;그 색공간\u0026rsquo;이다..\nHue(색조): 색상, 보색이 존재한다. 보색끼리 더하면 흰색이다. 보통 색조의 차이가 가장 알기 쉽다. Saturation(채도): 흰색과 순수색의 혼합 비율. \u0026lsquo;연하다\u0026rsquo;, \u0026lsquo;진하다\u0026rsquo; 등으로 표현. Lightness(광도): 명암. 밝기. color palette # 모듈에서 기본적으로 제공해주는 palette도 있지만, github에 다양한 color palette가 있다. 전문적으로는 adobe color를 사용한다고 한다. https://color.adobe.com/create/color-wheel\nRColorBrewer palettes # https://www.datanovia.com/en/blog/top-r-color-palettes-to-know-for-great-data-visualization/\n이게 구분이 잘되고 이쁘다고 하셨다.\n","date":"22 August 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-08-22-color/","section":"Posts","summary":"","title":"color","type":"posts"},{"content":" GPUUtil # nvidia-smi같은 모듈 gpu, mem 통계를 지속적으로 콘솔에 찍어준다. !pipi install GPUtil import GPUtil GPUtil.showUtilization() tensor의 누적 # gpu 메모리를 사용하는 tensor 변수들이 있다. (대부분)\n이런 변수들의 값이 지속적으로 누적되는 loop문이 있다면 gpu의 메모리가 금방 고갈될 것이다.\ne.g.,\ntotal_loss = 0 for i in range(10): optim.zero_grad() output = model(input) loss = criterion(output) loss.backward() optim.step() total_loss += loss ## here!!! 이렇게 누적되거나, 한번만 사용하거나, 간단한 tensor의 경우는 되도록 python 기본 객체로 변환해서 처리하자.\nOut of memory(OOM) # batch size = 1로 해보고 이것저것 실험해보며 메모리를 확인해보자. torch.no_grad() # inference(추론) 시점에서는 반드시 사용하자. 당연한건데, 사용하지 않으면 학습 과정과 동일하게 backward pass가 쌓인다.\nmodel의 사이즈 # 가령 LSTM은 메모리를 꽤 많이 잡아먹으니, 모델 자체의 사이즈도 고려하자.\ntensor dtype # float precision을 16bit로도 사용 가능.\n","date":"22 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-22-trouble-shooting/","section":"Posts","summary":"","title":"Trouble shooting","type":"posts"},{"content":" Hyperparameter tuning # Hyperparameter # 개발자가 수동으로 정해야하는 값들\nlearning rate size of model optimizer의 종류 epoch etc\u0026hellip; 개요 # 모델, 데이터, hyper paramter 중에서 hyper parameter의 수치는 중요도가 가장 떨어진다. 모델이 가장 중요하지만 보통 좋은 모델들은 널리 알려져있다. 따라서 데이터를 가장 중요시한다. hyperparameter에 너무 힘쓰진 말자. 그럼에도 마지막 0.01 단위의 성능 향상을 위한다면 반드시 수행해야될 것! 최근에는 AutoML 계열의 NAS model의 경우, 자동으로 hyperparameter tuning도 수행해준다고 한다. recipe: hyperparameter를 어떻게 튜닝할지 미리 모델에서 결정해주는 것 방법 # grid: 일정한 간격을 두고 수치 탐색 random: 랜덤하게 수치 탐색 최근에는 베이진안 기법들도 사용(BOHB) Ray # multi node, multi processing module 기본적으로 ml/dl을 위한 모듈이지만, 최근에는 python에서 기본적으로 사용하게 되는 병렬처리 모듈이라고 한다. hyperparameter tuning시에 가능성이 없는 수치들에 대해서는 미리 가지치기해준다. ","date":"22 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-22-hyperparameter-tuning/","section":"Posts","summary":"","title":"Hyperparameter tuning","type":"posts"},{"content":"","date":"22 August 2021","externalUrl":null,"permalink":"/tags/hyperparameter-tuning/","section":"Tags","summary":"","title":"Hyperparameter Tuning","type":"tags"},{"content":"","date":"22 August 2021","externalUrl":null,"permalink":"/tags/tqdm/","section":"Tags","summary":"","title":"Tqdm","type":"tags"},{"content":"tqdm을 쓰면 원래 accuracy나 loss를 따로 찍어줘야한다. tqdm 내에서만 사용되는 변수를 update하는 것으로 이걸 해결해볼 수 있다.\nhttps://adamoudad.github.io/posts/progress_bar_with_tqdm/\n","date":"22 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-22-tqdm-with-epoch-statics/","section":"Posts","summary":"","title":"tqdm with epoch statics","type":"posts"},{"content":" 3주차 학습정리 # 강의 복습 내용 # pytorch (1~11번 포스팅) # https://velog.io/@naem1023/series/pytorch\ndata viz (3~4번 포스팅) # https://velog.io/@naem1023/series/Data-Viz\n과제 수행 과정 / 결과물 정리 # Custom model # custom model 과제의 경우 assert까진 아니더라도 유닛테스트와 비슷한 형식으로 검증 코드들이 있어서 과제 수행 여부를 체크하기 편했다. 가령, tensor([1])인지 혹은 tensor([1.0], dtype=float64)인지..\n과제량이 많아서 수행시간은 길었지만 시간을 투자한만큼 진행됐다. 강의에서는 짧게 언급하고 넘어간 부분들을 과제에서 상세하게 다시 상기시킬 수 있었다. 덕분에 velog에 강의 내용 정리하기 편했다.\npytorch model을 처음 다루는 것이 아니지만, dimension 관련 부분을 풀 때는 처음 언어를 접했던 것마냥 막막했다. torch.gather나 dimension을 맞춰서 답을 맞춰야하는 부분들이 그랬다.\nCustom dataset/dataloader # 앞선 과제와 다르게 문제 서술이 애매한 부분이 많았다. 내가 전혀 다뤄보지 않았던 내용들도 많아서 시간을 정말 많이 썻다.. 마지막 NLP dataset문제에서는 4시간을 쓴거 같다.\ntorchtext의 vocab과 vocab을 통해 encoding dictionary를 만드는 부분에서 정말 많이 해맸다. 생성하는데 시간이 너무 걸렸는데, 알고보니 vocab에서 따로 관련 메서드를 지원해주더라. vocab docs에서 vocab 생성 관련된 부분이 너무 짧아서 docs는 묻혀두고 과제를 했었는데, docs를 자세히 읽도록 하자..\n피어세션 정리 # 이번주는 피어세션에서 도움이 되고자 진행했던 알고리즘 스터디가 주된 내용이었다. 아무래도 다들 필수과제를 금요일까지 힘들게 푼 것도 있어서, 남는 요일에 토의할 내용이 비어있기도 했었다. 석사과정을 다들 꺼려하는 분위기라 알고리즘 스터디에 다들 호의적이었다.\n기초부터 시작하자는 마인드여서 탐색, 정렬 알고리즘부터 시작했다. 다른분들은 C++로 하셔가지고 속도 관련 이슈가 없었는데, 나는 python으로 해서 pypy3로도 안 풀리는 문제들이 몇몇 있었다. 특히 정렬 문제에서 심했는데, I/O에서 속도 저하가 매우 심했다. 검색으로 해결하긴했는데, 백준 문제는 c++로 푸는 것을 고려중이다..\n학습 회고 # 21/08/17: 저번주 선택과제 ViT 해결, pytorch 메서드 정리, 블로그 정리 21/08/18: 필수 과제 1번 해결, pytorch 메서드 공부, 블로그 정리 21/08/19: 필수 과제 2번 해결, 블로그 정리 21/08/20: mlops 관련 정보 검색, 블로그 정리\n","date":"20 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-20-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-3%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 3주차 학습정리","type":"posts"},{"content":" node # system과 동일하게 쓰이는 용어\nmodel parallelization # ![](/assets/images/Multi gpu/94cb8fdb-a3de-471a-a93c-998795dd3c17-image.png) model parallelization은 alex-net에서 이미 쓰였던 적이 있다.\n![](/assets/images/Multi gpu/67beca0a-0359-4bb8-8dd6-e8b1ebe83c05-image.png) 좋은 gpu parallelization은 그림처럼 gpu가 동시에 사용되도록 파이프라인 구조로 코딩해야한다.\ndata parallelization # ![](/assets/images/Multi gpu/31b41aa1-a575-4581-aef3-26b9cce4c85f-image.png)\ngpu1에서 데이터 취합 후 뿌려주기 각자 알아서 forward gpu1이 forward 취합 gpu1이 gradient 정보 뿌려주기 각자 알아서 gradient 계산 graident 취합 후 계산 pytorch의 DataParallel # 위 방식을 그대로 구현할 수 있다. 단순히 데이터를 분배 후, 평균을 취함 gpu 불균형 사용으로 인해 batch size 감소 pytorch의 DistributedDataParallel # 각각의 gpu가 cpu 스레드를 할당받아서 알아서 평균 취한 결과를 구한다.\nsampler: dataloader에서 어떻게 데이터를 sampling 할지 결정해주는 객체. torch에서 제공해준다. train_sampler = torch.utils.data.distributed.DistributedSampler(train_data) shuffle = False pin_memory = True train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, shuffle=shuffle, pin_memory=pin_memory, num_workers=4, sampler=train_sampler) num_workers: thread 수. 보통 gpu의 4배로 사용한다고 한다. pin_memory: 데이터는 memory에 page되고 pinned되고 gpu에 올라간다고 하는데 바로 pinned되게 해준다고 한다. def main(): ngpus_per_node = torch.cuda.device_count() world_size = ngpus_per_node torch.multiprocessing.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, )) worker를 만들고 python에서 map 함수를 쓰듯이 spawn에 넣어준다.\nref: https://blog.si-analytics.ai/12\n","date":"20 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-20-multi-gpu/","section":"Posts","summary":"","title":"Multi gpu","type":"posts"},{"content":"model 자체 코딩보다는 모델을 어떻게 다룰 것인지.\nTransfer learning # 다른 데이터셋으로 만든 모델(pre-trained model)을 현재 데이터에 적용 텅 빈 모델로부터 개발하지 않아서 효율적 대용량 데이터셋으로 만들어진 모델을 사용시, 성능은 더 좋다. 가장 일반적인 학습 방법 일부분만 변경하여 학습 수행 CNN: torchvision NLP: HuggingFace가 사실상 표준 e.g., vgg로 binary classification을 하고 싶다면, torchvision에서 pre-trained vgg를 load하고 vgg 끝에 linaer layer를 추가한다.\nsource task, target task # ![](/assets/images/pytorch transfer/b0288c4b-3ec2-42f4-81f0-43f4151b2e00-image.png)\n선택과제에서 제시된 과제였다. transfer learning의 대표적인 예시였다. 즉, source task에서 학습된 지식을 target task로 전이하는 것이 목적이다.\n목적: fashion-mnist 데이터를 학습, 분류 해결방법: source task로 imagenet, mnist_resnet을 설정한다. source task의 모델이 pretrained 돼있다면 target task의 모델로 바로 쓰자. source task의 모델에 변경이 필요하다면, 일부 레이어를 추가, 변경한다. 변경된 레이어들만 weight, bias를 초기화하고 재학습을 한다. target task에서는 이러한 모델을 받아서 다시 레이어를 변경, 추가해야된다면 source task에서 했던 방식대로 변경, weight\u0026amp;bias init, 재학습을 한다. Frozen # parameter update, backpropagation을 전체에 대해서 적용하지 않고, pre-trained model의 일부 레이어에서만 적용한다. 즉, pre-trained된 parameters 일부를 유지하면서 나의 데이터셋에 맞게 튜닝하는 것이 목적.\n![](/assets/images/pytorch transfer/fe30774b-f113-4b19-bc16-4909c3dd0e30-image.png)\nStepping frozen # 학습 step별로 frozen하는 layer를 바꿔준다.\npth, pt # pytorch 모델의 확장며들이다. 둘 다 사용 가능하지만, pth는 이미 python에서 사용중인 확장자라고 한다. 따라서 pt를 권장한다.\nnn.BCEWithLogitsLoss() # Binary Classification을 할 때, Loss를 계산해주는 criterion으로 사용한다. 이러면 모델의 마지막에 sigmoid가 없어도 sigmoid를 달아준다.\n","date":"20 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-20-pytorch-transfer/","section":"Posts","summary":"","title":"pytorch transfer","type":"posts"},{"content":"쓸 때마다 쓰기 싫고 사용법 익히자마자 잊어버리는 pandas다. 까먹지 않게 정리해둔다.\nPandas # tabular 데이터를 다루기 위한 라이브러리. R의 체계를 많이 따왔다고 한다. numpy와 통합되면서 성능도 향상됐다고 한다.\nDataFrame # Data table 전체를 포함하는 object. 모든 데이터의 wrapper라고 생각하면 된다. DataFrame의 Serires들은 Series마다 data type이 다를 수 있다. Series # ojb = Series(data=data, index=index) ojb.index # -\u0026gt; index list ojb.values # -\u0026gt; only list of values DataFrame 중 하나의 column에 해당하는 object. numpy의 wrapper지만 indexing에서 다른 점이 있다. numpy처럼 숫자로만 indexing이 되는 것이 아니라 문자로도 가능하다. data에 list를 넣으면 자동으로 숫자가 indexing. data에 dict를 넣으면 자동으로 dict 형태에 맞춰서 indexing. index parameter가 최우선으로 indexing 우선권을 가진다. read_csv() # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\npd.read_csv(data, sep=\u0026#39;\\s+\\, header=None) data: file system, web url 모두 가능 seperator: seperator 지정 s: single space +: 여러개가 있다. 나도 이 정도만 썻던거 같다. 필요하면 docs에서 계속 찾아가면서 쓰자. head(n) # 상위 n개의 데이터만 불러온다.\ncolumns # 리스트 형태이고 column들의 이름을 지정할 수 있다.\ndf_data.columns = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] values # pandas를 numpy 형태로 반환한다.\niloc, loc # loc는 column 명을 접근하는 방식을 지원한다. iloc는 numpy처럼 접근 가능하게 해준다. 난 iloc가 편한다..\n","date":"19 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-19-pandas/","section":"Posts","summary":"","title":"pandas","type":"posts"},{"content":"","date":"19 August 2021","externalUrl":null,"permalink":"/categories/pandas/","section":"Categories","summary":"","title":"Pandas","type":"categories"},{"content":"","date":"19 August 2021","externalUrl":null,"permalink":"/tags/pandas/","section":"Tags","summary":"","title":"Pandas","type":"tags"},{"content":"","date":"19 August 2021","externalUrl":null,"permalink":"/tags/dataloader/","section":"Tags","summary":"","title":"Dataloader","type":"tags"},{"content":"","date":"19 August 2021","externalUrl":null,"permalink":"/tags/datdaset/","section":"Tags","summary":"","title":"Datdaset","type":"tags"},{"content":"졸업프로젝트 때 직접 dataset, dataloader를 구현했었는데 시간에 쫓겨서 개발한지라 정말 개발새발로 내 기억 속에 남아있다.. 이 참에 헷갈리거나 몰랐던 내용들 위주로 정리해봤다.\ndata 흐름 # ![](/assets/images/pytorch dataset, dataloader/a659d452-c938-4671-a516-55c059003eec-image.png)\n중요한 것은 데이터를 tensor로 바꿔주는 것 또한 따로 고려를 해야한다는 것이다. 난 그냥 아무 곳에나 마구잡이로 막 넣었는데 \u0026hellip;\ntorch.utils.Data.Dataset # _len_, _getitem_ 등은 데이터에 맞춰 적당히 개발하면 된다.\nTensor 변환 # __getitem__에서 안한다! 즉, data를 load하는 시점에서는 데이터를 tensor로 변환하지 않는다. 학습이 시작되는 시점에 transformer와 같은 함수를 통해 일괄적으로 tensor로 변환한다.\n다행히 cpu, gpu가 병렬적으로 이러한 작업들을 하게 개발되서 빠르다고 한다.\n최근에는 HuggingFace라고 표준화된 라이브러리를 사용하기도 한다고 한다.\ntorch.utils.Data.DataLoader # data의 batch를 생성해주는 클래스 학습직전(gpu feed전) 데이터 변환 담당 tensor로 변환 병렬적 데이터 전처리에 대한 고민이 필요하다. 아래 블로그 참고 https://subinium.github.io/pytorch-dataloader/\nsampler # getitem의 idx를 조절하는 방법을 정의한다고 한다. batch_sampler도 동일하다고 하다.\ncollate_fn # getitem을 통해 [[data, label], [data, label], [data, label] \u0026hellip; ]과 같은 형식으로 모여있는 batch를 [[data, data, data \u0026hellip;], [label, label, label, \u0026hellip;]]로 바꿔주는 형식을 정의한다고 한다.\ntorchvision.transforms # data_transform = transforms.Compose([ transforms.RandomSizedCrop(224), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) 이처럼 아예 데이터 변환을 위한 transform을 따로 구성해야 한다. 졸업프로젝트처럼 dataset에서 하나하나 전부 변환하는 바보같은 짓 하지말자..\n","date":"19 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-19-pytorch-dataset-dataloader/","section":"Posts","summary":"","title":"pytorch dataset, dataloader","type":"posts"},{"content":"","date":"19 August 2021","externalUrl":null,"permalink":"/tags/apply/","section":"Tags","summary":"","title":"Apply","type":"tags"},{"content":"nn.Module의 모든 하위 모듈들에 일괄적으로 적용하고 싶은 함수를 map과 같이 적용시켜주는 함수다.\nPostorder traversal 방식으로 module들을 순회한다고 한다. left child 우선으로 탐색.\ndef do_something(m): # do something! return m model = #something very complex model result = model.apply(print_module) ","date":"19 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-19-pytorch-apply/","section":"Posts","summary":"","title":"pytorch apply","type":"posts"},{"content":"","date":"18 August 2021","externalUrl":null,"permalink":"/tags/hook/","section":"Tags","summary":"","title":"Hook","type":"tags"},{"content":"고등학생 때 DLL injection으로 피카츄 배구 해킹같은걸 했었는데, 그 때 사용했던 기법들이 일종의 hooking이다. 그런 기법들을 공식적으로 pytorch의 nn.Module에서 지원해준다.\n규칙 # pytorch의 hook들은 다음과 같은 규칙을 가진다.\nreturn이 있다면 해당 return을 본래 객체에 적용한다. return이 없다면 기존 객체의 동작대로 동작한다. hook될 함수는 객체로 전달되기 때문에 아무 이름이나 붙여되 된다. 아래 코드들을 보면 알거다.\ntensor hook # tensor는 backward에 대해서만 hook을 지원한다.\ntorch.tensor.register_hook(function)\nnn.Module hook # 아래와 같은 4개의 hook을 지원한다.\nregister_forward_pre_hook register_forward_hook register_backward_hook (deprecated) register_full_backward_hook forward_pre_hook 형식 # def pre_hook(module, input) return Anything\nreturn이 있다면 forward의 input을 Anything으로 바꿀 수 있다. return이 없다면 단순히 input을 조회할 뿐이다.\nforward_hook 형식 # def hook(module, input, output) return Anything\nreturn이 있다면 forward의 결과값이 Anything으로 교체된다. return이 없다면 단순 조회.\nfull_backward_hook # def module_hook(module, grad_input, grad_output)\nreturn이 있다면 backard()를 통해 grad_ouput으로 업데이트 될 때, grad_output을 교체할 수 있다. return이 없다면 단순 교체.\n","date":"18 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-18-pytorch-hook/","section":"Posts","summary":"","title":"pytorch hook","type":"posts"},{"content":" Template # 지금까지 졸업프로젝트, 회사 인턴이나 알바들을 할 때 tf도 쓰고 pytroch도 썻지만 template이란 것들 정해두고 쓰지 않았다. 중구난방하게 필요에 따라서 디렉터리와 스크립트를 추가하고 분리하고 구현하고\u0026hellip;\n물론 템플릿이 만능은 아니다. 어느 템플릿이 그렇듯 흥망성쇠를 하겠지만 그래도 형식이란 것을 갖추고 개발을 시작하는 것만큼 효율적인 것은 없다고 생각한다.\nhttps://github.com/victoresque/pytorch-template\n예제 템플릿인데 템플릿은 물론이고 여러 구현 기법들이 포함돼있기에 참고하기 좋다.\ngetattr # https://technote.kr/249\n객체의 속성을 가져올 때, 이름으로 가져올 수 있게 해준다.\nclass Person: def __init__(): self.name = \u0026#39;a\u0026#39; jack = Person() getattr(jack, \u0026#39;name\u0026#39;) \u0026gt;\u0026gt;\u0026gt; \u0026#39;a\u0026#39; 객체의 모든 속성에 대한 접근은 항상 하드코딩 방식이다. 가령, Jack의 name에 접근하기 위해서는 무조건 다음과 같이 명시해야 한다.\njack.name 이 때, 속성을 가변적으로 사용하고 싶을 때 getattr을 사용할 수 있다. 즉, 속성이 바뀔 때마다 코드를 변경하지 않고 config.json과 같은 설정 파일만 수정해서 사용할 수 있게 해준다.\nabstract # java, c++ 등에서는 abstract를 function 명 앞에 명시해서 abstract method를 정읳나ㅡㄴ데, python은 decorator를 통해 정의한다.\n@abstractmethod def _train_epoch(self, epoch): \u0026#34;\u0026#34;\u0026#34; Training logic for an epoch :param epoch: Current epoch number \u0026#34;\u0026#34;\u0026#34; raise NotImplementedError 그냥 정의하면 안되괴, NotImplemnetedError를 raise해줘서 에러도 정의해준다.\n","date":"17 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-17-pytorch-template-tip/","section":"Posts","summary":"","title":"pytorch template, tip","type":"posts"},{"content":"numpy와 비슷하다.\nview, reshape # 동일한건줄 알았는데 다르다고 한다. https://sanghyu.tistory.com/3\nview: 원본 데이터와 데이터 공유 reshape: 원본 데이터와 데이터 공유될지 보장 못함. 될수도 안될수도 있다. squeeze, unsqueeze # 많이 사용했던 함수인데 제대로된 정의를 몰랐었다.\nsqueeze: 차원이 1인 차원을 제거 e.g., (2,1,2)면 (2,2)로 unsqueeze: 차원이 1인 차원을 추가, index 지정 가능하다. e.g., (2,3)에 unsqueeze(2)를 하면 2번째에 차원이 1인 차원을 추가해서 (2,3,1)이 된다. dot, matmul, mm # 이전 포스팅에서도 정리했던 내용이다.\n벡터간 내적일 때 사용 가능: dot, matmul, mm 다차원행렬간 내적일 때 사용 가능: matmul, mm mm: broadcasting 불가 matmul: broadcasting 지원 torch.index_select # https://pytorch.org/docs/stable/generated/torch.index_select.html\n이해가 어려웠는데 python의 list나 numpy에서 slicing하는 것을 tensor 단위로 한다고 생각하면 편하다.\ntorch.index_select(input, dim, index, *, out=None) → Tensor\ninput: 검색 대상이다. dim: axis, index의 기준을 설정한다. index: tensor를 입력으로 한다. 어떤 것을 찾을지 결정한다.\n\u0026gt;\u0026gt;\u0026gt; x = torch.randn(3, 4) \u0026gt;\u0026gt;\u0026gt; x tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-0.4664, 0.2647, -0.1228, -1.1068], [-1.1734, -0.6571, 0.7230, -0.6004]]) \u0026gt;\u0026gt;\u0026gt; indices = torch.tensor([0, 2]) \u0026gt;\u0026gt;\u0026gt; torch.index_select(x, 0, indices) tensor([[ 0.1427, 0.0231, -0.5414, -1.0009], [-1.1734, -0.6571, 0.7230, -0.6004]]) \u0026gt;\u0026gt;\u0026gt; torch.index_select(x, 1, indices) tensor([[ 0.1427, -0.5414], [-0.4664, -0.1228], [-1.1734, 0.7230]]) 가령 torch.index_select(x, 0, indices)는 x tensor에서 0차원을 기준으로 indices에 해당하는 것들을 찾고자 하는 것이다. 즉, 0번째 차원의 0번째, 2번째 요소들을 반환한다.\ntorch.tensor slicing # numpy랑 같다\ntorch.tensor, torch.Tensor # 두 함수 모두 tensor를 객체로 구현하기 위한 함수이다. 다만 차이점이 있다.\ntorch.tensor는 tensor 생성에 데이터가 필요하다. torch.tensor()는 오류다. torch.Tensor는 원형 클래스 자체이다. 즉, torch.empty()의 기능을 수행하기 위해 torch.Tensor()만을 부를수도 있다. torch.gather # https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather\ntorch.gather(input, dim, index, *, sparse_grad=False, out=None) → Tensor\ndim 차원 관점에서 input을 index만큼 slicing해준다. 사용하라면 사용 가능한데 남들처럼 깔끔하게 사용을 못하겠다.https://data-newbie.tistory.com/709\n나는 input과 동일한 tensor를 생성하고 해당 tensor에서 원하는 값의 위치를 적어주는 식으로 사용하고 있다.\nA = torch.Tensor([[[1, 2], [3, 4]], [[5, 6], [7, 8]]]) indicies = torch.tensor([[[0], [1]], [[0], [1]]]) output = torch.gather(A, 2, indicies).squeeze() torch.nn.Linear # 내가 아는 그 mlp의 한 layer이다. 이걸 아래처럼 사용하기도 하더라.\nX = torch.Tensor([[1, 2], [3, 4]]) forward = nn.Linear(2, 5) forward(X).shape 당연한거긴한데, 쉽게 생각이 안났다..\ntorch.nn.Identity # 말 그대로 입력과 동일한 tensor를 출력으로 내보내주는 layer다.\ntorch.nn.LazyLinear # https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html#torch.nn.LazyLinear\n내가 이해한게 맞다면 첫번째 forward에서는 torch.nn.UninitializedParameter에서 weight와 bias를 가져와서 연산한다.\n피어세션 때 알게 된 내용\ninput을 고정하지 않고 output channel만 정의 데이터의 크기를 가변적으로 받는데 사용하지는 않는다. 이미지의 채널이나 데이터 크기 외의 dimension을 결정하기 위해 사용한다. nn.Module.register_buffer # https://pytorch.org/docs/stable/generated/torch.nn.Module.html?highlight=register_buffer#torch.nn.Module.register_buffer\nhttps://discuss.pytorch.org/t/what-is-the-difference-between-register-buffer-and-register-parameter-of-nn-module/32723\nstate_dict로 모델을 저장할 때, 보통 네트워크의 w, bias들과 같은 parameter들만 저장된다. 가령, parameter외의 tensor를 저장하고 싶을 때는 register_buffer를 쓰면 된다. BN의 tensor를 어떻게 사용할건지 등, 사용할 곳이 있다고는 하더라.\n","date":"17 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-17-pytorch-%EA%B8%B0%EB%B3%B8-%EB%AC%B8%EB%B2%95/","section":"Posts","summary":"","title":"pytorch 기본 문법","type":"posts"},{"content":"한번 봤던 내용들인데 복습 차 필사했다.\nPrinciple of Proportion Ink # 실제 값과 그에 표현되는 잉크 양은 비례해야 한다.\n모든 시각화 자료에서 통용되는 원칙이다. 반드시 x축의 시작은 0이다. plot의 세로 비율을 통해 데이터의 차이를 나타내야 한다. ![](/assets/images/Bar plot/fe90cd8b-91de-49c6-bac0-766292955d7b-image.png)\n왼쪽은 0부터 시작하지 않았다. 따라서 표현되는 그래프의 비율이 실제 데이터의 비율을 반영하지 못한다. 즉, 불필요한 오해를 방지하기 위해서는 오른쪽 그림과 같은 그래프를 사용해야 한다.\nBar plot # 말 그대로 bar로 데이터를 표현. caetgory에 따른 수치값을 비교하기에 적합 막대의 방향에 따른 분류 # matplotlib에서의 표현\n.bar() : x축에 범주, y축에 데이터 .barh() : y축에 범주, x축에 값 범주가 많을 때 적합 여러 개의 범주 # Bar plot은 범주에 대한 값을 표현 1개의 feature에 대한 값만을 표현 여러 group를 보여주려면 다른 방법을 찾아봐야 한다. solution\n여러개의 plot 1개의 plot에 동시에 표현 쌓아서 겹쳐서 이웃에 배치해서 Stacked bar plot # ![](/assets/images/Bar plot/38e77e6d-742f-45ef-8e2f-305c05bb31dc-image.png)\n2개 이상의 그룹을 stack 가장 아래에 stack된 bar의 분포는 파악하기 쉽다. 그 위에 stack된 bar들은 분포를 알기 어렵다. matplotlib .bar()에서 bottom parameter로 쌓아준다. -.barh()에서 left parameter로 쌓아준다. Percentaged stacked bar chart # ![](/assets/images/Bar plot/f9755ff7-f0ca-40e6-8eb2-cc49add0ea39-image.png)\nstacked bar plot은 전체 분포를 알기 어려우니, percentage로 stacked bar chart를 바꿔본 형태이다.\nOverlapped bar plot # ![](/assets/images/Bar plot/2eb0f831-b391-487f-adb8-c3ff8e3b599d-image.png)\n2개 그룹만 비교하려면 좋음. 투명도(alpha)를 조절하여 비교. 색상의 명도, 채도에 따라서도 투명도의 효과가 달라지니 유념하자. 3개 그룹 이상을 비교하려면 별로다. 이럴 때는 area plot이 좋다. Grouped bar plot # 가장 효과적인 방법!! ![](/assets/images/Bar plot/9bfd23d5-dba3-493b-9c5b-202dea6a5008-image.png)\n그룹별 bar를 이웃되게 배치 matplotlib에서는 구현이 까다로워서 seaborn에서 다룬다. .set_xticks(), .setxticklabels()를 사용한다. 앞선 방법들 모두 그룹이 5~7개 일 때 효과적이다. 그룹이 더 많다면 다른 방법 필요.\n데이터 정렬 # 정렬을 필수다 가령, pandas에서는 sort_valeus(), sort_index() 시간순, 크기순, 범주의 순서, 범주의 값에 따라 적합하게 정렬해주자. 적절한 공간 활용 # matplotlib의 bar plot은 ax에 꽉 차서 답답하다 다음의 메서드로 적절히 조정하자 X/Y axis limit(.set_xlim(), .set_ylime()) Spines(.spines[spine].set_visible()) gap(width) legend(.legend()), 범례를 어디에 두느냐 margins(.margins()) ","date":"16 August 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-08-16-bar-plot/","section":"Posts","summary":"","title":"Bar plot","type":"posts"},{"content":" 문제 # 네트워크란 컴퓨터 상호 간에 정보를 교환할 수 있도록 연결된 형태를 의미합니다. 예를 들어, 컴퓨터 A와 컴퓨터 B가 직접적으로 연결되어있고, 컴퓨터 B와 컴퓨터 C가 직접적으로 연결되어 있을 때 컴퓨터 A와 컴퓨터 C도 간접적으로 연결되어 정보를 교환할 수 있습니다. 따라서 컴퓨터 A, B, C는 모두 같은 네트워크 상에 있다고 할 수 있습니다.\n컴퓨터의 개수 n, 연결에 대한 정보가 담긴 2차원 배열 computers가 매개변수로 주어질 때, 네트워크의 개수를 return 하도록 solution 함수를 작성하시오.\n제한사항 컴퓨터의 개수 n은 1 이상 200 이하인 자연수입니다. 각 컴퓨터는 0부터 n-1인 정수로 표현합니다. i번 컴퓨터와 j번 컴퓨터가 연결되어 있으면 computers[i][j]를 1로 표현합니다. computer[i][i]는 항상 1입니다.\n풀이 # def dfs(computers, v, visited): visited[v] = True for node in range(len(computers)): if visited[node] == False and computers[v][node] == 1: dfs(computers, node, visited) def solution(n, computers): answer = 0 visited = [False] * n for v in range(n): if not visited[v]: dfs(computers, v, visited) answer += 1 return answer 재귀함수로 computers array를 계속 파고드는 형태이므로 dfs이다. 원래는 computers의 값을 바꿔가면서 찾는걸 구상했었는데, 도저히 생각이 안났다..\n관건은 2차원으로 구성된 computers array라고 할지라도, 결국 n개의 vertex에 대한 탐색을 진행하는 것이다. 즉, 2차원 array에 현혹되지말고 n개의 vertex에 대해서만 탐색하자.\nn개의 vertext를 row로 삼아서 computers를 탐색한다.\n","date":"15 August 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-08-15-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%ACpython/","section":"Posts","summary":"","title":"[프로그래머스] 네트워크(python)","type":"posts"},{"content":"","date":"15 August 2021","externalUrl":null,"permalink":"/tags/programmers/","section":"Tags","summary":"","title":"Programmers","type":"tags"},{"content":" 문제 # n개의 음이 아닌 정수가 있습니다. 이 수를 적절히 더하거나 빼서 타겟 넘버를 만들려고 합니다. 예를 들어 [1, 1, 1, 1, 1]로 숫자 3을 만들려면 다음 다섯 방법을 쓸 수 있습니다.\n-1+1+1+1+1 = 3 +1-1+1+1+1 = 3 +1+1-1+1+1 = 3 +1+1+1-1+1 = 3 +1+1+1+1-1 = 3\n사용할 수 있는 숫자가 담긴 배열 numbers, 타겟 넘버 target이 매개변수로 주어질 때 숫자를 적절히 더하고 빼서 타겟 넘버를 만드는 방법의 수를 return 하도록 solution 함수를 작성해주세요.\n제한사항 주어지는 숫자의 개수는 2개 이상 20개 이하입니다. 각 숫자는 1 이상 50 이하인 자연수입니다. 타겟 넘버는 1 이상 1000 이하인 자연수입니다.\n풀이 # bfs, dfs로 풀 수 있는 문제라고 한다. 물론 난 몰라서 고민하다가 답지 찾아봤다. n개의 빈칸을 맞춰야되는 문제고, 사용 가능한 부호는 +,-니까 단순하게 풀면 $2^n$이다. 무조건 효율적인 알고리즘을 찾아야 하는 문제다.\ndfs # answer = 0 def DFS(idx, numbers, target, value): global answer N = len(numbers) if(idx== N and target == value): answer += 1 return if(idx == N): return DFS(idx+1,numbers,target,value+numbers[idx]) DFS(idx+1,numbers,target,value-numbers[idx]) def solution(numbers, target): global answer DFS(0,numbers,target,0) return answer 가장 이해가 가는 풀이였다.\n등식을 풀어나가는 과정을 재귀함수를 통해 구현했다. 등식에서 +, -를 결정하는 것을 재귀함수를 만들면서 value에 쌓았다.\n답을 찾는 조건은 모든 빈 칸을 value에 쌓았을 때, 그리고 value의 값이 target이 되어야 하는 순간이다. 또한 모든 빈 칸을 value에 누적했음에도 idx가 N과 같다면 답을 못 찾은 경우의 수다.\ndfs가 아니라 bfs같지만\u0026hellip; 암튼 이해가 젤 쉬었다.\nproduct # from itertools import product def solution(numbers, target): l = [(x, -x) for x in numbers] s = list(map(sum, product(*l))) return s.count(target) product는 DB에서 사용되던 join과 같은 작업을 리스트 사이에서 수행해준다. 이 때, list를 unpacking해서 줘야된다고 한다. 따라서 asterisk로 l을 넘겨준다. https://mingrammer.com/understanding-the-asterisk-of-python/\nnumbers의 각각의 요소를 +, - 부호를 붙여준 tuple을 가진 리스트를 생성한다. product로 l간의 모든 조합을 구한다. map을 통해 모든 조합들에 개별적으로 sum을 적용시킨다. 3번의 결과 중 target과 동일한 합을 가지는 것들의 숫자를 반환한다. 가장 깔끔한 코드다. 성능 보장이 어떻게 되는지는 모르겠지만, 문제의 제약을 감안하면 python 빌트인함수만으로도 성능이 보장되나보다.\nbfs # import collections def solution(numbers, target): answer = 0 stack = collections.deque([(0, 0)]) while stack: current_sum, num_idx = stack.popleft() if num_idx == len(numbers): if current_sum == target: answer += 1 else: number = numbers[num_idx] stack.append((current_sum+number, num_idx + 1)) stack.append((current_sum-number, num_idx + 1)) return answer 앞서 언급한 dfs 풀이와 비슷하다. current_sum을 업데이트해주고 재귀함수에 넘겨주고 체크하는 것을 반복한다.\n","date":"15 August 2021","externalUrl":null,"permalink":"/posts/algorithms/2021-08-15-%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-%ED%83%80%EA%B2%9F-%EB%84%98%EB%B2%84python/","section":"Posts","summary":"","title":"[프로그래머스] 타겟 넘버(python)","type":"posts"},{"content":" Latent Variable Models # D.Kingma가 만든 모델이라고 한다. Adam, varitaional auto-encoder도 만든 대단한 분이라고 한다\u0026hellip;\nAutoencoder는 generative model인가? variational auto-encoder는 generatiev model이니까 auto-encoder도 generative model인가? 그렇지 않다.\n즉, variational auto-encoder를 generative model로 만들어주는 이유가 있고 이것을 아는 것이 중요하다!\nVariational inference(VI) # The goal of VI is to optimize the variational distribution that best matches the posterior distribution.\nposterior distribution: $p_{\\theta}(z|x)$ observation이 주어졌을 때, 관심있어하는 random variable의 확률분포 z: latent vector 반대로 뒤집은걸 보통 likelihood라고 한다. $p(x|z)$ Varitaional distribution: $q_{\\phi}(z|x)$ posterior distribution은 구하기 불가능하거나 어려운 경우가 많다. posterior distribution에 근사하는 확률분포를 의미한다. KL divergence: 근사 방법 이를 사용해 true posterior와 variational distribution의 차이를 최소화하겠다. ![](/assets/images/Generative model - 2/2dfa4a4e-fdc2-46fc-a9d9-9f1ae935fd95-image.png)\nEncoder: Variational distribution을 학습하는 곳 How to find objective? # gradient descent에서 true y를 알아야 loss function의 값을 계산할 수 있듯이, Variational inference도 posterior distribution을 알아야 variational distribution을 근사할 수 있을 것이다.\n문제는 variational distribution을 구하는 이유가 posterior distribution을 구하기 어렵기 때문이라는 것이다. 모순이 발생한다.\n![](/assets/images/Generative model - 2/be4bc4a1-f73c-4432-a993-d9f416c4763d-image.png)\n이를 수식으로 설명하면 위 수식이 된다고 한다. 학부 수준의 통계학을 들었다면 이해 가능한 수식이라고 한다. (\u0026hellip;)\n수식에서 Objective를 줄이는 것이 곧 posterior와 varitional의 차이를 줄이는 것이다. 하지만 실제 posterior를 모르기 때문에, 앞 항인 ELBO(Evidence Lower bound)를 늘려줘서 반대급부로 objective가 줄어드는 것을 유도한다고 한다.\n이러한 방법 자체를 sandwitch method라고 부르기도 한다고 하더라.\nKL divergence를 모르기 때문에 ELBO를 키우는 방법을 활용해 variational inference는 학습을 진행한다.\nELBO # ![](/assets/images/Generative model - 2/cacd1361-d2a5-4c38-b8c4-5ab06d57df5a-image.png) ELBO는 위처럼 다시 풀어쓸 수 있고, 이는 계산 가능하다.\nReconstruction Term: Auto-encoder의 reconstruction loss term Prior Fitting Term: Latent Prior Term? 정리 # Variational Inference의 궁극적인 목표는 입력 X에 대해서, X를 잘 표현하는 latent space(잠재변수 공간?)인 Z를 찾고 싶은 것이다. 하지만 posterior distribution인 $p_{\\theta}(z|x)$를 모른다. 그래서 posterior distribution을 찾기 위해 variational distribution 혹은 encoder로 posterior distribution을 근사하고자 한다.\nposteriror distribution을 모르는 상태에서 KL divergenec를 통해 추정치와 실제값의 거리를 구할 수는 없다! 따라서 Variational inference라는 기법을 사용해 ELBO를 Maximize하면 KL divergenec를 줄여서 추정치와 실제값의 거리를 줄여주는 효과를 유도한다.\nELBO는 Reconstruction term과 Prior fitting term으로 나뉜다.\nReconstruction term\nX라는 입력을 encoder를 통해 latent space로 보낸다. 이를 다시 decoder로 보냈을 때 발생하는 reconstruction loss를 줄이는 것이 reconstruction term Prior fitting term\nX라는 입력을 latent space로 올려놨다고 해보자. 올라간 데이터들의 분포가 latent space의 prior distribution과 얼마나 차이가 있는지를 나타내는 term이다. 따라서, generative model이고 explicit model아닌 implicit model이다.\nVariational Auto-Encoder(VAE) # 입력 X가 주어지고, 이를 latent space로 보내서 무언가를 찾고 이를 통해 reconstruction term으로 만든다.\nGenerative model이 되기 위해서는 latent space의 prior distribution인 z를 샘플링하고 이를 Decoder에 통과시켜 나오는 output을 generation result라고 보는 것이다.\n하지만 auto encoder는 이렇나 과정이 없다. 그냥 input이 latent space로 갔다가 output으로 나온다. 그래서 엄밀한 의미에서 auto-encoder는 generative model이 아니다.\nVA의 특징은 아래와 같다.\nIntractable model이다. likelihood를 계산하는 것이 어렵다. implicit model이다. Prior fitting term은 계산하기 매우 어렵다. 미분했을 때 무언가 값을 얻기 쉽도록 isotropic Gaussian을 사용한다. Adversarial Auto-encoder(AAE) # VAE의 단점은 prior fitting term에서 gaussian이 아닌 다른 것을 활용하기가 힘들다. 하지만 많은 경우에, prior distribution으로 gaussian을 활용하고 싶지 않을 때가 많다.\n![](/assets/images/Generative model - 2/7ffffc51-6fe7-43c9-9bc5-732d7bd8f99c-image.png)\n이를 위한 해결법으로 AAE가 있다. GAN을 사용해서 latent distribution의 분포를 맞춰준다고 한다. 즉, VAE의 prior fitting term을 GAN으로 바꾼 것이다.\nlatent distribution에 샘플링만 가능한 어떤 분포만 있어도, prior fitting에서 사용할 수 있다. e.g., uniform distribution, 혹은 복잡한 distribution도 가능.\nVAE보다 성능이 더 좋을때도 많다고 한다. 물론 항상 좋진 않다.\nwasserstein autoencoder라는 논문이 2018년에 나왔는데, 사실 AAE는 latent space 사이의 wasserstein distribution을 줄여주는 것과 동일한 효과임을 수식으로 증명했다고 한다. 따라서 AAE도 wasserstein autoencoder의 한 종류라고 봐도 된다고 한다.\nGenerative Adversarial Network(GAN) # ![](/assets/images/Generative model - 2/82bba9f4-c2e2-4470-821a-449a286f669c-image.png)\nGenerator는 위조지폐를 만들고 Discriminator(구별자?)는 위조지폐를 감별한다고 해보자.\nDiscriminator는 본인이 알고있는 지식과 generator의 결과를 바탕으로 더욱 위조지폐를 잘 구분하려고 학습한다. 만약 Fix된 discirminator라면 서로 학습이 잘 안될텐데, 이것이 매우 큰 장점이다.\nGenerator는 Discriminator를 더 잘 속일 수 있도록 학습한다.\nGAN의 목표는 Generator의 성능을 향상시키는 것이다. Implicit model이다.\nVAE vs GAN # ![](/assets/images/Generative model - 2/1afdb9a4-0def-4958-a98d-3cf508ea0a71-image.png) VAE\n학습 x라는 입력을 encoder를 통과시켜 z라는 latent vector로 만든다. decoder를 통해 다시 x라는 도메인으로 출력. generation p(z)에서 z를 샘플링한다. z를 decoder를 통과시켜 원하는 결과를 generate. GAN\nz라는 latent distribution을 입력. z는 G를 통과해 Fake를 생성. Discriminator는 Real, Fake를 구분하는 Classifier를 학습. Generator는 Discriminator가 True를 출력하도록 학습. Discriminator는 Real, Fake를 더 잘 구분하도록 재학습. GAN Objective # A two player minimax game between generator and discriminator.\nDiscriminator # 앞서 본 GAN의 수식을 Discriminator에 대해서만 보면 아래와 같다. ![](/assets/images/Generative model - 2/1e608f4e-3813-45df-893b-8407adc25632-image.png)\n이 때, optimal discriminator는 아래와 같다. ![](/assets/images/Generative model - 2/35117cdb-1d73-4ee2-84a1-9c02c1bece1c-image.png) Generator를 fix했을 때의 optimal한 형태라고 한다. 해당 값이 높으면 true, 낮으면 false 같은 형식이라고 한다.\nGenerator # ![](/assets/images/Generative model - 2/9f376be0-376a-4985-87c9-7befee5bd62a-image.png) 위에서 얻은 optimal discriminator를 generator에 대해서만 풀어쓴 GAN 수식에 대입해보자.\n그러면 아래와 같이 Jenson-Shannon Divergence(JSD)가 유도된다고 한다. ![](/assets/images/Generative model - 2/bf78062d-73b8-417b-99a6-c3b30c43eb1c-image.png)\n즉, 실제 true data와 generator가 생성한 data간의 거리를 재는데 Jenson-Shannon divergence가 사용된다고 할 수 있다.\n하지만, 이는 optimal discriminator가 보장된 상태에서만 유도 가능한 결과이다. 즉, 이론적으로는 맞지만 실제로는 사용하기 어렵다.\nDCGAN # 처음의 GAN은 MLP로 만들었다. 이를 이미지 도메인을 위해 만든 것이 DCGAN이다. ![](/assets/images/Generative model - 2/577497c3-7219-4bd1-a83e-addb75e5e0ac-image.png)\nEncoder에서는 deconvolution을 사용했고 discriminator에서는 convolution을 사용했다.\n알고리즘적으로 향상된 부분은 없지만, 여러 테크닉들을 사용했다고 한다. 에러 종류도 바꿔보고 hyper parameter 세팅 등..\nInfo-GAN # ![](/assets/images/Generative model - 2/4e31bc73-8b5c-434e-ad2c-e984646651f9-image.png) z라는 입력을 넣을 때, c라는 클래스를 나타내는 원핫벡터를 넣는다. 이는 Generation을 할 때, GAN이 c라는 벡터를 활용해서 특정 모드에만 집중할 수 있도록 해준다.\n따라서, multi modal distribution을 학습하게 되는 현상을 c 벡터를 통해 잡아주는 효과가 생긴다.\nText2Image # ![](/assets/images/Generative model - 2/50bd34dc-60be-46bb-b678-1b968825daaf-image.png)\n문장이 주어지면 그림을 만들어준다고 한다. openai의 DALL-E 연구가 이 논문을 바탕으로 시작됐다고 한다.\nPuzzle-GAN # ![](/assets/images/Generative model - 2/1aedd9ee-9e26-451c-a3ea-f9321b015274-image.png)\n교수님께서 저자로 참여하신 논문이라고 한다. \u0026hellip; 이미지의 sub patch를 넣어주면 전체 이미지를 복원하는 모델이라고 한다.\nCycleGAN # ![](/assets/images/Generative model - 2/78622e5e-5b90-4303-8564-014e2e9aa7cb-image.png)\n이미지 사이의 도메인을 바꿔주는 모델이다. 가령, 그림처럼 zebra를 horse로 바꾸는 것이다.\nCycle-consistency loss # ![](/assets/images/Generative model - 2/91a7f1a2-d62e-4882-8b15-9c7f290b7440-image.png)\n일반적으로 이미지의 도메인을 바꾸기 위해서는 두 개의 이미지가 필요하다. 가령, 얼룩말 사진을 말 사진으로 바꾸고 싶다면, 동일한 곳에서 찍은 얼룩말과 말의 이미지가 필요하다.\n하지만 cycleGAN은 그냥 만든다. 즉, 야생의 말 사진과 야생의 얼룩말 사진들을 잔뜩 학습하면 위의 과정을 알아서 만들어준다.\nStar-GAN # ![](/assets/images/Generative model - 2/ac3adf3e-9fad-497f-95ca-97bbe5b3cf82-image.png)\n한국인 학생분께서 쓰신 논문이라고 한다. 이미지에 단순히 도메인 변화를 주는 것이 아니라 도메인에 대한 컨트롤을 할 수 있는 방법론이라고 한다. 인용도 엄청 많이 됐다.\nProgresive-GAN # undefined ![](/assets/images/Generative model - 2/c8163a52-6c4a-4b71-81fd-dff3545ba04d-image.png)\n처음부터 고차원의 이미지를 학습하지않고, 4x4이미지의 차원을 늘려가면서 1024x1024까지 점진적으로 학습했다고 한다. 이것이 성능향상에 큰 기여를 했다고 한다.\n정리 # ![](/assets/images/Generative model - 2/d795046d-704b-49cc-bfe0-7f46d51ee8d1-image.png)\nGAN 논문의 수를 나타낸 그래프다. 2018년에만 500개라고 하니, 모든 GAN에 대해 다 아는 것을 불가능하다. openai의 DALL-E 연구를 보면 GAN을 쓰는 것이 아니라 transformer를 쓰는 것이 제일 좋을 수도 있겠다고 하셨다.\n중요한 것은 알아가는 것이다.\n","date":"14 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-14-generative-model---2/","section":"Posts","summary":"","title":"Generative model - 2","type":"posts"},{"content":"","date":"14 August 2021","externalUrl":null,"permalink":"/tags/lataent-variable-model/","section":"Tags","summary":"","title":"Lataent-Variable-Model","type":"tags"},{"content":"https://deepgenerativemodels.github.io/ 스탠포드 대학의 수업이라고하는데 참고해서 수업을 진행하셨다.\nGenerative model # 단순히 이미지와 문자를 만드는 것이 아니다.\n![](/assets/images/Generative Models/0f8e765f-c03c-4ef0-a45b-2717e4b2bf37-image.png)\n강아지 이미지들을 받았다고 해보자.\nGenerative model에 probability distribution $$p(x)$$를 학습할 것을 기대할 수 있다.\nGeneration: $$x_{new} \\sim p(x)$$를 샘플링 했을 때, $$x_{new}$$는 개처럼 보여야한다. Density estimation: $$p(x)$$를 사용해서 임의의 입력 $$x$$에 대해서 개인지, 혹은 개가 아닌지, 고양이인지 등의 판단이 가능하다. (anomaly detection, 이상행동 감지) 엄밀한 의미에서 Generative model은 Descriminator model을 포함하고 있다. 확률값을 얻을 수 있는 모델을 _explicit model_이라고 한다. Unsupervised representation learning(feature learning): feature를 unsupervised 방식으로 학습하는 것 교수님은 의아하다고 하지만 스탠포드 대학 강의에서는 이 또한 generative model이 지향하는 것이라고 했다고 한다. Basic Discrete Distributions # 시작하기 전에 알아둬야할 간단한 수학적 지식이다. 앞서 임성빈 교수님 수업에서도 말씀해주셨던 내용이지만 복습하는 의미로 적는다.\nBernoulli distribution # Bernoulli에는 1개의 paramter만 필요하다.\nD = {Heads, Tail} P(X=Heads) = p, then P(X=Tails) = 1 - 0 Write: X ~ Ber(p) Categorical distribution # Categorical에는 m-1개의 parameter가 필요하다. m-1개의 요소들을 안다면, 나머지 1개의 요소는 자동적으로 결정되기 때문이다.\nD = {1, \u0026hellip;, m} P(Y=i) = $$p_i$$, such that $\\sum_{i=1}^m p_i$ = 1 Write: Y ~ Cat(p1, \u0026hellip;, pm) RGB # ![](/assets/images/Generative Models/63e385e6-f325-4ec2-afcf-9b9fcd77925e-image.png)\n$(r, g, b) \\sim p(R, G, B)$ number of cases = 256 x 256 x 256 number of parameters = 256 x 256 x 256 - 1 1개의 rgb 픽셀을 표현하기 위한 parameter의 수는 매우 많다! 당연한 이야기지만.. Binary image # ![](/assets/images/Generative Models/1daa072d-8af6-407d-a896-ae7f1968ea23-image.png)\nn pixel의 binary image를 가정해보자. $2^n$ state가 필요하다. Sampling from $p(x_1, \u0026hellip;, x_n)$ generate an image. $p(x_1, \u0026hellip;, x_n)$를 샘플링하기 위해서는 $2^n - 1$의 parameters가 필요하다. 즉, parameter의 수가 너무 많다. 줄여볼 수 없을까?\nStructure through independence # Binary image에서 $X_1, \u0026hellip;, X_n$이 independent하다고 가정해보자. 사실 말이 안된다. 모든 픽셀이 independent하다면 표현할 수 있는 이미지는 화이트 노이즈일 뿐일 것이다. 하지만 그래도 가정해보자.\n$p(x_1, \u0026hellip;, x_n) = p(x_1)p(x_2)\u0026hellip;p(x_n)$\n이 때, possible state의 수는 동일하게 $2^n$이다.\n하지만 $p(x_1, \u0026hellip;, x_n)$를 위한 parameter의 수는 n개이다. 왜냐하면 각각의 픽셀에 대해서 필요한 parameter의 수는 1개이다. 또한 모두 independent하기 때문에, 모두 더하면 n이다.\nChain rule # ![](/assets/images/Generative Models/b07552e7-d5e4-4cd6-988e-2f1d85cd0c99-image.png)\n그 어떤 가정도 필요없는 정리이다. 즉, 기본적인 출발선에서 시작하기 때문에 fully dependent model이라고 생각하자.\n모든 parameter의 수는 $2^n -1 $이다. exponential reduction을 했다!\nBayes\u0026rsquo; rule # ![](/assets/images/Generative Models/7a57ce2b-cbd0-4d91-9b25-2d41da442046-image.png)\nConditional independence # ![](/assets/images/Generative Models/ee99d572-1499-4f76-9c52-cb92e8965eee-image.png) x and y are conditional independent given z, p of x given y and z는 p of x given z와 같다. 라고 영어로 말하시더라.\nz가 주어지고 x와 y가 indepedent하다면, random한 x를 볼 때 y는 상관없다는 것이다.\n즉, chain rule이나 혹은 다른 수식에서 independent한 관계인 변수들이 있다면 조건부에서 날려주는 역할을 하는 정리이다. 이 정리를 사용해서 fully depedent model과 fully independent model 사이의 좋은 모델을 만들 것이다.\nMarkov assumption # chain rule에 Markov assumption을 적용해보자. RNN에 나왔던 가정인거 같은데, 현재 상태를 바로 이전의 상태만을 활용해서 정의하는 것이다. 즉, chain rule에서 이전의 모든 정보를 활용하는 항들이, n시점에서는 n-1만을 참조하는 항들로 바뀐다. ![](/assets/images/Generative Models/da0176cf-190b-48bb-bd3b-493be8dfcc58-image.png)\n모든 parameter의 수는 $2n-1$이다.\nfully independent하게 계산했던 parameter의 수인 n과 비교하면 크지만, dependent하게 계산했던 chain rule인 $2^n-1$에 비하면 exponential reduction하다.\n즉, 이러한 형태로 중간의 sweet spot을 찾는 것이 auto-regressive model.\nAuto-regressive model # ![](/assets/images/Generative Models/62f2297a-0040-4490-b2d0-a82b1b6c6f84-image.png)\n28x28의 binary image를 사용한다고 가정. $p(x) = p(x_1, \u0026hellip;, x_785)$를 $x\\in{0,1}^{784}$에서 학습하는 것. $p(x)$를 어떻게 parametrize할 것인가? chain rule을 사용해 joint distribution을 나눈다. 이것을 _autoregressive model_이라고 한다. markov assumption처럼 바로 이전의 정보만을 활용하는 것도 autoregressive model이다. 모든 random variables에 대해 순서를 부여해야한다. 순서에 따라 성능이 달라지기도 한다. 이전 1개만 고려하는 모델 = AR(1) model 이전 n개를 고려하는 모델 = AR(N) model\nNADE # Neural autoregressive density estimator ![](/assets/images/Generative Models/a97f82ee-7046-49e2-85d4-391c5a12f469-image.png)\n![](/assets/images/Generative Models/a3890504-a4f3-4ee2-b285-a9150e347893-image.png)\ni번째 픽셀을 첫번째 ~ i-1번째 픽셀에 dependent하게 구성.\n첫번째 픽셀에 대한 확률분포를 어느 것에도 dependent하지 않게 구성. 두번째 픽셀에 대한 확률분포는 첫번째 픽셀에만 dependent하게 구성. 다섯번째 픽셀에 대한 확률분포는 첫번째 ~ 네번째 픽셀에만 dependent하게 구성. i번째 픽셀은 i-1개의 픽셀에 dependent하다. 입력차원이 달라지면서 weight는 게속 커진다. i번째 입력은 i-1개의 입력을 받을 수 있는 weight가 필요하니까. 마지막 layer에 \u0026lsquo;a mixture of Gaussian\u0026rsquo;을 써서 continuous random variables를 만들 수도 있다. NADE는 explicit model이다. chain rule처럼 확률이 계산되기 때문에, 어떻게든 확률을 계산할 수 있다.\ninplicit model은 generation만 할 수 있다.\nPixel RNN # RNN을 auto-regressive하게 만들자.\nn x n RGB image에 대한 수식은 다음과 같다. ![](/assets/images/Generative Models/6b6072b9-6fe8-4dc3-a816-9589dbaac990-image.png)\nOrdering에 따라서 두가지로 나뉜다. ![](/assets/images/Generative Models/66d30707-badf-4fa0-ad24-4d6bdea3e71a-image.png)\nRow LSTM 위 쪽의 정보들을 활용 Diagonal BiLSTM 이전의 모든 정보들을 활용 ","date":"13 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-13-generative-models/","section":"Posts","summary":"","title":"Generative Models","type":"posts"},{"content":"","date":"13 August 2021","externalUrl":null,"permalink":"/tags/generative-models/","section":"Tags","summary":"","title":"Generative-Models","type":"tags"},{"content":"","date":"13 August 2021","externalUrl":null,"permalink":"/tags/transformer/","section":"Tags","summary":"","title":"Transformer","type":"tags"},{"content":" SDPA # Scaled Dot-Product Attention. transformer를 배우면서 attention이 1개만 존재했던 그 모델이다. 위 그림처럼 여러 개의 Q, K, V를 생성하는 것이 MHA이고 한 쌍의 Q, K, V만 생성하면 SDPA.\n![](/assets/images/Transformer 실습/2caaf910-d820-4ce6-a286-b6497e5928e9-image.png)\nclass ScaledDotProductAttention(nn.Module): \u0026#34;\u0026#34;\u0026#34; Attention이 1개인 구조. 입력: embedding 결과인 n차원의 벡터 query, key, value를 찾고 attention 연산을 위의 수식과 같이 수행한다. 출력: n개의 word vector에 대해서 value vector만큼의 dimension을 가진 tensor \u0026#34;\u0026#34;\u0026#34; def forward(self, Q, K, V, mask=None): d_K = K.size()[-1] # key dimension scores = Q.matmul(K.transpose(-2, -1)) / np.sqrt(d_K) if mask is not None: scores = scores.masked_fill(mask==0, -1e9) attention = F.softmax(scores,dim=-1) out = attention.matmul(V) return out,attention # Demo run of scaled dot product attention SPDA = ScaledDotProductAttention() \u0026#34;\u0026#34;\u0026#34; n_batch: n_batch개의 단어가 있음을 의미 d_K: key vector의 dimension d_V: Value vector의 dimension n_Q: Query vector의 개수 n_K: Key vector의 개수 n_V: Value vector의 개수 \u0026#34;\u0026#34;\u0026#34; n_batch, d_K, d_V = 3, 128, 256 # d_K(=d_Q) does not necessarily be equal to d_V n_Q, n_K, n_V = 30,50,50 Q = torch.rand(n_batch,n_Q,d_K) K = torch.rand(n_batch,n_K,d_K) V = torch.rand(n_batch,n_V,d_V) out,attention = SPDA.forward(Q,K,V,mask=None) def sh(x): return str(x.shape)[11:-1] print (\u0026#34;SDPA: Q%s K%s V%s =\u0026gt; out%s attention%s\u0026#34;% (sh(Q),sh(K),sh(V),sh(out),sh(attention))) 수식에서도 나와있듯이, query와 key의 차원은 $$\\mathbb{R}^{n\\times d_K}$$이다. 즉, query와 key는 동일 차원이어야만 연산이 가능하다.\nvalue의 차원은 $$\\mathbb{R}^{n\\times d_V}$$이지만 편의상 query, key와 동일하게 구현한다. 즉, 같아도 무방하다.\nQ, K, V의 개수 # Encoder, decoder가 존재한다면 # 코드를 보면 SPDA에 대한 vector를 생성할 때 다음과 같이 생성했다.\nn_Q $$\\neq$$ ( n_K = n_V)\nencoder에서 V와 K에 대한 정보를 받고 decoder는 decoder에 들어오는 입력으로 Q를 생성하기 때문에 이렇게 수가 달라도 된다.\nencoder, decoder를 상정한 것이기 때문에, 이것이 더 일반적이다.\nSPDA의 목적이 여기서 나온다! Query vector에 대한 encoding을 하고 싶은데, key와 value vector를 참고해서 만드는 것이다.\n즉, SPDA의 출력 vector 또한 Query vector와 개수가 같아야 한다.\nSelf-attention이라면 # n_Q = n_V = n_K\n모두 같아야한다.\nK.transpose(-2, -1) # pytorch의 tensor에 대해서는 위와 같이 transpose가 가능하더라. 즉, 두 개의 인자로 들어온 차원에 대해서 서로 바꿔준다. 위에서는 마지막에서 첫번째 차원과 마지막에서 두번째 차원을 서로 바꿔주는 것이라고 이해할 수 있다.\ntorch.nn.Softmax() # dim = -1의 의미를 몰라서 docs를 찾아보니 다음과 같다.\ndim (int) – A dimension along which Softmax will be computed (so every slice along dim will sum to 1).\nhttps://stackoverflow.com/questions/49036993/pytorch-softmax-what-dimension-to-use\n해당 차원에 대해서 softmax를 계산한다는 것 같다.\n![](/assets/images/Transformer 실습/4a19b3f9-5b06-497f-84dd-b90f8839e484-image.png)\nsoftmax의 정의는 위와 같은데 여기서 $$x_j$$의 j를 dim 옵션을 통해 지정한다.\nSDPA를 위해 구현한 코드가 MHA에서도 작동하는 이유 # Batch the multiplication이기 때문에 된다고 하셨다. 정확히 뭐라고하신건지는 모르겠다..\n내 의견은 어차피 matrix 연산을 통해 SDPA가 구현됐기 때문에 Q, K, V가 몇차원이든 상관없이 연산이 가능하도록 차원의 수만 맞추면 된다는 뜻인 것 같다.\nMHA(Multi-head attention) # class MultiHeadedAttention(nn.Module): def __init__(self,d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=None): \u0026#34;\u0026#34;\u0026#34; :param d_feat: feature dimension :param n_head: number of heads :param actv: activation after each linear layer :param USE_BIAS: whether to use bias :param dropout_p: dropout rate :device: which device to use (e.g., cuda:0) \u0026#34;\u0026#34;\u0026#34; super(MultiHeadedAttention,self).__init__() if (d_feat%n_head) != 0: raise ValueError(\u0026#34;d_feat(%d) should be divisible by b_head(%d)\u0026#34;%(d_feat,n_head)) self.d_feat = d_feat self.n_head = n_head self.d_head = self.d_feat // self.n_head self.actv = actv self.USE_BIAS = USE_BIAS self.dropout_p = dropout_p # prob. of zeroed self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS) self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS) self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS) self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS) self.dropout = nn.Dropout(p=self.dropout_p) def forward(self,Q,K,V,mask=None): \u0026#34;\u0026#34;\u0026#34; :param Q: [n_batch, n_Q, d_feat] :param K: [n_batch, n_K, d_feat] :param V: [n_batch, n_V, d_feat] \u0026lt;= n_K and n_V must be the same :param mask: \u0026#34;\u0026#34;\u0026#34; n_batch = Q.shape[0] Q_feat = self.lin_Q(Q) K_feat = self.lin_K(K) V_feat = self.lin_V(V) # Q_feat: [n_batch, n_Q, d_feat] # K_feat: [n_batch, n_K, d_feat] # V_feat: [n_batch, n_V, d_feat] # Multi-head split of Q, K, and V (d_feat = n_head*d_head) \u0026#34;\u0026#34;\u0026#34; Q, K, V를 분할해준다. 가령 (100,)이라고 하면 (10,10)으로 만들어준다. 여기서는 d_feat을 n_head개로 분할해 d_head 차원으로 만들었다. \u0026#34;\u0026#34;\u0026#34; Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3) K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3) V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3) # Q_split: [n_batch, n_head, n_Q, d_head] # K_split: [n_batch, n_head, n_K, d_head] # V_split: [n_batch, n_head, n_V, d_head] # Multi-Headed Attention d_K = K.size()[-1] # key dimension scores = torch.matmul(Q_split, K_split.permute(0, 1, 3, 2)) / np.sqrt(d_K) if mask is not None: scores = scores.masked_fill(mask==0,-1e9) attention = torch.softmax(scores,dim=-1) x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper # attention: [n_batch, n_head, n_Q, n_K] # x_raw: [n_batch, n_head, n_Q, d_head] # Reshape x x_rsh1 = x_raw.permute(0,2,1,3).contiguous() # x_rsh1: [n_batch, n_Q, n_head, d_head] \u0026#34;\u0026#34;\u0026#34; n_head개로 d_head차원만큼 분할됐던 tensor를 합쳐준다. n_head * d_head = d_feat니까 바로 d_feat를 사용한다. \u0026#34;\u0026#34;\u0026#34; x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat) # x_rsh2: [n_batch, n_Q, d_feat] # Linear x = self.lin_O(x_rsh2) # x: [n_batch, n_Q, d_feat] out = {\u0026#39;Q_feat\u0026#39;:Q_feat,\u0026#39;K_feat\u0026#39;:K_feat,\u0026#39;V_feat\u0026#39;:V_feat, \u0026#39;Q_split\u0026#39;:Q_split,\u0026#39;K_split\u0026#39;:K_split,\u0026#39;V_split\u0026#39;:V_split, \u0026#39;scores\u0026#39;:scores,\u0026#39;attention\u0026#39;:attention, \u0026#39;x_raw\u0026#39;:x_raw,\u0026#39;x_rsh1\u0026#39;:x_rsh1,\u0026#39;x_rsh2\u0026#39;:x_rsh2,\u0026#39;x\u0026#39;:x} return out # Self-Attention Layer \u0026#34;\u0026#34;\u0026#34; n_batch: 학습 데이터 중에서 128개씩 batch 뜯어오겠다. n_src: n_src만큼의 word가 들어간다. = n_src만큼의 sequence를 한꺼번에 처리하겠다. d_feat: feature의 차원 n_head: multi-head attention을 몇개로 할건지 \u0026#34;\u0026#34;\u0026#34; n_batch = 128 n_src = 32 d_feat = 200 n_head = 5 src = torch.rand(n_batch,n_src,d_feat) self_attention = MultiHeadedAttention( d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device) # self attention이니까 모두 같은 차원으로 Q, K, V가 구성된다. out = self_attention.forward(src,src,src,mask=None) Q_feat,K_feat,V_feat = out[\u0026#39;Q_feat\u0026#39;],out[\u0026#39;K_feat\u0026#39;],out[\u0026#39;V_feat\u0026#39;] Q_split,K_split,V_split = out[\u0026#39;Q_split\u0026#39;],out[\u0026#39;K_split\u0026#39;],out[\u0026#39;V_split\u0026#39;] scores,attention = out[\u0026#39;scores\u0026#39;],out[\u0026#39;attention\u0026#39;] x_raw,x_rsh1,x_rsh2,x = out[\u0026#39;x_raw\u0026#39;],out[\u0026#39;x_rsh1\u0026#39;],out[\u0026#39;x_rsh2\u0026#39;],out[\u0026#39;x\u0026#39;] $$head_{\\color{red}i} = \\text{Attention}(Q {\\color{green}W}^Q_{\\color{red}i},K {\\color{green}W}^K_{\\color{red}i}, V {\\color{green}W}^V_{\\color{red}i})$$\n논문에는 dropout이 없다. 하지만 실제로는 dropout을 모든 attention에서 사용하기 때문에 사용했다. 원래의 MHA는 k개의 header를 여러개 만들고 나중에 이 결과들을 aggregate한다. 실제 구현은 미리 k개로 분할하고 Scaled Dot Production을 진행한다. 따라서, d_feat은 n_head로 분할 가능해야 한다. torch.Tensor.permute # transpose와 동일한 기능이다. 다만, transpose는 두개의 차원에 대해서만 치환이 되고 permute는 모든 차원에 대해서 가능하다.\n결론 # 약간 헷갈릴 수 있는데 정리하면 아래와 같다.\nn_Q $$\\neq$$ ( n_K = n_V) d_Q = d_K 1번이 성립하는 이유\nKey와 Value는 encoder에서 넘어오는 정보 Query는 decoder에 입력으로 받는 정보 2번이 성립하는 이유\nattention을 계산할 때, Query와 Key를 inner product 해야하기 때문 Value의 차원은 두 vector와 달라도 되고 같아도 무방 ","date":"13 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-13-transformer-%EC%8B%A4%EC%8A%B5/","section":"Posts","summary":"","title":"Transformer 실습","type":"posts"},{"content":" Background # 기존의 rnn들도 sequence data들을 다룰 수 있지만, 위와 같이 원본 데이터에서 일부 데이터가 빠진 sequence data들에 대해서 다루기는 매우 어려웠다.\n이를 다루기 위해 transformer가 등장했다.\nTransformer # RNN처럼 재귀적인 구조가 없다.\nTranasformer is the first sequence transduction model based entirely on attention.\n본래는 위와 같이 기계어 번역을 위한 모델이었다. 하지만 transformer는 sequential data를처리하고 이를 encoding하는 방법론이기 때문에, 기계어 번역 외에서도 사용할 수 있다.\n최근에는 transformer, self atttention이 거의 모든 분야에서 사용되고 있다.\ntransformer는 위와 같이, sequence to sequence model이다. 좀 더 자세하게 뜯어보자.\nRNN과 다르게 재귀적으로 돌지 않는 차이점이 여기서 나타난다. RNN이라면 3개의 단어가 입력일 때, 재귀적으로 3번 돌면서 결과를 출력할 것이다.\n하지만 transformer는 3개의 단어든, 100개의 단어든 한번의 encoding 과정을 통해 encoding vector를 한번에 만들어낸다. 출력을 위해서는 재귀적으로 무언가를 한다고 한다.\nKey concept of transformer # n개의 단어가 encoding에서 어떻게 한번에 처리 되는지? encoder과 decoder 사이에서 어떤 정보가 오가는지? decoder가 어떻게 generation할 수 있는지? Encoder # 입력으로 모든 벡터를 받는다. Self-attention이 encoder과 decoder에서 중요한 역할을 한다. 뒤의 Feed forward nn은 우리가 아는 흔한 mlp이다.\nself-attention은 n개의 벡터를 받는다. 입력 벡터 $$x_1$$을 $$z_1$$로 변환하기 위해 모든 $$x$$를 활용한다. $$z$$ 벡터를 만들기 위한 모든 paths들은 서로 종속적이다. $$z$$를 feed forward nn을 통과시킬 때는, 병렬적으로 비종속적으로 통과시킨다. Self-attention # 아래 문장을 분석하고자 한다면 아래와 같은 종속적인 네트워크를 구성해준다.\nThe animal didn\u0026rsquo;t cross the street because it was too tired.\n인간은 당연하게 it이 animal을 나타낸다고 생각한다. 이를 self-attention을 통해 학습하면 그림과 같이 animal 부근에서 강한 종속성을 나타낸다.\nQuery, Key, Value 벡터들은 단어별로 계산된다. (=embedding) = 한번의 embedding으로 query, key, value가 1개씩 계산된다.\nEncoder 계산 과정 # 강의를 들어보니 말로 설명하면 정말 어렵고, 수식으로 설명하면 매우 간단했다.\n말로 정리하면 다음과 같다.\nscore = query와 key를 내적(inner product) $$d_k$$ = key vector의 차원 softmax result = score를 $$d_k$$로 나눈 값에 softmax를 적용 sum = softmax 결과 x value 이를 행렬과 수식을 통해 설명하면 아래와 같다.\n입력 X를 행렬로 표현한다.\nrow = nubmer of words column = embedding dimension query, key, value에 대한 각각의 weight 행렬을 X와 곱해서 query, key, value를 계산한다.\nattention dimension = key vector의 dimension 나머지는 말로 설명한 부분을 그대로 수식으로 옮긴 것이다.\nsoftmax = row-wise softmax dim(V)는 dim(Q), dim(K)와 달라도 된다. 구현상 편의를 위해 보통은 모두 같게 만든다. Transformer의 특징 # mlp, cnn은 입력이 고정된다면 출력도 반드시 고정된다.\n하지만 transformer는 하나의 입력이 고정되더라도, 다른 입력들이 달라진다면 출력이 달라질 여지가 있다. =\u0026gt; 훨씬 더 많은 것을 표현할 수 있다. =\u0026gt; 더 많은 계산이 필요하다. -\u0026gt; 입력이 무한정 길어질 수가 없다.\nMHA(Multi-head attention) # 하나의 입력에 대해 query, key, value를 하나가 아닌 여러개 만든다.\n즉, 하나의 입력에 대해 n개의 attention을 적용하면 n개의 출력이 나올 것이다.\n관건은 입력과 출력의 차원을 맞춰줘야 하는 것이다. 이는 결과들을 하나로 concatenate하고 입력 차원으로 맞춰주는 행렬을 곱해주면서 해결한다.\n이러한 모든 과정을 그림으로 정리하면 아래와 같다.\nref: https://jalammar.github.io/illustrated-transformer/\n이론적으로는 위 그림처럼만 하면 되지만, 실제 구현은 다르다고 한다.\n가령, 입력 X가 100차원이면 이를 10개로 나눠서 한다고 말씀하신거 같은데.. 실습 포스팅에서 설명해야겠다.\nPositional encoding # 마치 bias처럼 입력에 어떤 값을 더한다. 이는 말 그대로 position에 따른 값의 변화가 필요해서 쓰였다고 한다. 가령, 문장의 순서가 뒤바뀐다고 해도 positional encoding이 없다면 이러한 변화를 알 수가 없다. 따라서 입력 순서에 따른 변화를 positional encoding을 통해 구현한다.\nEncoder Overview # Encoder와 Decoder 사이의 정보 이동 # undefined\nencoder 정보가 decoder로 옮겨가는 과정을 gif로 나타낸 것이다.\nencoder는 key, value를 decoder로 보낸다. encoder에서 query와 나머지 단어들의 key들을 inner product해서 attention을 만들고 여기에 value를 곱한다. 즉, attention map을 알고 싶으면 key와 value를 알면 된다. 왜냐하면 decoder의 입력으로 decoder 내에서 query를 따로 만들기 때문이다. encoder가 stack처럼 쌓여있으니까 상위 레이어의 단어들을 만든다고 한다. (?) output sentence를 자가회귀적으로 만든다. Decoder # Self-attention # softmax step 이전에 미래 정보들에 대한 masking이 된 정보를 생성한다. 즉, 미래의 정보를 알고 decoder를 통해 학습하는 것은 의미가 없으므로 앞단의 정보에만 접근이 가능하도록 만드는 것이다.\nEncoder-Decoder attention # 앞서 언급한 내용들이다. \u0026ldquo;Encoder-Decoder Attention\u0026rdquo; layer는 MHA(Multi-headed self-attention)처럼 작동한다. 다만, query는 이전 layer의 출력 matrix로 만들고 key와 value는 encoder stack에서 얻는다.\nFinal layer # 편의상 final layer로 명명. stack of decoder의 결과물을 단어들의 분포로 만들어준다.\nVision Transformer # 최초 transformer 논문은 기계번역을 위한거였지만, CV에서도 활용이 되기 시작했다.\n이미지를 patch로 나누고 word에서 하는 것과 비슷한 embedding을 거치고 transformer를 사용한다.\nDALL-E # 문장을 이미지로 만드는 논문. GPT-3를 활용했다고 한다.\n","date":"13 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-13-transformer/","section":"Posts","summary":"","title":"Transformer","type":"posts"},{"content":" 2주차 학습정리 # 강의 복습 내용 # DL Basic (1~17번 포스팅) # https://velog.io/@naem1023/series/DL-Basic\nData Viz (1~2번 포스팅) # https://velog.io/@naem1023/series/Data-Viz\n과제 수행 과정 / 결과물 정리 # 저번주 선택과제 이야기를 해야겠다. 모분산, 표본분산에서 n과 n-1을 나누는 차이에 관한 의문이 피어세션 팀에서 제기됐다. 분명 대학교 확률과통계 시간에 배웠던 내용인데\u0026hellip;\n찾아보니 자유도에 관한 내용이었다. https://brunch.co.kr/@zhoyp/174\n표본추정의 경우 모수를 추정하기 위함이 목적이다. 즉, 표본분산을 구하는 이유도 모분산을 구하기 위함이다. 이 때, 표본의 n-1개의 데이터를 관측했을 때, 모수를 맞추기 위해서는 표본의 마지막 1개 데이터는 모수를 위해 맞춰줘야 한다.\n즉, 표본분산은 자유도를 하나 잃게된다.\n같은 논리로 여러 통계량에 대해서 자유도를 구해볼 수도 있다. 아직까지는 자유도에 대해서 1개를 잃는 것만 봐서 모르겠다..\n피어세션 정리 # 팀원분께서 논문 리뷰를 제의해주셨다. 구현까지는 못하더라도, 같이 논문 읽어보고 내용 공유하고 구현된 코드에 대해서 리뷰를 하기로 했다. 리뷰 전에는 각자 abstract만 읽어오기로 했다. 안 그러면 이탈자가 발생한다는 후문이..\n리뷰할 논문의 리스트는 다음의 링크와 멘토님께 부탁해 선택해보기로 했다. https://www.notion.so/c3b3474d18ef4304b23ea360367a5137?v=5d763ad5773f44eb950f49de7d7671bd\n학습 회고 # 수식과의 싸움이었던 한 주였다. 특히 parameter의 수를 센다던지, 이전의 likelihood를 활용하는 내용들은 너무 어려웠다. 컴퓨터공학과임에도 수학적 기반이 탄탄하지 못해서 발생한 문제 같다. 수식을 보는것 자체는 문제 없는데, 수식을 활용하는 것이 어려움이 많다.\n다음 주에 진행되는 논문 리뷰와 더불어, transformer에 관한 내용을 대비하더라도 복습을 많이 할 필요성이 있다.\n","date":"13 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-13-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-2%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 2주차 학습정리","type":"posts"},{"content":"수업을 기준으로 하는 2020년에는 parameter의 수를 줄이는 것이 관건이라고 했다. 왜냐하면 그럴수록 학습이 잘되고 일반적인 성능을 올리거나 generalize performance에도 좋다.\n하지만 오늘 master 클래스에서 교수님께서 2021년에서는 아니라고 하셨다. openai에서 발표된 논문이 있는데, parameter를 늘릴수록 generalize performance가 좋다고 한다. 또한 학습을 할 수 있는 자원이 있다면, 늘리는 것이 학습이나 성능 면에서 더욱 유리하다고 했다.\n","date":"13 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-13-parameter-%EC%88%98/","section":"Posts","summary":"","title":"parameter 수","type":"posts"},{"content":" add_module() # # Concatenate all layers self.net = nn.Sequential() for l_idx,layer in enumerate(self.layers): layer_name = \u0026#34;%s_%02d\u0026#34;%(type(layer).__name__.lower(),l_idx) self.net.add_module(layer_name,layer) self.init_param() # initialize parameters tf에서는 layer를 만들 때부터 name을 따로 설정할 수 있는데, pytorch는 변수 이름을 따라간다.\n다만, 위 처럼 nn.Sequential()을 정의하고 해당 객체의 add_module()을 호출하면 마치 tf처럼 layer의 name을 자유롭게 설정할 수 있다.\nCNN의 train # print (\u0026#34;Start training.\u0026#34;) C.init_param() # initialize parameters C.train() # to train mode EPOCHS,print_every = 10,1 for epoch in range(EPOCHS): loss_val_sum = 0 for batch_in,batch_out in train_iter: # Forward path y_pred = C.forward(batch_in.view(-1,1,28,28).to(device)) loss_out = loss(y_pred,batch_out.to(device)) # Update loss.zero_grad() # reset gradient loss_out.backward() # backpropagate optim.step() # optimizer update loss_val_sum += loss_out loss_val_avg = loss_val_sum/len(train_iter) # Print if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)): train_accr = func_eval(C,train_iter,device) test_accr = func_eval(C,test_iter,device) print (\u0026#34;epoch:[%d] loss:[%.3f] train_accr:[%.3f] test_accr:[%.3f].\u0026#34;% (epoch,loss_val_avg,train_accr,test_accr)) print (\u0026#34;Done\u0026#34;) mlp든 뭐든 특별한 과정이 들어가는게 아니면 동일한 과정의 train이다. 커스텀이 필요하다면 졸프에서 했던 것과 같이, network의 input과 ouput을 자유롭게 수정도 가능하다.\n결국 달라지는 것은 network의 종류이다.\nnn.Module.train() # batch normalization과 같이 train에서만 사용되고 eval에서는 사용되지 않는 layer가 있다면, 학습 전에 반드시 train()을 호출하자.\n","date":"12 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-12-convolution-%EC%8B%A4%EC%8A%B5/","section":"Posts","summary":"","title":"Convolution 실습","type":"posts"},{"content":"https://zzsza.github.io/diary/2017/12/30/2017-retrospect/\nhttps://carpedm20.github.io/\n","date":"11 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-11-%EC%B0%B8%EA%B3%A0-%EB%B8%94%EB%A1%9C%EA%B7%B8/","section":"Posts","summary":"","title":"참고 블로그","type":"posts"},{"content":" CNN # ILSVRC # ImageNet Large-Scale Visual Recognition Chagllenge Classification, Detection, Localization, Segmentation 1000 different categories ![](/assets/images/CNN key concept/4cf1564c-fafb-425d-8e60-9030768a67e7-image.png)\n2015년도를 기준으로 사람보다 error rate가 낮아졌다. 참고로 저 \u0026lsquo;사람\u0026rsquo;은 어느 테슬라 개발자가 직접 해봤다고 한다.\n이후에 설명할 CNN Model들은 해당 대회에서 검증이 됐다고 한다.\nAlexNet # ![](/assets/images/CNN key concept/c1b7c1ab-eecb-4719-8faf-81c3b1f3ef35-image.png)\nAlexNet은 당시 gpu의 자원이 부족했기 떄문에, 네트워크를 2개로 나누어서 서로 다른 gpu 2개에서 학습이 되도록 했다고 한다.\n11x11 filter를 input에 적용했다. 사실 이는 좋은 선택이 아니다. 왜냐하면 filter가 바라보는 이미지의 영역(receptive field)은 넓어지지만 그만큼 parameter의 수가 늘어나기 때문이다.\nKey point\nReLU 해석은 많지만, 어쨌든 network가 깊어져도 network를 망치지 않는 효과적인 activation function이다. 2 GPU LRN(Local response normalization) 출력이 강한 영역을 죽이는 것. 요즘은 잘 안 쓴다. 하지만 data commendation은 무조건 활용한다. 용어가 맞는지 모르겠다.. Overlapping pooling Data augmentation Dropout 2021년에는 당연히 사용하는 기법들이지만, 2012년에는 존재하지 않았던 Deep learning에 대한 기준점들이었다.\nReLU # ![](/assets/images/CNN key concept/26a5bd12-59ad-447b-8279-6e0ec479230f-image.png)\nlinaer model들이 가지고 있는 좋은 성질들을 가지고 있다. gradient 값이 0보다 아주 많이 커져도, 해당 값을 유지할 수 있다. gradient descent가 용이하다. good generalization gradient vanishing 해결 기존의 activation function들은 입력이 0보다 아주 많이 커지면 gradient가 0에 가까워지면서 vanishing한다. VGGNet # ![](/assets/images/CNN key concept/8e9be04a-4c98-4956-a2ca-a68bc40bef27-image.png) ICLR 2015 1등\n3x3 convolution filter만 사용(with stride 1) 1x1 convolution for fully connected layers 최근에 1x1 filter를 사용하는 것처럼 paramter를 줄이기 위해 사용한 것은 아니다. Dropout(p=0.5) ![](/assets/images/CNN key concept/d167cf16-1508-418c-8c7e-84f6914ecccc-image.png)\nReceptive field : filter를 통해 입력받는 영역의 크기\n3x3 filter를 2번 거친 것이 5x5 filter를 한번 쓴것과 receptive field는 5x5로 같다.\n하지만 parameter의 수는 거의 1.5배 차이가 난다. 직접 계산한 식은 위와 같다.\n따라서, 이후의 CNN 논문들은 대부분 3x3, 5x5의 filter를 쓰고 커봤자 7x7을 넘지 않는다. AlexNet의 11x11이 얼마나 비효율적인지 알 수 있다.\n요약하면, receptive field를 늘리고자 한다면 작은 차원의 filter를 여러개 쌓는 것이 훨씬 유리하다.\nGoogLeNet # 지금까지 l을 소문자로 쓰는 논문을 찾아보니까 L을 대문자로 쓰더라. ![](/assets/images/CNN key concept/4e30c6bc-670c-4f2f-a9e1-8ae66fe2e556-image.png)\nILSVRC 2014 1등 NIN(Network in Network), 네트워크 내에 비슷한 모양의 네트워크가 또 존재한다. Inception block 사용 Inception block # ![](/assets/images/CNN key concept/ad954212-d5c2-4e03-afbe-c071dce347b2-image.png)\n여러 path에 대한 convolution 결과들을 concatenete 1x1 convolution 연산을 통해 parameter의 수를 줄인다. 1x1은 channesl 방향으로 dimension을 줄이는 효과가 있다. 1x1 convolution # ![](/assets/images/CNN key concept/41e65537-9b53-4a42-b592-cb7879b0b5f4-image.png) 일반적인 convoltuion 3x3 filter가 128 channel이 있다. ouput channel이 128이기 때문에 filter가 128개 있어야한다.\n따라서, 3x3x128x128 = 147,456개만큼의 parameter가 필요하다.\n1x1 convolution 중간 output의 channel이 32개이다. 즉, 1x1x128 filter가 32개 존재한다. 3x3 convolution을 거친 결과가 128개의 channel을 가진다. 즉, 3x3x32 filter가 128개 있다. 이를 표현하면 위의 수식과 같다. 합하면 40,960개의 parameter가 필요하다.\n1x1 convolution을 섞은 효과 input, output, reception field 크기를 유지하면서 parameter의 수를 줄였다.\nCNN model 중간 비교 # number of parameters\nAlexNet(8 layers): 60M VGGNet(19 layers): 110M GoogLeNet(22 layers): 4M ResNet # 그 유명한 Kaiming He가 쓴 논문이라고 한다. 누군지는 모르겠다..\n등장 배경 # ![](/assets/images/CNN key concept/5d3f16d1-0eff-4c11-92c9-88759de3ba40-image.png)\noverfitting은 과도한 parameter 수를 가질 때 발생한다. 56layer의 네트워크는 아무리 많이 학습해도 20 layer의 네트워크보다 학습이 안된다. Skip connection # ![](/assets/images/CNN key concept/7ee3c4dc-34ed-45dd-bb2c-99302668dddb-image.png) 차이만을 학습하게 해보자\n\u0026ndash;\n![](/assets/images/CNN key concept/131dc1c9-66e5-4b8a-909a-5105a000110b-image.png)\nskip connection을 넣으니 layer를 많이 쓸수록 더 학습이 잘된다.\n![](/assets/images/CNN key concept/1086f419-273a-4fda-9517-eed6532425cf-image.png)\nsimple shortcut 단순히 입력과 convolution 결과를 더한 것 많이 사용 projected shorcut 1x1 convolution과 convolution 결과를 더한 것 잘 사용 안된다. batch nomarlization ResNet 논문에서는 convolution 연산 후, activation function 전에 넣는다. 논란이 많다. conv-\u0026gt;relu-\u0026gt;bn 이 성능이 더 좋다는 의견도 있고, 아예 bn을 안 쓰는게 좋다는 의견도 있다. Bottleneck architecture # ![](/assets/images/CNN key concept/d7cd47b1-05de-440f-9d88-c0641f32eb34-image.png)\n왼쪽이 기존 네트워크, 오른쪽이 bottlenet architecture.\nconvolution 앞뒤로 1x1 filter를 자유롭게 넣어서 입력고 출력 차원을 맞춰주는 효과를 기대함과 동시에 parameter 수를 줄인다.\nCNN model 비교 # ![](/assets/images/CNN key concept/53dd7dd9-0d6f-4312-9073-90c7e997b659-image.png)\nPerformance는 늘었고, parameter는 더 줄었다!\nDenseNet # ![](/assets/images/CNN key concept/57cc94ce-8ddc-4dd6-b4c1-018cbeb8f48d-image.png) ResNet의 Skip connection에서 결과들을 더하면 값이 섞일 것이다. 따라서, concatenate하자. 차원이 같으니 문제도 없을것이다.\n간단한 분류에서 굉장히 유용하다!\n문제점 차원이 2배씩 커질 것이다.\nDense Block, Transition Block # ![](/assets/images/CNN key concept/2aaac941-df7b-4ebe-9c05-645b673705f7-image.png) 문제점을 해결하기 위해 차원을 줄여주면서 network를 구성한다.\n즉, Dense block으로 차원이 커지면, transition block으로 차원을 줄인다.\ntransition block = bn-\u0026gt; 1x1 conv -\u0026gt; 2x2 AvgPooling\n","date":"11 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-11-cnn-key-concept/","section":"Posts","summary":"","title":"CNN key concept","type":"posts"},{"content":"다른 캠퍼분이 정리해주신 내용이 있는데 공유한다. https://velog.io/@hanlyang0522/weight-init%EC%9D%84-%ED%95%98%EB%8A%94-%EC%9D%B4%EC%9C%A0\n정리하면, weight init을 0으로 하지 않는 이상 문제는 없다. 자동으로 해주기도 하고, 실습에서 강사님께서 하신것처럼 pytorch 빌트인 함수를 통해 init을 하는 경우 기본 init과 동일하기 때문이다. 다만, 0으로 하면 학습이 안될 수도 있다.\n","date":"11 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-11-weight-init/","section":"Posts","summary":"","title":"weight init","type":"posts"},{"content":" Convolution # 수식으로 # 역할 # 원하는 feature를 뽑을 수 있다.\n가령, 모든 kernel의 값이 1/9인 (3,3) kernel을 사용했다고 하자. 그러면 평균을 구하는 convolution 연산이 된다.\ntensor # channel이 3개인 RGB이미지를 가정해보자. 이 이미지에 (5,5) filter를 적용한다고 하면, 3개의 channel을 가진 filter를 적용한다고 생각하면 된다.\n가령, 위와 같이 RGB 이미지에 (5,5,3) filter를 4개 적용한다고 하면 channel이 1개인 (28,28) feature가 4개 나올 것이다.\nStack of convolution # MLP처럼 stack을 할 때 non linear function을 통과시켜서 쌓는다.\nConvolution and Neural networks # 위 그림이 가장 고전적인 CNN이다.\nConvolution and pooling layers: feature extraction Fully connected layers: decision making(eg. classification, regression)\n요즘은 Fully connected layer를 줄이는 추세이다. 왜냐하면 parameter의 수를 줄이기 위해서다. parameter 수가 많다면 학습이 어렵고 generalization performance가 떨어지기 때문이다.\nStride # kernel을 stride만큼 옮기면서 convolution 연산을 한다. 1d이기 때문에 stride의 값도 1d이다.\nPadding # 가장자리에 대해서는 convolution 연산이 불가능하다. 따라서, 임의의 값을 채워주고 이미지의 가장자리에 대해서 convolution 연산을 수행한다. e.g., zero padding = 덧대는 부분을 0으로 채운다.\npadding을 하면 input과 ouput의 spacial dimension을 맞출 수 있다.\nundefined\nCounting parameters # Convolution 연산의 parameters = kernel의 parameters\nPadding(1), Stride(1), 3x3 kernel\n3x3 kernel이라고 하지만, 앞서 서술했듯 kerenel의 channel은 input의 channel과 맞춰준다. 즉, (3,3,128) kenrel을 사용한다. 채널 수를 맞춰준 kernel과 input을 convolution하면 반드시 channel을 1개가 된다. output의 channel 수는 64이다. 따라서 (3,3,128) kernel이 64개 존재해야 한다. 이러한 과정을 통해서 대략적인 parameter의 수에 대해 감을 잡는 것은 중요하다!\nAlexnet # convolution 사이의 parameter 숫자와 dense layer 사이의 parameter 숫자가 매우 상이하다!! 이유는 아래와 같다.\nconvolution은 어찌보면 같은 weight를 kernel이라는 요소를 통해서 공유한다. input 이미지의 어느 위치에 있는 요소든 상관없이 동일한 kernel을 사용한다. dense layer는 알고 있듯이 모든 node가 서로 다른 weight를 가지고 있다. 1x1 convolution # 1x1 convolution은 영역을 볼 수 없다. 당연하다. 1x1의 영역에 대해서만 convolution 연산을 반복하는 kernel이기 때문이다.\n하지만 아래와 같은 역할을 기대할 수 있다.\nchannel(dimension) 감소 depth가 증가할 때, parameter 감소를 기대 e.g., bottleneck architecture ","date":"11 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-11-convolution/","section":"Posts","summary":"","title":"Convolution","type":"posts"},{"content":"ref: https://blog.naver.com/cjh226/221356884894\n고등학교 때 배운 2차원 행렬곱 이상은 그림으로 그리기 어렵거나 불가능하다. 100차원 x 100차원을 어떻게 그림으로 표현하겠는가.\n그래서 아래와 같이 수식으로 정의를 하고, 이 정의에 따라서 두가지로 나뉜다.\nnp.dot # A x B라고 하면 A의 마지막 차원과 B의 마지막에서 두번째 차원이 일치해야 곱한다.\nnp.dot(A,B)[i,j,k,m] == np.sum(A[i,j,:] * B[k,:,m])\nnp.matmul # A x B라고 하면 A와 B의 마지막 두 개의 차원이 일치해야 곱한다. 즉, 마지막 두 개의 차원에 대해서 나머지 차원만큼 stack처럼 쌓은거라고 간주한다. e.g., (2,3,4)면 (3,4)를 2개 쌓았다.\nnp.matmul(A,B)[i,j,k] == np.sum(A[i,j,:] * B[i,:,k])\n결론 # 정의는 이해했는데, 사용법이 어떻게 다른지는.. 모르겠다.\n","date":"10 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-10-numpy.matmul-numpy.dot/","section":"Posts","summary":"","title":"numpy.matmul, numpy.dot","type":"posts"},{"content":"","date":"10 August 2021","externalUrl":null,"permalink":"/tags/optimizer/","section":"Tags","summary":"","title":"Optimizer","type":"tags"},{"content":" colab 설정 # %config InlineBackend.figure_format=\u0026#39;retina\u0026#39; matplotlib와 같은 출력물의 해상도를 retina로 설정\n노이즈 # n_data = 10000 x_numpy = -3+6*np.random.rand(n_data,1) # y_numpy = np.exp(-(x_numpy**2))*np.cos(10*x_numpy) y_numpy = np.exp(-(x_numpy**2))*np.cos(10*x_numpy) + 3e-2*np.random.randn(n_data,1) plt.figure(figsize=(8,5)) plt.plot(x_numpy,y_numpy,\u0026#39;r.\u0026#39;,ms=2) plt.show() x_torch = torch.Tensor(x_numpy).to(device) y_torch = torch.Tensor(y_numpy).to(device) print (\u0026#34;Done.\u0026#34;) ![](/assets/images/Optimizer 실습/f9fa06fa-733a-458b-8c5c-4cf9aba778bb-image.png) 본래 의도한 함수의 그래프는 위와 같다. 해당 그래프에 노이즈를 추가해보자. 노이즈는 위 코드에 서술됐듯이, np.random.randn()에 작은 실수값인 3e-2를 곱해줘서 구현된다. ![](/assets/images/Optimizer 실습/95e2a7f4-1755-447c-929d-e3bdb570a047-image.png)\noptimizer 비교 # ![](/assets/images/Optimizer 실습/ec72c958-f2cd-45c4-a3d6-f6065c0fb9b3-image.png) ![](/assets/images/Optimizer 실습/851dd821-a73e-4f8d-aede-9a0960ae7c9c-image.png) ![](/assets/images/Optimizer 실습/fce0de37-8504-4b73-936a-7d5a347684f3-image.png)\n500번째, 3500번째, 9999번재 epoch에서 각각의 optimizer를 활용한 모델이 함수를 얼마나 근사하고 있는지 나타내는 그래프들이다. GT는 근사하고자 하는 그래프다.\nADAM은 초기부터 벌써 근사했다. 매우 빠르다. SGD와 Momentum은 비슷해보인다.\n","date":"10 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-10-optimizer-%EC%8B%A4%EC%8A%B5/","section":"Posts","summary":"","title":"Optimizer 실습","type":"posts"},{"content":" 부캠 강사님께서 용어에 대해 확실히 알고 가라고 하셨다.\n어제(21.08.09) 선택과제 2번의 AAE에 대해서 알아보다가 기겁을 했다. 한 문장 안에서 모르는 단어를 세는 것보다, 아는 단어를 세는게 빨랐다. 분명 영어로 쓰여있는데 외계어 같았다\u0026hellip;\n인턴을 하면서, 졸업프로젝트를 하면서 혼자 독학으로 잡다하게 지식을 쌓아올린 폐해라고 생각한다. ML 분야에서 사용되는 용어들만이라도 확실하게 알고 있자.\n그런 의미로 알고 있던 용어들이라도 수업에서 다뤘던 것들은 모두 기술했다.\nIntroduction # Gradient descent # undefined\nFirst-order iterative optimization algorithm for finding a local minimum of a differentiable function.\nHyper parameter # parameter : 가중치, bias, convolution weight 등 학습이 되면서 업데이트 되는 것들. hyper parameter : learning rate, network의 크기, loss function의 종류 등 개발자가 직접 정하는 것들.\nGeneralization # 보통 학습이 오래 지속되면 test error는 증가한다. train과 test 사이의 성능 gap을 Generalizaiton gap이라고 한다.\nGeneralization performance가 좋다. = 모델의 성능이 학습 데이터를 사용했을 때와 비슷함을 보장.\n하지만, Generalization performance가 좋다고 모델의 성능이 좋음을 보장하지는 않는다. 왜냐하면 학습 데이터가 제대로 학습되지 않은 모델임에도 Generalization performance는 좋을 수 있기 때문이다.\nUnderfitting, Overfitting # Overfitting(과적합) : 학습 데이터에서는 모델이 잘 작동하지만, 실제 성능은 좋지 않은 것. Underfitting : 학습이 제대로 되지 않아, 학습조차 제대로 안된 것.\n물론, 위 그림은 concept만을 명시한 것이다. 즉, Overfitting의 예시 도표가 실제로는 우리가 원하는 결과일 때도 분명 존재한다는 것이다. 문제의 정의, 도메인 지식 등 여러가지를 살려서 판단하자.\nCross-validation # K-fold validation. ref: https://blog.quantinsti.com/cross-validation-machine-learning-trading-models/\ntrain data를 k개로 구분한다. k-1개를 train에 사용한다. 나머지 1개를 validation에 사용한다. hyper parameter에 대한 clue는 보통 없다! 그래서 cross validation을 통해 최적의 hyper parameter 조합을 찾는다.\n최적의 hyper parameter를 찾으면, hyper parameter를 고정 후 모든 학습 데이터를 활용해 학습한다.\n당연하지만 test data는 어떠한 방식으로든 학습에서 활용하면 안된다. 엄연히 cheating과도 같은 행위이다. 물론 cheating을 한다고 좋은 모델을 만든다고 보장할 수도 없다.\nBias, Variance # ref : https://work.caltech.edu/telecourse\n탄착군이랑 똑같이 생각하자. Variance\n입력에 대해 출력이 얼마나 일관적인지를 의미. 낮을수록 일관적이다. 높을수록 일관적이지 않다. Bias\n원하고자 하는 값과 얼마나 떨어져 있는지 Bias and Variance Tradeoff # 학습 데이터에 노이즈가 끼어 있다고 가정할 한다. We can derive that what we are minimizing(cost) can be decomposed into three different parts: $$bias^2$$, $$variance$$, and $$noise$$.\n즉, 내가 minimize하는 값은 한가지 값이지만 그 값은 3가지 component이다. 또한 3가지 component는 무언가가 작아지면, 무언가가 커지는 trade-off 관계이다.\n즉, cost를 위와 같이 3가지 term으로 구분해서 생각할 수 있는 것이다. 보통 bias와 variance가 trade-off라고 한다.\nBootstrapping # 통계학에서 사용되는 용어. Bootstrapping is any test or metric that uses random sampling with replacement.\n가령 100개의 학습데이터에 대해서 무작위로 80개씩 추출하는 행위를 통해 여러 개의 모델 configuration을 만들어 서로 비교할 때 사용한다.\nBagging, Boosting # Bagging(Boostrapping aggregating)\neg. Bootstrapping을 통해 학습 데이터를 여러개로 subsampling한다. 여러개의 학습 데이터별로 서로 다른 모델의 output이 발생하고 이를 활용한다. (통계값을 추출하거나 앙상블 학습을 하거나 등등) 전체 학습 데이터를 하나의 모델에 대해 한번만 학습해 하나의 결과를 추출하는 것보다 더 좋은 성능을 보여주는 경우가 대부분이다. Kaggle같은 대회에서 대표적으로 사용되는 기법이다.\nBoosting 가령 학습을 할 때, 80개의 데이터에 대해서는 분류가 잘 되지만 나머지 20개의 데이터에 대해서는 분류가 잘 안된다고 가정해보자. 나머지 20개에 대해서는 별도의 모델을 생성해서 학습을 한다. 이러한 방식으로 만들어진 모델을 weak learner라고 하자.\n이러한 weak learners를 sequence하게 묶어서 하나의 strong learner를 만든다. boosting에서는 weak learner들의 weight들을 sequence하게 학습한다.\nPractical Gradient Descent Method # Stochastic gradient descent 한번에 하나의 데이터만을 사용해 parameter 업데이트. mini-batch gradient descent 한번에 subset 데이터만을 사용해 parameter 업데이트. batch gradient descent 모든 데이터를 한번에 활용 Batch size matters # We .. present numerical evidence that supports the view that large batch methods tend to converge to sharp minimizers of the training and testing functions. In constrast, small-batch methods consistently converge to flat minimizers\u0026hellip; this is due to the inherent noise in the gradient estimation.\n해당 논문에서 large batch method는 sharp minimizer를, small-batch method는 flat minimizer를 가진다고 한다. 이에 대한 설명은 아래 그래프와 같다.\nref: https://arxiv.org/pdf/1609.04836.pdf (ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA)\nflat minimum : test function과 training function이 멀더라도, 어느정도 학습이 된다. Generalization performance가 높다! sharp minimum : Geenralization performance가 낮다.\nGradient descent methods # (Stochastic) Gradient descent # 너무나도 익숙한 기본적인 Gradient descent의 parameter update 수식이다.\n문제점 : η(learning rate)를 잡는 것이 어렵다. 너무 작으면 학습 진행이 안되고, 너무 크면 학습이 제대로 안된다.\nMomentum # 말 그대로 Momentum(관성)을 유지하면서 parameter를 업데이트한다. β(momentum)은 hyper parameter이다. β와 gradient, 이전 step의 accumulation을 통해 새로운 accumulation을 얻는다. 이를 통해 이전 step에서 사용된 정보를 SGD처럼 모두 버리지 않고, 어느 정도 유지하면서 W를 업데이트한다.\nNesterov Accelerated Gradient(NAG) # 수식의 큰 틀은 momentum과 같다. 다른 점은 다음 스텝의 graident를 미리 계산해보고 이렇게 계산된 Lookahead gradient를 활용해 accumulation을 업데이트한다.\nref: https://golden.com/wiki/Nesterov_momentum\nMoementum과 Nesterov momentum은 위 그림과 같은 차이가 있다. 직관적으로 이해해보면, momentum은 converge point에 한번에 수렴하지 못하고 진자마냥 움직이면서 수렴을 하지 못하는 경우가 생긴다.\nNesterov는 다음 step의 gradient를 활용해 업데이트하기 때문에, 한쪽 방향으로만 가는 효과가 있다. 따라서 NAG가 보통 좀더 빨리 converge한다.\nAdagrad # parameter가 지금까지 얼마나 많이 변화했는지를 업데이트에 반영한다. $$G_t$$는 sum of gradient squares다. 즉, parameter가 많이 변화했으면 $$G_t$$가 커지므로 parameter는 적게 변화한다. parametert가 적게 변화했으면 $$G_t$$가 작으므로 parameter는 크게 변화한다.\n$$\\epsilon$$은 zero division을 방지하기 위해 들어갔다.\n문제점 $$G_t$$가 무한정 커질 수 있다. 즉, 분모가 무한대가 되면서 해당 항이 0에 수렴하게 된다. parameter가 더 이상 업데이트 되지 않는 문제점이 발생한다.\nAdadelta # Window size만큼의 시간동안 gradient의 변화만을 보는 방법론이다.\n문제는 $$g_t$$의 parameter가 모델의 parameter 개수만큼 존재해야하는 것이다. model의 parameter는 개별적으로 하나의 gradient를 가지기 때문이다. 그렇다면 GPT3와 같은 대형모델에서 천억개의 parameter를 가진다고 하면, $$g_t$$ 또한 천억개의 parameter에 대한 gradient정보를 windows size만큼 가지고 있어야한다.\n이를 해결하기 위해 exponential moving average(EMA, 이동평균)을 사용한다. 위 수식에서 $$\\gamma$$를 사용한 방식이라고 한다\u0026hellip;(?)\nlearning rate가 없다! = hyper parameter를 변경할 수 있는 여지가 없다. 따라서 실용적으로 활용되지는 않는다.\nRMSprop # 논문으로 발표된 방식은 아니고, Geoff Hinton이 강의 중에 공개한 optimzation. 실제로 논문들이 RMSprop을 사용할 때, Geoff Hinton의 lecture link를 citation했다고 한다.\nAdam(Adaptive Moment Estimation) # past graidents(momentum)과 squared gradients(Adagrad, RMSprop\u0026hellip;)을 합친 것. 즉, momentum 정보와 adaptive learning rate 방식을 혼용한 것. 4개의 hyper paramter를 조정하는 것도 매우매우 중요하다.\n$$\\epsilon$$ : 매우 작은 값 $$\\beta_1$$ : momentum $$\\beta_2$$ : graident squares $$\\eta$$ : learning rate Regularization # 학습을 규제, 방해해서 학습데이터에서만 모델이 잘 작동하는 것이 아니라, 테스트 데이터에서도 잘 작동하도록 하는 것이 목적.\nEarly stopping # test data가 아닌 validation data를 활용해, 적절하게 멈추는 시점을 정한다.\nParameter Norm Penalty # parameter의 크기가 커지지 않게 하는 것. total cost를 작게 하는 방향으로 학습하자.\n함수 공간 내에서 함수를 되도록 부드럽게 만들자.(?) 라고 강사님이 말씀하셨는데 무슨 말인지 모르겠다..\nparameter norm penalty를 weight decay라고 부르기도 한다.\nData Augmentation # DL, NN은 전통적인 ML과 다르게 데이터가 많으면 많을수록 좋다. 그래서 이미지를 뻥튀기하듯이 데이터를 많이 생성하면 되도록 좋다.\n위 그림과 같이 이미지의 크기, 기울기, crop 정도를 변화하면서 data를 늘린다. 단, 라벨은 고정되어야한다.\nNoise Robustness # 입력 데이터와 weight에 noise를 넣어주어 학습하면 테스트 단계에서 더욱 잘 작동한다. 완벽하게 증명은 안됐지만, 실험적으로는 증명됨. Label Smoothing # 학습 단계에서 학습 데이터 두 개를 추출해 섞어서 새로운 학습 데이터를 생성. Decision boundary를 부드럽게 만들어주는 효과가 있다고 한다.\n모델의 성능을 매우매우 잘 올릴 수 있는 방법론이다!\nref: https://arxiv.org/pdf/1905.04899.pdf (CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features)\nMixup : 두 개의 이미지와 라벨을 섞는다. Cutout : 이미지의 일정 부분을 제거 CutMix : Mixup과 다르게 잘라서 섞는다.\nDropout # 랜덤하게 일부 뉴런을 0으로 만들어준다. 뉴런들이 좀더 robust한 feature를 가진다고 해석한다. 이 또한 증명은 없다.\nBatch Normalization # layer별로 weight들의 mean과 variance를 활용해 weight를 normalize한다. 위 수식처럼 mean을 빼고 variance로 나눠줘서 새로운 weight를 구한다.\n논문에서는 이러한 행위가 Internal covariate shift를 줄이기에 성능이 향상된다고 해석했는데, 여러 반박 논문들이 있다고 한다..\n확실한 것은 BN을 사용하면 Network가 깊어질수록 사용하지 않을 때보다 성능이 많이 좋아진다.\nBN과 비슷한 방법론들이 있다. 적절하게 사용하자.\n","date":"10 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-10-optimization/","section":"Posts","summary":"","title":"Optimization","type":"posts"},{"content":"","date":"10 August 2021","externalUrl":null,"permalink":"/tags/optimization/","section":"Tags","summary":"","title":"Optimization","type":"tags"},{"content":" Data viz # 데이터 # 시각화를 진행할 데이터\n데이터셋 관점(global) 개별 데이터의 관점(local) 정형 데이터 # CSV 파일. ![](/assets/images/데이터 시각화/1484c59d-c573-4c81-9589-73f236ab76e7-image.png)\nitem = row 1개 attribute(feature) = column\n시계열데이터 # ![](/assets/images/데이터 시각화/e48008d8-1e3e-4129-8973-8dbc01291416-image.png)\n시간의 흐름에 따른 Time-Serires 형태. 음성, 비디오 시간 흐름에 따른 추세(trend), 계절성(SEasonality), 주기성(cycle)을 살핀다. 지리/지도 데이터 # ![](/assets/images/데이터 시각화/fe86e094-5886-4ab7-afb1-c7131245d584-image.png)\n거리, 경로, 분포 등을 사용 관계 데이터 # ![](/assets/images/데이터 시각화/3929c3f1-8c33-4731-9c2d-fede261217d2-image.png)\n객체 간의 관계를 시각화 객체는 Node 관계는 Link 계층적 데이터 # ![](/assets/images/데이터 시각화/75e8e44a-586d-4da1-8c8d-f4776af7e2d7-image.png)\n포함관계가 분명한 데이터 Tree, Treemap, Sunburst 등.. 데이터의 종류 # 수치형(numerical) 연속형(continuous) : 길이, 무게, 온도.. 이산형(discrete) : 주사위 눈금, 사람 수 \u0026hellip; 범주형(categorical) 명목형(nominal) : 혈액형, 종교 \u0026hellip; 순서형(ordinal) : 학년, 별점, 등급 \u0026hellip; 이산형과 순서형이 혼용될 수 있다.\n이산형 : 수치적으로 비례해서 사용 가능하다. 순서형 : 이산형이 아니고 순서 같은 것들이 존재하는 것? mark, channel # Mark\n점, 선, 면으로 이루어진 시각화 데이터 Channel\n각 마크를 변경할 수 있는 요소들 ![](/assets/images/데이터 시각화/5486582e-4644-44a4-96fb-a0d097d0f8e7-image.png) Pre-attentive Attribute(전주의적 속성) # 특별히 관심을 가지지 않아도 사람이 자연스럽게 인지하게 되는 요소들을 의미한다. ![](/assets/images/데이터 시각화/15c051bd-5598-496c-8194-443cc576886f-image.png)\n가령 위 그림에서 Orientation은 가운데 요소만 기울기만 다른 것을 한번에 알 수 있다.\n동시에 사용하면 인지하기 어렵다! 적절하게 사용해, 시각적 분리(visual pop-out) 유도하자.\n","date":"9 August 2021","externalUrl":null,"permalink":"/posts/old_postings/2021-08-09-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%8B%9C%EA%B0%81%ED%99%94/","section":"Posts","summary":"","title":"데이터 시각화","type":"posts"},{"content":" init parameters # pytorch에서 자동으로 해주기는한다. 하지만 수동으로 parameter를 원하는대로 초기화해야하는 경우가 분명 발생한다. 이를 수동으로 해결하는 방법은 기본적으로 아래와 같다.\nclass MultiLayerPerceptronClass(nn.Module): \u0026#34;\u0026#34;\u0026#34; Multilayer Perceptron (MLP) Class \u0026#34;\u0026#34;\u0026#34; def __init__(self,name=\u0026#39;mlp\u0026#39;,xdim=784,hdim=256,ydim=10): super(MultiLayerPerceptronClass,self).__init__() self.name = name self.xdim = xdim self.hdim = hdim self.ydim = ydim self.lin_1 = nn.Linear( # FILL IN HERE ) self.lin_2 = nn.Linear( # FILL IN HERE ) self.init_param() # initialize parameters def init_param(self): nn.init.kaiming_normal_(self.lin_1.weight) nn.init.zeros_(self.lin_1.bias) nn.init.kaiming_normal_(self.lin_2.weight) nn.init.zeros_(self.lin_2.bias) def forward(self,x): net = x net = self.lin_1(net) net = F.relu(net) net = self.lin_2(net) return net M = MultiLayerPerceptronClass(name=\u0026#39;mlp\u0026#39;,xdim=784,hdim=256,ydim=10).to(device) loss = nn.CrossEntropyLoss() optm = optim.Adam(M.parameters(),lr=1e-3) print (\u0026#34;Done.\u0026#34;) session # pytorch의 큰 장점은 session이 없다는 것이다. 물론 tf도 ver 2부터는 session이 없기는하다. session이 없어서 아래처럼 바로 forward를 할 수 있다.\nforward # 본래 forward를 쓰지 않아도 아래와 같이 알아서 forward를 해준다. 하지만 명시하는 것이 읽기에 편하니, 써보자.\nx_numpy = np.random.rand(2,784) x_torch = torch.from_numpy(x_numpy).float().to(device) y_torch = M.forward(x_torch) # forward path # y_torch = M(x_torch) # forward path y_numpy = y_torch.detach().cpu().numpy() # torch tensor to numpy array print (\u0026#34;x_numpy:\\n\u0026#34;,x_numpy) print (\u0026#34;x_torch:\\n\u0026#34;,x_torch) print (\u0026#34;y_torch:\\n\u0026#34;,y_torch) print (\u0026#34;y_numpy:\\n\u0026#34;,y_numpy) model.eval() # 애매하게 알고 썻던 내용인데 정리해본다.\nBatchNormalization이나 DropOut 같이 학습 시에만 사용되고 predict 단계에서는 사용하면 안되는 것들이 있다. 이런 부분들을 막기 위해서 predict시에는 반드시 model.eval() 사용하는 것을 관례처럼 여기자.\nview # 원소의 수를 유지하면서 tensor의 차원을 바꿔주는 함수. numpy의 reshape이다. 차원에 -1을 넣어주면 pytorch가 알아서 설정하도록 하는 것이다.\nbatch_in.view(-1, 28*28) item # tensor는 모두 tensor라는 객체로써 관리된다. 이를 실수와 같은 형태로 바꾸고 싶다면 item을 사용한다.\nn_correct += (y_pred==y_trgt).sum().item() train # print (\u0026#34;Start training.\u0026#34;) M.init_param() # initialize parameters M.train() EPOCHS,print_every = 10,1 for epoch in range(EPOCHS): loss_val_sum = 0 for batch_in,batch_out in train_iter: # Forward path y_pred = M.forward(batch_in.view(-1, 28*28).to(device)) loss_out = loss(y_pred,batch_out.to(device)) # Update optm.zero_grad() # reset gradient loss_out.backward() # backpropagate optm.step() # optimizer update loss_val_sum += loss_out loss_val_avg = loss_val_sum/len(train_iter) # Print if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)): train_accr = func_eval(M,train_iter,device) test_accr = func_eval(M,test_iter,device) print (\u0026#34;epoch:[%d] loss:[%.3f] train_accr:[%.3f] test_accr:[%.3f].\u0026#34;% (epoch,loss_val_avg,train_accr,test_accr)) print (\u0026#34;Done\u0026#34;) optm.zero_grad() # 앞서 optim을 아래와 같이 정의하여 어떠한 parameter를 학습할지 정의했다.\noptm = optim.Adam(M.parameters(),lr=1e-3) zero_grad()를 통해 해당 paramter의 gradient를 0으로 초기화하는 함수이다.\nloss() # 앞서 cross entropy로 정의한 loss function을 의미한다. loss에 매개변수로 모델의 출력인 y_pred와 학습 데이터인 batch_out을 넘겨주면 이에 대한 weight 객체를 반환한다.\nbackward() # 각각의 weight에 대한 backward propagation을 수행한다.\nstep() # 앞서 정의한 optimizer의 learning rate와 여러 다른 hyper parameter를 통해, 인자로 넘겨받았던 parameter를 업데이트한다.\n","date":"9 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-09-pytorch/","section":"Posts","summary":"","title":"pytorch","type":"posts"},{"content":" Neural Networks # 인간이 가진 뇌의 신경망을 모방했기 때문에 잘 작동한다고도 한다. ![](/assets/images/NN \u0026amp; Multi layer perceptron/bec49730-5ab0-475b-9da8-3c21210874a5-image.png) 어느 정도 맞는 말이다. 실제 뉴런의 형태를 모방해서 구현된 것이 NN의 node와 흡사하다.\n하지만 굳이 뇌를 모방한 것이라 하기에는 Back propagation과 같은 과정이 NN에는 필수적이다.\n![](/assets/images/NN \u0026amp; Multi layer perceptron/163090cf-0a5c-43ab-a4a4-651564b282ae-image.png)\n초창기의 비행기는 박쥐, 새를 모방했다. 라이트 형제의 비행기도 어느정도 그러한 형태이다. 하지만 최근의 비행기에서는 박쥐, 새를 모방했다고 할 수는 없다.\nNN도 마찬가지이다. 인간의 지능을 모방하기 위해서 인간의 뇌를 모방하는 것에서 시작했지만, 최근의 DL 연구들은 인간의 작동방식과 매우 상이하다.\n결론은 NN이 잘 작동하는 이유를 단순히 인간을 모방했기 때문이라 단정하지 말고, 수학적으로 왜 그러한지 분석해야 한다.\nDefine # Neural networks are function approximators that stack affine tarnsformations follwed by nonlnear transformations.\n함수를 근사한다. Activation function을 통해 비선형적으로 구현. Linear Neural Network # Simple data # ![](/assets/images/NN \u0026amp; Multi layer perceptron/1ca7f55c-4f9d-41cc-99d4-a3d33d1c8fea-image.png)\ndata, model, loss를 위와 같이 정의해보자. 이 때, model의 parameter인 최적의 w와 b를 찾아보자.\n![](/assets/images/NN \u0026amp; Multi layer perceptron/07c46c4e-0f0f-4c37-9f36-6b875157b1c8-image.png) ![](/assets/images/NN \u0026amp; Multi layer perceptron/8bff603b-ec9b-4cfb-9408-aeb4e9beca63-image.png)\nLinear regression 문제이고, convex function이고, 많이 않은 학습 데이터들이 사용됐기에 최적의 w와 b를 한번에 구하는 방법론이 분명 존재한다. 하지만 DL에서는 이를 위와 같이 back propagation을 통해 구한다.\nback propagation의 목표 : loss를 최소화하는 방향으로 parameter 업데이트. back propagation에 대해서는 다른 포스팅에서 자세히 설명했기 때문에, 더 자세하게 서술 안한다.\n![](/assets/images/NN \u0026amp; Multi layer perceptron/ff1ceb8c-7a52-407f-b478-520282df6956-image.png)\nMore large data # ![](/assets/images/NN \u0026amp; Multi layer perceptron/9264a0b0-a60f-4569-a695-92eecd9a942d-image.png) 행렬을 통해 가중치들을 표현. W와 b를 통해 x를 y로 보내는 것이 목표.\nMore layer stack # ![](/assets/images/NN \u0026amp; Multi layer perceptron/84ff7973-780d-4703-8547-2beb5225d078-image.png)\nlayer를 더 쌓고 싶다면 가중치의 곱으로 표현할 수 있다. 위의 형태는 앞선 기본적인 수식을 중첩시킨 것이다. (bias는 무시하고 표현)\n위의 방식으로 hidden layer가 존재하는 multi layer를 의도했지만, 결론은 이 또한 하나의 layer라고 생각할 수 있다. 왜냐하면 W2와 W1이 행렬곱을 통해 하나의 가중치처럼 표현되기 때문이다.\n![](/assets/images/NN \u0026amp; Multi layer perceptron/bffeb80b-7d38-4cb8-8009-ca32340fc60a-image.png) 따라서 위와 같이 Nonlinear transform을 수행한 후에, 다시 선형변환과 결합시켜야지 layer를 쌓는 효과가 발생한다.\nActivation functions # ![](/assets/images/NN \u0026amp; Multi layer perceptron/38244fff-f134-4a27-b7d9-ba9d99eda595-image.png) 어떤 것이 좋을지는 문제마다 다르다.\nBeyond Linear Neural Networks # ![](/assets/images/NN \u0026amp; Multi layer perceptron/a53406e4-8bcb-4a0b-bb80-318de84b70be-image.png) 임의의 집합 K에서 우리가 원하는 continous한 function은 한 개의 hidden layer만으로 우리가 원하는만큼 근사할 수 있다. =\u0026gt; 존재성만을 암시. 즉, 내가 학습시킨 NN이 내가 원하는 함수를 근사할 것이라는 보장이 아니다.\nNN의 표현력만을 보여준다.\nLoss function # ![](/assets/images/NN \u0026amp; Multi layer perceptron/6b1295f4-f730-4a80-ab47-57273668294d-image.png)\ncross entropy는 분류 문제에서 사용된다.\n분류 문제에서의 라벨은 보통 one-hot 벡터로 표현된다. 즉, 분류하고자 하는 차원만 값이 존재하고 나머지는 전부 0인 것이다. 이 때, 해당 값은 뭐든 상관없다. 1이어도 되고 1000000이어도 된다. 다른 값들과 다른 것으로 분류가 되기 때문이다.\n이러한 특성을 수식으로 표현하기 위해 cross entropy를 사용한다고 한다.\n혹은 사람의 얼굴을 보고 나이대를 추측하는 모델을 만들고자 한다. 이럴 때는 보통 확률로써 표현하게 되는데, MSE를 통해 log likelihood를 활용해 구현한다.\n","date":"9 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-09-nn--multi-layer-perceptron/","section":"Posts","summary":"","title":"NN \u0026 Multi layer perceptron","type":"posts"},{"content":" DL # 모호하게 알고 있던 내용들을 부캠강의에서 명료하게 정리해줬다. 당연한 이야기들도 많은데 그냥 다 정리했다.\n갖춰야할 능력 # 구현 능력 수학 능력(Linear algebra, probability) 최신 트렌드의 논문들을 많이 알고 있는 것 정의 # ![](/assets/images/DL /ca2391dc-7082-4d0c-9318-adaf8ae8d212-image.png)\nAI = 사람의 지능을 모방하고자 한다 ML = 데이터 기반으로 문제 풀이 접근 DL = 데이터 기반으로 사람의 지능을 모방하고자 할 때, NN을 사용하는 분야\nKey components # Data : the model can learn from Model : How to transform the data Loss function : that quantifies the badness of the model Algorightm : Adjust the parameters to minimize the loss Data # ![](/assets/images/DL /e0b03f19-eeb4-4af7-b923-11162a010c1d-image.png) 문제의 정의에 따라서 달라진다.\nClassification : 라벨링 데이터 Semantic segmentation : segmentation data Detection : bounding box data Pose estimation : skleton data Visual QnA : Color data, etc\u0026hellip; Model # 문제의 정의에 맞는 모델 필요\nLoss function # data와 model이 정해져있을 때, 문제의 정의에 따라 이를 어떻게 학습할지 정의. ![](/assets/images/DL /0dcd9b7d-2e4e-407b-958f-8a98c1a9ffdc-image.png)\nLoss function이 정상적으로 작동한다고 할지라도, 원하는 결과를 뱉을거란 보장은 없다.\n가령, 회귀문제라 할지라도 노이즈 굉장히 많이 낀 데이터를 사용한다고 해보자. 그러면 outlier는 MSE의 제곱에 의해 굉장히 커질 것이다.\n이런 문제를 방지하기 위해 MSE에서 제곱 대신 절대값을 사용하거나 아예 다른 loss function을 사용하는 것을 고려할 수 있다.\n따라서 문제와 데이터에 따라 적절하게 정의해주자.\nOptimization algorithm # undefined\nLoss function을 최적화시켜주는 방법들.\n보통 NN의 parameter를 loss function에 대해 1차 미분한 정보를 활용하는데, 이를 그냥 활용하는 것이 SGD. 실제로 SGD는 잘 안 쓰이고 나머지 것들이 쓰인다.\n그 외에도 다음과 같은 기법들이 쓰인다.\nDropout Early stopping k-fold validation weigth decay batch normalization MixUp Ensemble Bayesian Optimization History of DL # AlexNet(2012) # ![](/assets/images/DL /75f40b7a-4e4c-427e-8803-94e890afca9f-image.png)\nCNN 최초로 DL을 사용해 ILSVRC(ImageNet Large Scale Visual Recognition Challenge) 우승 이후의 모든 ILSVRC 회차에서는 DL이 우승했다. 이 시점을 기점으로 판도가 바뀐거다. DQN(2013) # ![](/assets/images/DL /7cf33bb9-514c-46b8-b380-536c31444421-image.png)\nAtari 게임을 강화학습으로 풀게한 DL DeepMind가 개발 알파고에 사용된 알고리즘 Encoder/Decoder(2014) # ![](/assets/images/DL /d46ac9c4-767b-42ce-ada9-79160c6042dd-image.png)\nNMT(Neural machine translation) 개발을 위해 개발 sequence to sequence model 이 시점을 기준으로 NMT가 바뀜. Adam Optimizer(2014) # ![](/assets/images/DL /8f051186-ea7d-4bd6-884e-79e5514d9523-image.png) 보통 논문들은 다양한 learning schedule을 통해 구현된다. learning rate를 epoch마다 바꾼다던지, SGD를 사용한다던지 등등\u0026hellip;\n이러한 방법론들을 보통 매우 큰 컴퓨팅 리소스를 요구한다. 가령, 대기업이 TPU를 1000개를 보유하고 있다고 할 때, 해당 기업은 한번에 1000개의 configuration을 돌려볼 수 있다. 학생들은 보통 GPU를 많아야 1, 2개 가지고 있는데 대기업 역량의 논문을 쓰고자 한다면 1년이 넘어갈 수도 있다.\n이 때, Adam은 대부분의 방법론에서 매우 잘 작동한다. 즉, 많은 configuration을 실험해야 하는 의무를 어느 정도 벗어주게 한 것이다.\nGAN(Generative Adversarial Network, 2015) # ![](/assets/images/DL /c7b5d85b-4897-424e-82c9-c6757ddb5c90-image.png) 기존의 이미지들을 사용해, 실제와 같은 이미지를 생성하는 DL. 연구자가 술집에서 술 먹다가 아이디어가 떠올랐다고 한다\u0026hellip;\nResidual Networks(ResNet, 2015) # DL을 DL이라고 불리게 해준 연구. NN을 굉장히 깊게 쌓은 논문.\n이전에는 깊은 layer를 구성하지 않았다. 왜냐하면 학습 데이터는 잘 학습될지라도 테스트 데이터를 사용하면 성능이 별로였다.\nResNet 이후에는 깊은 layer를 쌓기 시작했다. 물론 1000 layer와 같은 구성방식은 여전히 안되고 비효율적이다. 하지만 이전에는 20 layer 밖에 쌓지 못했던 것을 100 layer로 쌓게 해주는 것과 같은 성과를 보여줬다.\nTransformer(2017) # \u0026ldquo;Atten is All You Need\u0026quot;라는 제목의 논문.\n당시에는, 해당 분야에서만 잘 작동하는 것이라 생각됐지만 현재는 거의 모든 분야의 RNN을 대체했다. CV까지 넘보는 중.\nBERT(fine-tuned NLP models, 2018) # Bidirectional Encoder Representations from Transformers.\nfine-tuned NLP models 커다랗고 범용적인 학습데이터를 활용해 pre training을 한다. 원하고자 하는 범주에 대해서 fine-tuning을 한다. BIG Language Models(GPT-X, 2019) # 약간의 fine-tuning을 통해 원하는 데이터 구성. 굉장히 많은 parameter로 구성(175billion) Self Supervised Learning(2020) # ![](/assets/images/DL /f7da3d8d-72c3-40dd-8505-d2e575726bcf-image.png)\n대표적으로 SimCLR과 같은 논문 라벨을 모르는 데이터를 활용하겠다. unsueprvised learning을 활용해, 학습 데이터 외에서도 좋은 representation을 얻겠다. self supervised data sampling 정의된 문제에 대한 고도의 도메인 지식이 있을 때, 학습 데이터를 스스로 만드는 방법론 ","date":"9 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-09-dl-/","section":"Posts","summary":"","title":"DL ","type":"posts"},{"content":"","date":"6 August 2021","externalUrl":null,"permalink":"/tags/math/","section":"Tags","summary":"","title":"Math","type":"tags"},{"content":" RNN # 시퀸스 데이터(sequence) # 소리, 문자열, 주가 등 순차적으로 진행되어야 하는 데이터. 독립동등분포(iid, independent and identically distributed)를 위배하기 쉽다. 가령, \u0026lsquo;개가 사람을 물었다\u0026rsquo;와 \u0026lsquo;사람이 개를 물었다\u0026rsquo;는 데이터 분포, 빈도, 의미 등 모든 것이 바뀐다. 순서를 바꾸거나 과거 정보에 손실이 발생하면 데이터의 확률분포가 달라진다. 과거의 정보 또는 맥락 없이 미래를 예측하는 것은 불가능하다. 시퀸스 데이터 다루기 # 이전 시퀸스의 정보를 통해 앞으로 발생할 데이터의 확률분포를 다루기 위해 _조건부확률_을 이용.\n모든 과거의 정보들을 활용해서 조건부확률을 계산하고자한다면 위와 같이 식을 세울 수 있다.\n보통은 아래와 같이 시퀸스 데이터를 다룬다. 즉, 과거의 모든 정보가 필요한 것은 아니다. 물론 도메인에 따라 천차만별이다.\n가령 주가를 예측하는데 30년 전에 창업된 기업의 창업일부터 현재까지의 모든 정보를 활용할 필요는 없다. 보통 5년가량의 정보를 가져와 다룬다.\n=\u0026gt; 정보를 truncation하는 것도 기술이다.\n시퀸스 데이터는 위와 같이 가변적으로 다룰 수 있어야 한다. 즉, 가변적 길이를 처리할 수 있는 모델이 필요하다.\nAutogressive model # τ만큼의 고정된 길이의 시퀸스만을 사용하는 경우도 있다. 이러한 경우를 AR(τ)(Autoregressive model)이라고 한다.\nτ를 결정하는 것조차도 많은 사전 지식이 필요하다 필요에 따라 짧고 긴 τ를 정해야한다. 잠재자기회귀모델 # Xt를 예측할 때, Xt-1과 Ht를 사용해서 예측한다. Ht(잠재변수)는 Xt-2부터 X1까지의 정보들이다. 가변적인 데이터를 고정적인 데이터로 바꿨다. 모델에서 처리하기 편해진다! 문제점 : Ht를 어떻게 인코딩할 것인가? RNN(Recurrent neural network) # 잠재자기회귀모델에서의 잠재변수 Ht를 신경망을 통해 반복사용하여, 시퀸스 데이터의 패턴을 학습하는 모델.\n네트워클을 수식화하면 아래와 같다.\nXt : 현재 시점의 시퀸스 데이터 Ht : 현재 시점까지의 잠재변수 W(1), W(2) : 시퀸스 데이터의 시점에 관계없이 모든 시퀸스 데이터에 사용되는 가중치 행렬들. 이러한 네트워크는 현재 시점의 시퀸스 데이터만을 다룰 수 있다. 따라서 아래와 같이 네트워크를 확장해본다. Wx(1) : 현재 시점의 시퀸스 데이터로와 결합되는 가중치 행렬 WH(1) : 이전 시점의 잠재변수와 결합되는 가중치 행렬 Ht : 새롭게 계산된 잠재변수. 복제되서 다음 순서의 잠재변수를 인코딩하는데 사용. 전체 네트워크에서 사용되는 고정된 가중치 행렬 : Wx(1), WH(1), W(2) BPTT(Backpropagation through time) # RNN의 역전파 방법 빨간색 : gradient의 전달 경로 파란색 : foward propagation 시퀸스 길이가 길어질수록 빨간색 박스 내의 항은 불안정해지기 쉽다. 해당 박스 내의 값이 0보다 작다면 값이 매우 작아지고, 0보다 크다면 값이 매우 커진다.\nTruncated BPTT # 모든 시퀸스 순서에 대해서 gradient를 모두 계산한다면 위에서 봤듯이 미분항이 매우 불안정해지면서 기울시 소실(gradient vanished)가 발생한다. 따라서 적절히 끊어준다.\n가령 위 그림에서는 정상적으로 BPTT를 하다가, Ht에 대해서는 Ot에서만 graident 정보를 받아서 graident를 업데이트한다.\n하지만 이조차도 한계가 있기 때문에 LSTM과 GRU를 통해 이를 해결한다.\n","date":"6 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-06-rnn/","section":"Posts","summary":"","title":"RNN","type":"posts"},{"content":"","date":"6 August 2021","externalUrl":null,"permalink":"/tags/%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84/","section":"Tags","summary":"","title":"부스트캠프","type":"tags"},{"content":" 1주차 학습정리 # 강의 복습 내용 # ai math (1~11번 포스팅) # https://velog.io/@naem1023/series/ai-math\npython (1~2번 포스팅) # https://velog.io/@naem1023/series/python\n과제 수행 과정 / 결과물 정리 # 선택과제 1이 관건이었다. gradient descent를 직접 구현할 때, 벡터 연산을 통한 구현은 수업에서 다뤄서 할만했다. 하지만 y = mx + c와 같은 일차함수의 gradient descent를 예시에 맞춰서 진행하는 부분에서 의외로 막막했다.\nhttps://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n이전에도 gradient descent를 notion에 정리하면서 익혔던 내용인데, 자주 까먹어서 위 링크의 내용을 참고했다. 앞으로도 자주 참고해야겠다.\n관건은 위 링크에서 서술하듯, loss function을 정의하고 m과 c에 대해서 미분한 것을 gradient vector처럼 사용하면 되는 것이다.\n![](/assets/images/부스트캠프 AI Tech 2기 1주차 학습정리/9b9e6416-22d6-4f53-aa73-f908b373f364-image.png) ![](/assets/images/부스트캠프 AI Tech 2기 1주차 학습정리/ee96d9af-354c-4004-b2c7-8fb64d8f879a-image.png) loss function을 m과 c에 대해 각각 미분하고, m과 c는 아래와 같이 익숙한 수식 형태를 통해 업데이트한다. ![](/assets/images/부스트캠프 AI Tech 2기 1주차 학습정리/d1271fde-207e-4577-9dc4-e5503f64c58e-image.png)\n해당 과정을 numpy를 활용해 그대로 코드화하여 해결했다.\n피어세션 정리 # 피어세션에서 자주 언급되고 상의했던 내용들을 임성빈교수님께서 정말 좋게 정리해주셨다. 모호하거나 정보 자체를 몰라서 피어세션에서 서로 해맸던 내용들이니 피어세션 정리란에 정리하겠다.\nhttps://naem1023.notion.site/4b3c83b157ca43a8b6d1ef706084a1fb\n이는 노션을 통해서 정리해봤다.\n학습 회고 # https://naem1023.notion.site/ML-68740e6ac0db42e9a01b17c9ab093606 그 동안 대학생활 하면서 차근차근 위 링크에 정리했던 내용들을 부캠에서 다시 배웠던 첫 주였다. 그럼에도 모두 새로웠다. 그만큼 기초조차도 탄탄하지 못한 뜻으로 이해했다.\nvelog에 공부 내용들을 모두 정리하면서 공부했던 것들이 잘 쌓여져가면 좋겠다.\n","date":"6 August 2021","externalUrl":null,"permalink":"/posts/records/2021-08-06-%EB%B6%80%EC%8A%A4%ED%8A%B8%EC%BA%A0%ED%94%84-ai-tech-2%EA%B8%B0-1%EC%A3%BC%EC%B0%A8-%ED%95%99%EC%8A%B5%EC%A0%95%EB%A6%AC/","section":"Posts","summary":"","title":"부스트캠프 AI Tech 2기 1주차 학습정리","type":"posts"},{"content":" CNN # MLP의 fully conneted layer는 가중치 행렬이 매우 크다. 반면 CNN은 kernel이라는 고정된 입력벡터를 사용한다.\n모든 i에 대해 커널 V를 적용한다. 커널의 사이즈만큼 x 상에서 이동하며 적용한다. 활성화 함수를 제외한 convolution 연산도 선형변환이다. 수식 # continous, discreate한 경우에 아래와 같이 수식이 이루어진다.\nconvolution 연산은 신호(signal)을 국소적으로 증폭/감소시켜 정보를 추출/필터링하는 것.\nCross-correlation # convolution 연산을 +로 엮은 것이다. 실제로 CNN을 구현할 때 cross-correlation이 사용된다. 전통적으로 cross-correlation을 convoltion으로 불렀지만, 실제로는 다른 연산이다.\nconvolution 연산 # translation invariant : 커널은 정의역 내에서 움직여도, 커널 자체가 변하지는 않는다. 또한 커널은 신호에 국소적으로만 적용된다. undefined undefined\n이미지에서의 convolution 예시 # 체험 링크 : https://setosa.io/ev/image-kernels/\n다차원에서의 convolution 수식 # convolution 적용 # f가 커널, g가 입력이다. 입력에 대한 좌표가 (i, j) 예시에서 p, q의 범위는 각각 01, 01이다. 즉, p, q의 범위는 커널 내의 요소와 입력 행렬의 요소를 한쌍으로 지정해주는 역할을 한다. 각각을 element-wise하게 곱해주고 sum한다. 이를 입력의 범위를 벗어나지 않는 선에서 반복한다. convolution 크기 예상 # 입력 크기 = (H, W) 커널 크기 = (KH, KW) 출력 크기 = (OH, OW) 2차원 convolution # 3차원부터는 행렬이라고 하지 않고 Tensor라고 한다.\n2차원 입력이 3채널로 들어올 경우 위와 같이 convolution 연산을 한다. 각각의 채널마다 커널을 생성하고, 해당 채널의 커널과 2차원 입력에 대해서 convolution 연산을 한다. 그리고 이 결과들을 모두 합한다.\n이를 그림으로 설명하면 아래와 같다.\n3차원 커널과 3차원 입력이 준비돼있다. 물론 2차원 입력에 대한 채널을 상정했기 때문에 3차원이 된 것이다.\n이 때, 3차원과 3차원의 convolution 연산을 하면 1개 채널의 2차원 출력물이 발생한다. 모든 채널에 대해서 커널을 모두 준비했기 때문이다.\n2차원 출력의 채널을 1개가 아닌 여러개로 만들고 싶다면, 3차원 커널 텐서를 여러개 만들어서 적용하면된다!\nCNN의 back propagation # 역전파를 계산할 때도, 똑같이 convolution 연산이 나온다. 말이 어려운데 수식으로 설명하면 아래와 같다. f : 커널 g : 시그널(입력) 하고자하는 것 : f와 g의 convolution 연산에 대한 미분 x에 대해 미분하고자하면 x 항은 g만 가지고 있기 때문에, 미분은 g에만 붙는다. 즉, 수식의 두번째 줄처럼 f와 g의 도함수에 대한 convolution 연산으로 변하는 것이다!!\n이는 discrete에서도 똑같이 적용된다.\n예시 # 입력과 커널이 벡터인 상태에서 convolution 연산을 시행한다고 해보자. 결과들은 출력 벡터에 저장된다.\nloss function에서 error값이 연산되고, 이에 대한 미분값이 역전파 단계를 통해 출력벡터까지 온 상황을 가정해보자.\n헷갈리수도 있는데, 위위 그림에서 보면 X3와 W3이 곱해져서 O1에 전달됐다. 같은 원리로 X3와 W2가 곱해져서 O2, X3와 W1이 곱해져서 O3로 전달됐다.\n이와 동일한 방식으로 미분값들도 커널의 W3, W2, W1과 곱해져서 X3에 전달된다.\n커널도 동일한 방식으로 업데이트가 된다고 한다. 사실 이부분은 잘 이해가 되지 않는다\u0026hellip;\n결국 모든 과정들을 종합해보면 back propagation조차도 convolution 연산과 동일하게 진행이 된다!\n","date":"6 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-06-cnn/","section":"Posts","summary":"","title":"CNN","type":"posts"},{"content":"","date":"5 August 2021","externalUrl":null,"permalink":"/categories/ml-math/","section":"Categories","summary":"","title":"ML-Math","type":"categories"},{"content":"이것도 고등학교 때 배운 내용들이 많은데 까먹은 것도 많다\u0026hellip;\n![](/assets/images/베이즈 통계학/2e826435-b849-4675-9750-62901080ee56-image.png)\n위 조건부확률은 사건 B가 일어났을 때, 사건 A가 발생할 확률을 의미.\n베이즈 정리 # ![](/assets/images/베이즈 통계학/48b3e05e-ca66-454f-b912-df5f88f6ca68-image.png)\n위 수식은 A라는 새로운 정보가 주어졌을 때, P(B)로부터 조건부확률을 계산하는 방법을 제공한다.\n베이즈 정리 예제 # ![](/assets/images/베이즈 통계학/159fb872-5842-4795-bbd3-d324de7a3a5d-image.png)\nD : 새로 관찰하는 데이터 Θ : hypothesis, 모델링하는 이벤트, 모델에서 계산하고자 하는 parameter 사후확률(posterior distribution) : D가 관찰됐을 때, Θ가 성립할 확률, 데이터를 관찰한 후이기 때문에 사후라고 한다. 사전확률(prior distribution) : D가 관찰되기 이전에, 사전에 관찰되는 Θ의 확률. 미리 가정된 모수, 확률 분포. 베이즈 정리의 분자 : likelihood 베이즈 정리의 분모 : Evidence, 데이터 자체의 분포 베이즈 정리 예제(COVID-99) # COVID-99의 발병률이 10%이다. COVID-99에 실제로 걸렸을 때, 검진될 확률은 99%이다. COVID-99에 실제로 걸리지 않았을 때, 오검진될 확률이 1%이다. 이 때, 어떤 사람이 COVID-99에 걸렸다고 검진결과가 나왔을 때, 실제로 COVID-99에 감염되었을 확률은?\n![](/assets/images/베이즈 통계학/a8988dbd-8938-4d45-9e56-dbb42bf5ec43-image.png)\nΘ를 COVID-99 발병 사건으로 정의(관찰 불가) D를 테스트 결과라고 정의.\nΘ와 ¬Θ에 대한 사건확률을 위와 같이 정의할 수 있다.\n![](/assets/images/베이즈 통계학/79dc6738-c032-4b09-8704-5a4cfcccdee5-image.png)\n베이즈 정리를 활용해서 Evidence를 구하기 위해 위와 같이 식을 세워볼 수 있다. Likelihood에 Θ의 확률을 곱해주어 더하자.\n조건부확률의 시각화 # ![](/assets/images/베이즈 통계학/aea0848c-2f0e-4ad4-8cab-0a38ff933aed-image.png)\nTrue Positive : _Recall._양성이라고 판별됐을 때, 실제로 양성일 확률. True Negative : 음성이라고 판별됐을 때, 실제로 음성일 확률 False Positive : False alarm(1종 오류). 양성이라고 판별됐을 때, 양성이 아닌 확률. False Negative : _(2종 오류)._음성이라고 판별됐을 때, 음성이 아닐 확률.\n사전 확률 P(Θ)에 따라서 Recall이 결정된다. 사전 확률 없이는 베이즈 통계를 활용할 수 없다. 사전 확률을 모르는 경우, 임의로 설정할 수 있지만 신뢰도가 매우 떨어진다. ![](/assets/images/베이즈 통계학/fc6f15c1-93de-43a9-aa73-6cbbedce00bc-image.png)\nPrecisoin은 위와 같이 계산한다.\n조건부확률의 활용 # 가령, 암환자 탐지에 대한 문제라고 하자. 이럴 때는, 2종 오류를 줄이는 것이 매우 중요하다. 암환자가 아니라고 판별했지만 실제로는 암화자인 경우가 2종 오류이기 때문이다.\n따라서 1종 오류와 2종 오류 사이의 균형을 맞출 때, 2종 오류에 더욱 민감하게 신경을 써야 한다.\n베이즈 정리를 통한 정보의 갱신 # ![](/assets/images/베이즈 통계학/c38ae7c8-20b5-4be3-b86f-172fc8bb4cb1-image.png)\n이전 step의 사후확률을 다음 step의 사전확률로써 사용 가능하다.\n용례 # ![](/assets/images/베이즈 통계학/518f641f-896b-4bb3-a053-d983990542dd-image.png)\nCOVID-99 검사할 때 첫번째 검진 시, 제대로 탐지할 확률은 52.4%였다. 동일한 사람에게 연속해서 한번 더 검사를 할 경우 91.7%로 검진 확률이 올라간다.\n이전 step에서 산출한 사후확률인 52.4%를 다음 step의 사전확률로써 사용한 용례이다.\n인과관계(casuality)에 대한 해석 # 조건부확률을 통해서만 인과관계를 전부 설명하는데 함부로 사용하면 안된다!!\n또한 데이터가 아무리 많아진다고 하더라도 조건부 확률을 통해서만 인과관계를 설명할 수 없다.\n설명할 수 있는 경우도 있겠지만, 항상 그렇다는 보장은 결코 없다. 매우 많은 데이터 분석을 통해서만 인과관계가 드러난다.\n인과관계를 활용한 강건한 모델 # 보통 모델을 구성하면 다음과 같은 결과를 나타낸다.\n조건부확률 기반 예측모형(99% 예측정확도) 기존 시나리오(95% 예측정확도) 변화된 시나리오(72% 예측정확도) 인과관계 기반 예측모형(85% 예측정확도) 기존 시나리오(83% 예측정확도) 변화된 시나리오(82% 예측정확도) 조건부 확률만을 사용한 모델들은 보통 예정된 시나리오에서는 높은 예측 정확도를 보장한다. 하지만 데이터 분포가 크게 변하면 예측정확도는 매우 낮아진다.\n인과관계만을 고려한 모델은 높은 예측 정확도를 보장하진 않는다. 하지만 변화에 강건한다.\n인과관계 # 데이터 분포의 변화에 강건한 예측모형을 만들 때 사용. ![](/assets/images/베이즈 통계학/17f2b608-6ff6-42c1-9a86-43ce1baa8081-image.png)\n인과관계를 알기 위해서는 T와 R에 모두 영향을 주는 중첩 요인(confounding factor)인 Z를 반드시 제거해야 한다. 만일 Z를 제거하지 않으면 가짜 연관성(spurious correlation)이 나온다.\n인과관계 추론의 예제 # ![](/assets/images/베이즈 통계학/421972e1-499f-44d3-964c-c795a80a8920-image.png)\n가령, 치료법 a, b에 대한 신장 결석 치료 결과를 분석해보자. 개별적인 치료법의 완치율은 a가 높지만, overall 완치율은 b가 높다. 이것이 _Simson\u0026rsquo;s paradox_이다.\n이는 조건부 확률만으로 해결할 수 없다. 즉, 신장 결석 크기가 유발하는 중첩 요인을 제거해야만 실제 완치율을 분석하는 것이 가능하다.\nZ의 개입 제거 # do(T=a)라는 조정(intervention)효과를 통해 z의 개입을 제거한다. ![](/assets/images/베이즈 통계학/faddb899-2c71-406b-aeee-766923a02ab7-image.png) ![](/assets/images/베이즈 통계학/7657d656-b448-49cc-8553-57f4d38a7fdf-image.png)\n","date":"5 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-05-%EB%B2%A0%EC%9D%B4%EC%A6%88-%ED%86%B5%EA%B3%84%ED%95%99/","section":"Posts","summary":"","title":"베이즈 통계학","type":"posts"},{"content":" 통계론 # 모수 # 통계적 모델링 = 적절한 가정으로 확률 분포를 추정하는 것 기계학습과 통계학이 공통적으로 추구하는 목표.\n유한한 데이터만 관찰해서 모집단의 정확한 분포 예측은 불가능하다. =\u0026gt; 근사적으로 확률분포 추정\n모수적(parametric) 방법론 # 데이터가 특정 확률 분포를 따른다고 선험적(a priori)으로 가정 그 분포를 결정하는 모수(paramter)를 추정 비모수(nonparametric) 방법론 # 확률 분포를 미리 가정하지 않는다. 데이터에 따라 모델의 구조 및 모수의 개수를 변형 보통 모수가 무한히 많거나 계속 변형되어야 할 때 사용.\n비모수 방법론이 모수를 쓰지 않는 것은 용어에 대한 오해이다.\n확률 분포를 가정하는 방법 # 아래의 가정표를 참조하지만, 기계적으로 정하는 것이 아니라 데이터를 생성하는 원리를 고려해서 확률 분포를 정해야 한다.\n데이터가 2개의 값(0, 1)만 가지는 경우 =\u0026gt; 베르누이분포 데이터가 n개의 이산적인 값만 가지는 경우 =\u0026gt; 카테고리분포 데이터가 [0, 1]사이에서 값을 가지는 경우 =\u0026gt; 베타분포 데이터가 0 이상의 값을 가지는 경우 =\u0026gt; 감마분포, 로그정규분포 등 데이터가 ℝ전체에서 값을 가지는 경우 =\u0026gt; 정규분포, 라플라스분포 등 모수 추정 # 확률 분포를 가정했다면 모수를 추정할 수 있다.\n가령, 정규분포의 모수는 평균과 분산이다. 이를 추정하는 통계량(statistic)은 다음과 같다.\n이상적이라면 표본평균은 본래 데이터인 모집단의 평균과 일치한다.\n분산을 구할 때 N-1로 나누는 이유를 대학교 때 배웠는데.. 기억에서 삭제됐다. 나중에 찾아봐야겠다.\n부캠 강의에서는 불편(unbiased) 추정량을 구하기 위함이라고만 설명하고 넘어갔다.\n표집분포(Sampling distribution) # 표집분포 = 통계량(표본평균, 표본분산)의 확률분포 표본분포 = 모집단의 분포\n표집분포(sampling distribution)과 표본분포(sample distribution)은 다르다.\n중심극한정리(central limit theorem) # 표본평균의 표집분포는 N(데이터의 수)가 커질수록 정규분포를 따른다.\n모집단의 분포가 정규분포를 따르지 않아도 성립한다. 위 그림의 모집단은 베르누이분포(이항분포)이다. 즉, 모집단을 아무리 모아봤자 정규분포를 따르지 않는다.\n하지만 모집단의 통계량에 대한 확률분포는 N이 커질수록 분산이 0에 가까워지면서 정규분포를 따르는 것을 볼 수 있다.\n최대가능도 추정법(MLE) # MLE(Maximum likelihood estimation)\n이론적으로 가장 가능성이 높은 모수를 측정하는 방법.\n가능도함수 # 가능도함수(Likelihood function) = L(Θ;x) 확률질량함수, 확률밀도함수랑 같은 의미이지만 다른 관점을 가진 것이다.\n확률밀도함수 = 모수 Θ가 주어져을 때, x에 대한 함수 가능도함수 = 변수 x가 주어졌을 때, 모수 Θ에 대한 함수\n즉, 가능도함수는 변수가 미리 주어졌을 때, 모수 Θ에 대해서 변화하는 함수이다.\n모수 Θ를 따르는 분포가 데이터 x를 관찰할 가능성을 뜻한다.\n전체 범위에 대한 적분을 하거나 급수를 구했을 때 1이되는 확률이 아니다. 단지, 대소 비교가 가능한 관찰 가능한 가능성일 뿐이다.\n로그가능도(Log likelihood) # 데이터 집합 X가 독립적으로 추출되었을 경우 아래와 같이 Likelihood 함수를 정의할 수 있다. 이 때, product로 정의되는 likelihood 함수에 로그를 씌워져서 가능성들의 합으로 정의하여 사용하기도 한다. 이것이 로그가능도이고 보통 이것을 최적화한다.\n로그가능도을 사용하는 이유 # 연산의 가능성 데이터가 매우 많을 때, 가능도를 곱셈으로만 정의하면 컴퓨터가 정확도를 보장할 수 없는 경우도 발생한다.\n하지만 로그가능도를 통해 덧셈으로만 가능도를 정의하면 컴퓨터로 연산이 가능하며 정확도를 보장할 수 있다.\ngradient descent에서 미분 연산의 알고리즘 효율성 곱셈으로만 가능도를 정의하면 연산량은 O(n²)이다. 덧셈으로만 가능도를 정의한 로그가능도의 연산량은 O(n)이다. gradient descent를 사용할 때는, 음의 로그가능도(negative log likelihood)를 사용한다.\n정규분포의 최대가능도 측정법 예제 # 정규분포를 따르는 확률변수 X로부터 독립적인 표본 {x1, \u0026hellip; , xn}을 얻었다고 가정해보자.\n목표 : Likelihood 함수를 최적화하는 Θ를 찾는 것\n정규분포를 따르는 데이터이기 때문에 Θ=(평균(뮤), 분산(시그마 제곱))으로 생각해보자. 특수기호 따오는게 귀찮아서 한글로 대체.\nlikelihood 함수에 로그를 씌워주면 곱셈으로 엮인 정규분포 식이 덧셈으로 분해된다.\n이를 평균과 분산에 대해 각각 미분하면 아래와 같다.\n두 미분식이 모두 0이 되는 뮤와 시그마를 찾으면 likelihood를 최대화하게된다. 두 미분식이 모두 0이 되게하는 MLE 식들은 아래와 같다.\nMLE에서는 불편추정량(unbiased)를 보장하지 않기에 그냥 n으로 나눈다.\n카테고리 분포의 최대가능도 추정법 예제 # 카테고리 분포 Multinoulli(x; p1, .. pd)를 따르는 확률변수 X로부터 독립적인 표본 {x1, \u0026hellip; ,xn}을 얻었다고 하자.\n이는 마치 one hot encoding처럼 xn은 d차원 벡터인데, 한개의 값만 1이고 나머지는 0으로 되어있다.\n이 때, 카테고리 분포의 모수 (p1, \u0026hellip;, pd)를 추정하는 방법을 알아보자.\n카테고리 분포의 모수 # 정규분포의 모수들은 평균과 분산과 같이 통계량이다.\n카테고리 분포의 모수는 확률을 나타낸다. (p1, \u0026hellip; , pd)는 d차원에서 각각의 차원이 0 또는 1이 되는 확률을 가진다. 따라서 p1부터 pd까지 모두 더해주면 1이다.\n정의 # i번째 x의 k번째 차원에 해당하는 값을 을 k번째 모수인 p에 승수로써 계산하라는 표현이다. 이를 카테고리 분포의 MLE에서 아래와 같이 활용한다.\n앞서 카테고리 분포의 모수를 정의할 때 언급했듯이 모든 모수 pk를 더하면 1이다.\npk의 승수에 있던 내용들이 로그에 의해 앞으로 나온다. 이것을 간략하게 nk라고 표현했다.\nnk는 당연하게도 주어진 주어진 데이터 xi에 대해서 k번째 차원의 값이 1인 데이터의 개수를 세는 것이다.\n제약식이 있으니 제약식을 활용하여 라그랑주 승수법을 통해 목적식을 최적화한다.\n이를 pk와 람다에 대해 미분한다. 미분된 두 수식 모두 0이 되어야 한다. 즉, 두 수식은 pk에 대한 식으로 한꺼번에 정리할 수 있다.\n딥러니에서의 최대가능도 추정법 # NN에서 가중치 Θ를 다음과 같이 정의해보자. 이 때, softmax 벡터는 카테고리분포의 모수 (p1, \u0026hellip; , pk)를 모델링한다. 이전 포스팅에서 NN의 출력단에서 softmax를 사용해 조건부확률을 구현한다고 했는데 그 확률을 마치 카테고리분포의 모수로 사용하는 것이다.\n원핫벡터로 표현한 정답레이블 y=(y1, \u0026hellip; , yk)를 관찰데이터로 이용해 확률분포인 softmax 벡터의 로그가능도를 최적화할 수 있다.\n즉, 아래의 로그가능도를 최적화하는 방향으로 Θ를 학습할 수 있다.\n확률분포의 거리 # 기계학습에서 사용되는 손실함수들은 모델이 학습하는 확률분포와 데이터에서 관찰되는 확률분포의 거리를 통해 유도합니다.\n이 때 사용되는 함수들은 다음과 같다.\n총변동 거리(Total variation distance, TV) 쿨백-라이블러 발산(Kullback-Leibler divergence, KL) 바슈타인 거리(Wasserstein Distance) 쿨백-라이블러 발산 # 쿨백 바이블러는 다음과 같이 분해 가능. 분류 문제에서\nP : 정답 레이블 Q : 모델 예측 라고 해보자. 분류 문제에서의 MLE는 쿨백-라이블러 발산을 최소화하는 것과 동일하다.\n","date":"5 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-05-%ED%86%B5%EA%B3%84%EB%A1%A0/","section":"Posts","summary":"","title":"통계론","type":"posts"},{"content":"작년에 lieklihood 관련해서 정리하면서도 이해가 잘 되지 않았던 내용들이다. 부캠 내용들 위주로 다시 재정리했다.\n확률론 # 딥러닝은 확률론 기반의 기계학습 이론이 바탕이다.\n확률분포 # 데이터 공간 (X x y)에서 확률분포 D는 데이터 공간에서 데이터를 추출하는 분포.\n이 때, y가 상정됐기 때문에 정답 레이블이 있는 지도 학습을 기준으로 설명한다.\n확률 변수 # 확률 변수 = 데이터 공간 상의 관찰 가능한 데이터.\n데이터를 추출할 때 확률 변수를 사용. 확률분포는 확률변수를 추출한 분포를 의미. 확률 변수의 종류 # 확률변수는 확률분포 D에 따라 discrete와 continuous로 구분된다.\n데이터 공간에 따라 구분되는 것이 아니다. 가령, 정수 공간의 확률 변수는 필연적으로 이산형이다. 하지만 실수 공간의 확류 변수라도 -0.5와 0.5만 선택 가능하다면 이산형이다.\n이산확률변수(discrete) # 확률 변수가 가질 수 있는 경우의 수를 모두 고려한 확률의 합으로 모델링. 확률질량함수라고 부른다.\n연속확률변수(continuous) # 데이터 공간에 정의된 확률변수의 밀도를 적분하여 모델링.\n이 때, 밀도는 다음과 같다.\n밀도는 누적확률분포의 변화율로 확률이 아니다!\n결합분포(Joint distribution) # 전체 데이터 X, y가 주어진 상황에서 분포를 상정할 수 있는데 이러한 분포를 결합분포(Joint distribution)라고 한다. 결합분포는 확률분포 D를 모델링한다.\n위 그림에서 실제 데이터들은 파란색 점이다. 연속확률변수처럼 보이지만, 결합분포를 빨간색 박스처럼 상정하면 마치 이산확률변수처럼 다룰 수 있다.\n이 때, 실제 데이터의 분포의 종류와 결합 분포의 종류는 무관하다. 모델링하기 나름이다.\n왜냐하면 컴퓨터로 데이터를 다루기 때문에, 원래 확률분포 D를 근사하기 위해 결합분포 P(X, y)는 적절하게만 설정하면 무방하다.\n주변확률분포(Marginal probability distribution) # P(x) = 입력 x에 대한 주변확률분포, y에 대한 정보는 없다. 위 그림처럼 x에 대한 수를 셀 수도 있고 적분을 한 정보를 줄 수도 있다.\n반대로 y에 대한 주변확률 분포도 상정 가능하다. 즉, y에 대한 수를 세거나 적분을 한 P(y)를 정의하는 것이다.\n조건부확률분포 # P(x|y) = 입력 x와 출력 y 사이의 관계를 모델링 위 그림처럼, 조건부확률분포는 y=1일 때의 x의 정보를 모델링할 수 있다.\n조건부확률과 기계학습 # P(y|x) = 입력변수 x에 대해 정답이 y일 확률\nLogistic Regression에서 선형모델과 softmax의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용한다.\n조건부확률 P(y|x)를 구하는 방법\n분류 문제에서 softmax(WΦ + b)는 데이터 x로부터 추출된 특징패턴 Φ(x)과 가중치 행렬 W를 통해 계산 P(y|x) 대신 P(y|Φ(x))라 해도 무방. 딥러닝\nNN을 통해 데이터로부터 특징패턴 Φ를 추출. 기대값 # 확률 분포가 주어진 데이터를 분석할 때, 여러 통계적 범함수(statistical functional)을 계산할 수 있다.\n이 때, 기대값(expection)은 데이터를 대표하는 통계량이다. 평균(mean)이다. 또한 확률분포를 통해 다른 통계적 범함수를 계산하는데 사용된다.\n연속활률분포에서는 적분으로, 이산확률분포에서는 급수로 계산한다.\n용례 # 분산, 첨도, 공분산 등을 계산할 때 사용한다.\n회귀 문제의 조건부기대값 추정 # 조건부기대값은 L2 norm을 최소화하는 함수와 일치한다.\n회귀문제에서 robust(강건)하게 추정하는 경우, 조건부기대값보다는 median을 사용.\n몬테카를로(Monte carlo) 샘플링 # 대부분의 기계학습 문제들은 확률분포를 모르는 상태로 문제풀이를 시작하게 된다.\n즉, 데이터만을 이용하여 기대값을 계산해야되는데 이 때 사용되는 것이 몬테카를로 샘플링이다. 수식 설명\nf에 샘플링한 데이터 x를 대입한다. 샘플링한 데이터들의 산술평균을 계산한다. 2번의 값이 기대값에 근사하게 된다. 몬테카를로는 이산형이든, 연속형이든 상관없이 사용 가능하다.\n몬테카를로 샘플링은 독립추출이 보장되어야 한다.\n대수의 법칙(law of large number)에 의해 수렴성 보장. 몬테카를로 샘플링 예시 # 위 함수를 [-1, 1]에서 적분하는 것은 해석적으로 불가능하다. 이럴 때, 몬테카를로 샘플링을 사용한다.\n함수의 적분식을 마치 몬테카를로 샘플링처럼 구성하기 위해 함수의 적분 수식에 2를 나눠준다. 왜냐하면 적분에서는 성분의 개수라는 것을 상정할 수 없기 때문에, 적분하고자 하는 x의 범위의 길이를 마치 성분의 개수처럼 사용하느 것이다. [-1, 1]에서 균등분포로 N개의 데이터를 추출하여 산술평균을 구한다. def mc_int(fun, low, high, sample_size=100, repeate=10): int_len = np.abs(high - low) stat = [] for _ in range(repeat): x = np.random.uniform(low=low, high=high, size=sample_size) fun_x = fun(x) int_val = int_len * np.mean(fun_x) stat.append(int_val) return np.mean(stat), np.std(stat) def f_x(x): return np.exp(-x**2) print(mc_int(f_x, low=-1, high=1, sample_size=10000, repeat=100)) ","date":"5 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-05-%ED%99%95%EB%A5%A0%EB%A1%A0/","section":"Posts","summary":"","title":"확률론","type":"posts"},{"content":"","date":"4 August 2021","externalUrl":null,"permalink":"/tags/backpropagation/","section":"Tags","summary":"","title":"Backpropagation","type":"tags"},{"content":" Neural network # Linear regression에서의 NN # ![](/assets/images/Neural network/a6465dd5-9e84-4b33-926b-28b292517916-image.png)\n행렬의 역할을 아주 잘 활용한 전형적이 예시가 NN이다.\nX 행렬에서 데이터를 모아둔다. W에서는 X의 데이터를 다른 차원으로 보내주는 역할을 한다.\nb 행렬은 y 절편을 열벡터에 한꺼번에 더해주는 역할을 한다.\n본래 (X, d) 차원이었던 X 행렬은 (n, p) 차원으로 변환된다.\n해석 # ![](/assets/images/Neural network/8f274dcc-7500-48bd-a9fd-c5e17920319e-image.png)\nd차원이었던 X를 p차원으로 연결.\n하나의 화살표는 W 벡터의 하나의 변수를 뜻한다. xd는 p개의 o를 가르키기 때문에 화살표는 d x p개만큼 존재하는데 이는 W 행렬의 차원과 동일하다.\nClassification에서의 NN # Softmax # ![](/assets/images/Neural network/feb29979-6787-45e1-9cab-37e2d7653619-image.png) ![](/assets/images/Neural network/0f1bfd63-8055-498f-88de-15a12d0905ee-image.png)\nclassification에서는 softmax를 벡터와 결합해 확률벡터로 표현한다. 즉, 선형 모델에 softmax를 결합하여 선형 모델의 결과를 원하는 형식대로 해석 가능하다.\nSoftmax 구현 # def softmax(vec): denumerator = np.exp(vec - np.max(vec, axis=-1, keepdims=True)) numerator = np.sum(denumerator, axis=-1, keppdims=True) val = denumerator / numerator return val np.max를 추가해서 overflow를 방지한다. 기존의 softmax 연산결과는 보장된다.\nPrediction # Prediction에서는 softmax를 사용하지 않고 onehot과 같은 메서드만을 사용한다. 이미 NN의 출력으로 확률이 나왔기 때문이라고 생각한다.\ndef one_hot(val, dim): return [np.eye(dim)[_] for _ in val] def one_hot_encoding(vec): vec_dim = vec.shape[1] vec_argmax = np.argmax(vec, axis=-1) return one_hot(vec_argmax, vec_dim) Activation function # 활성화함수는 선형함수의 출력을 비선형으로 바꿔준다. 활성화 함수로 변형된 벡터 = Hidden 벡터, 잠재 벡터, 뉴런 신경망(NN) = 뉴런으로 이루어진 모델 Perceptron = 뉴런으로만 이루어진 전통적인 모델 ![](/assets/images/Neural network/37b7ce96-3cdb-4ecf-8f69-e4f5ebd1906c-image.png) softmax와의 차이점은 softmax는 모든 변수값을 고려하지만 활성화함수는 본래 실수에 대해서만 적용된다고 한다. ?? softmax도 활성화 함수인줄 알았는데 내가 잘못 알고 있었다.\n정의 # 실수에서 정의되는 비선형 함수. 활성화 함수를 쓰지 않은 NN은 선형모델과 전혀 차이가 없다!\n종류 # ![](/assets/images/Neural network/dd670b29-c6d3-44d1-9be2-3325d9c2bc39-image.png)\n전통적으로는 sigmoid와 tanh를 사용. 최근에는 relu와 relu 변형 사용.\nNN(Neural network) # 정의 # 선형모델과 활성함수를 합성한 함수 ![](/assets/images/Neural network/d1932628-aeb5-4062-bc29-9b03ab49e7ff-image.png)\n신경망 내에서의 입력 z를 잠재 벡터 h로 변환하는 과정을 반복하여 신경망의 layer를 쌓는다. 위 그림은 two layer NN이다. 이를 일반화하면 아래와 같다.\n![](/assets/images/Neural network/d98c927d-f6b5-4cd4-a12f-57c1d45aef04-image.png)\n활성화 함수가 적용될 때는 위에서 언급했듯이 하나의 벡터 내의 실수에 개별적으로 적용된다.\nlayer를 2개 이상 쓰는 이유 # universal approximation theorem\n2층 신경망으로도 임의의 연속함수 근사 가능 실현이 어렵다. 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런의 숫자가 급감한다.\n따라서 보통 깊은 층의 NN을 사용한다. 하지만 최적화는 어려워진다.\nforward propagation(순전파) # NN의 layer를 쌓는 과정을 그대로 따라가며 가중치를 조정하는 것.\nback propagation(역전파) # 고전파 ㅋㅋ ..\n선형모델의 parameter update # 선형모델은 어찌보면 1개의 layer라고도 생각할 수 있다. 즉, 모든 parameter들이 한번에 모두 업데이트된다.\nNN의 parameter update # 반면 NN은 여러 개의 layer로 구성된다. 즉, 한꺼번에 모든 parameter들을 업데이트할 수 없다. 순차적으로 진행해야 한다.\n원리 # ![](/assets/images/Neural network/0f6f5ef9-5c0a-4f2b-ace9-e1e420235db0-image.png) 최종 목표 : L개의 모든 층에 사용된 모든 paratmer를 업데이트\n![](/assets/images/Neural network/cb01ea48-d602-4bbd-8082-c3c949585094-image.png)\n미분의 연쇄작용을 사용하여 출력단에서 입력단까지 거꾸로 거스르면서 parameter를 업데이트한다.\nchain-rule기반 자동미분(audo-differentitaion) # 미분의 chain-rule은 고등학교 때 배웠던 내용 그대로이다. ![](/assets/images/Neural network/c0783dd2-a4d4-4540-98f1-8bb9e3dd8eaf-image.png)\n이 때, 컴퓨터가 각 node의 텐서들을 알고 있어야지 chain-rule을 통한 계산이 가능하다.\n반면 forward propagation은 단순히 순차적으로 계산하면 되기 때문에 메모리적으로 back propagation보다 유리하다.\n![](/assets/images/Neural network/f3efca3c-9652-432b-b539-237129bcb9ab-image.png)\n위 그림에서 파란색 화살표가 forward propagation, 빨간색 화살표가 back propagation이다. W1에 대한 gradient vector를 구하기 위해 chain-rule을 사용한 과정을 보여주고 있다.\n![](/assets/images/Neural network/9c2e9174-87e2-44cf-bef6-eafabdbcc9d9-image.png)\n","date":"4 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-04-neural-network/","section":"Posts","summary":"","title":"Neural network","type":"posts"},{"content":"tistory, notion에 산개했있는데 ML 관련된 notion 정리글들만 업로드.\nhttps://naem1023.notion.site/ML-68740e6ac0db42e9a01b17c9ab093606\n","date":"4 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-04-%EC%9D%B4%EC%A0%84%EC%97%90-%EC%A0%95%EB%A6%AC%ED%96%88%EB%8D%98-%EB%82%B4%EC%9A%A9%EB%93%A4/","section":"Posts","summary":"","title":"이전에 정리했던 내용들","type":"posts"},{"content":"이 또한 notion에 여러번 정리했엇던 내용이다. 부캠에서 배운 내용만을 기준으로 재정리해봤다.\n그 동안 정리했던 gradient descent의 내용은 아래 링크에 있다. https://naem1023.notion.site/Gradient-descent-429308fbd0184aaab90c0ac50e90b526 https://naem1023.notion.site/ML-68740e6ac0db42e9a01b17c9ab093606\nGradient descent # 사용 목적 # 이전 포스팅에서는 gradient descent를 사용하여 선형회귀분석을 알고리즘적인 구조만 만들어봤다. 이번에는 수학적으로 어떻게 쓰였는지 알아본다.\n선형회귀분석에서의 사용 # ![](/assets/images/Gradient descent 증명/5a0c0d90-cb90-4b5c-b72c-b52eee765fbc-image.png)\n선형회귀분석의 목적은 실제 데이터 값들인 y와 예측치인 yhat의 L2 norm값을 최소화하는 것이다.\n본래 Moore-Penrose 역행렬을 통해 구할 수 있으나 실제로는 사용이 거의 불가능하다. yhat을 gradient descent를 통해 구해보자.\n선형회귀 목적식(cost function) # 일반적으로 cost function이 뭔지 설명해주고 gradient descent를 풀어가는 블로그 포스팅들이 대부분이라 이부분에 대한 이해가 어려웠다. 가령, 상단에 링크한 노션 정리글에서도 J(Θ0, Θ1)에 대해서 미분을 풀어나가는 것처럼 말이다.\n부캠에서 L2 norm에 대한 접근으로 잘 알려줘서 이해가 쉬웠다.\n![](/assets/images/Gradient descent 증명/445f962f-2928-4899-a956-df4bbded9bfe-image.png)\n결과적으로 이상적인 yhat을 구하는 것이 목적이기 때문에 L2 norm이 아니라, L2 norm의 제곱을 목적식으로 설정해도 무방하다.\n위 식에서 의도해야 하는 것은 목적식을 최소화하는 β를 찾는 것이다. 이에 대한 방법론으로 우리는 graident descent를 설정한 것이다.\n즉, 앞선 포스팅의 알고리즘을 사용하기 위해서 목적식의 gradient vector를 구해야한다.\nβ의 k번째 요소에 대한 목적식의 편미분 # β의 k번째 요소에 대한 편미분식은 다음과 같다. ![](/assets/images/Gradient descent 증명/43b3c8a5-24f2-4cc8-8681-07c0281a401c-image.png)\n이 때, 일반적인 L2 norm과 다르게 평균을 구하는 것이 중요하다.\n수식 중에서 다음과 같은 기호가 있는데 ![](/assets/images/Gradient descent 증명/e4b0b06a-b2c3-46dd-8716-3309d38bc0a9-image.png) 이는 행렬 X의 k번째 열벡터(column)를 전치시킨 것이라는 뜻이다.\n목적식의 gradient vector # β에 대한 gradient vector는 다음과 같다. β의 k번째 요소에 대한 편미분식을 gradient vecotr에 나열한 구조이다. ![](/assets/images/Gradient descent 증명/2124cda7-fbcd-4322-a1be-7a44d8fbb93a-image.png)\n이를 간략화시키면 아래와 같다. ![](/assets/images/Gradient descent 증명/6e3989ef-8dde-47d7-bf17-3df15753854c-image.png) 복잡한 과정을 거쳤지만 결국, Xβ를 β에 대한 미분한 결과인 XT만 곱해진 형식이다.\n목적식을 최소화하는 gradient descent algorithm # t번째 단계에서 t+1번째 coefficient인 β를 업데이트하는 방법은 다음과 같다.\n![](/assets/images/Gradient descent 증명/45118802-286f-4592-a3b2-d639646316ca-image.png)\ngradient descent의 의도대로 목적식을 최소화하는 방향으로 t번째 β에서 gradient vector를 빼주는 형태이다.\n위에서 계산된 gradient vector를 대입하게 되면 부호가 바뀌게 된다. ![](/assets/images/Gradient descent 증명/af3dc779-3c25-4ca4-8c67-c7cf4cec7e9d-image.png)\nL2 norm의 제곱에 대한 목적식 # L2 norm에 대해서만 계산해보니 gradient vector가 매우 더럽게 나왔다. 제곱에 대해서 계산하면 루트가 지워질 것이니 깔끔해질 것이다.\n![](/assets/images/Gradient descent 증명/49a8ccd7-4f21-4c63-82fd-b0eba4d9c60e-image.png)\n최종적인 gradient descent algorithm # # norm: l2 norm을 계산해주는 함수 # lr: 학습률 # T: 학습횟수 for t in range(T): error = y - X @ beta grad = - transpose(X) @ error beta = beta - lr * grad gradient descent의 보장성 # 적절한 학습횟수와 학습률을 사용하면 미분가능한 convex function에 대해서는 항상 수렴이 보장된다.\n선형회귀의 목적식은 β에 대해 convex function이기 때문에 수렴을 보장한다.\n하지만 비선형회귀는 β에 대해 convex function일 것이라는 보장이 없기 때문에 수렴을 보장하지 못한다.\n![](/assets/images/Gradient descent 증명/28bc1210-5e3e-4dc9-8097-6f357e489a6b-image.png)\nSGD(확률적 경사하강법) # SGD(stochastic gradient descent)는 모든 데이터를 사용하지 않고 일부 데이터만 활용해 업데이트를 진행한다.\n만능은 아니지만 cost function이 convex하지 않은 경우가 대부분인 딥러닝의 경우, SGD가 gradient descent보다 실증적으로 더 낫다는 것이 검증됐다.\n교수님은 모든 데이터를 활용한 일반적인 gradient descent와 비교하여 일부분의 데이터만을사용하는 SGD는 확률적으로 본래 gradient descent의 결과와 유사하다는 것이 실증됐다고 하셨다.\n![](/assets/images/Gradient descent 증명/bdea073e-957e-4b80-bcd2-7c72ec129a50-image.png)\n이 때 사용되는 일부분의 데이터를 mini batch라고 부른다. 관례적으로 데이터 한개만 사용하면 SGD, 일부분의 데이터들을 사용하면 mini batch SGD라고 한다.\n효율성 # ![](/assets/images/Gradient descent 증명/21d5bdbd-f88e-4862-b128-344fc711f856-image.png)\n전체 데이터 (X, y)가 아닌 mini batch (Xb, yb)를 사용했기 때문에 연산량은 b/n으로 감소한다.\n또한 데이터를 분할하는 효과가 발생하기 때문에 GPU의 메모리를 효율적으로 사용할 수 있다.\n원리 # 기존 gradient descent는 전체 데이터를 활용하여 nabla theta L을 계산한다. 이는 아래 그림과 같다.\n![](/assets/images/Gradient descent 증명/8f7aa27a-e537-4a45-a3b6-dc0ae58e2a88-image.png)\nSGD는 아래와 같이 mini batch인 D(b)를 사용해 gradient vector를 계산한다.\n![](/assets/images/Gradient descent 증명/338084a7-0b3d-4f13-af7d-c3ef15f67215-image.png)\n이는 본래 gradient descent와 유사한 방향성을 제공해준다.\n![](/assets/images/Gradient descent 증명/d0a7658b-4fb8-487f-a7cf-32f1ac90aded-image.png)\n또한 D(b)가 매 step별로 바뀌기 때문에 목적함수 또한 매 step마다 달라진다. 따라서 극소점에 도달한다고하더라도 목적함수가 달라지기 때문에, 극소점에서 탈출하는 경우가 확률적으로 발생한다.\ngradient descent와 비교 # ![](/assets/images/Gradient descent 증명/c7f7ab44-27b4-44fe-bed7-e9bc68c7d5e8-image.png) convex에서는 gradient descent보다 비효율적일수도 있다. 하지만 일반적인 머신러닝의 목적함수에서는 보다 효율적이다.\n","date":"4 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-04-gradient-descent-%EC%A6%9D%EB%AA%85/","section":"Posts","summary":"","title":"Gradient descent 증명","type":"posts"},{"content":"여러번 notion에 정리했던 내용인데 부캠에서 배운 내용 중심으로 다시 정리해봤다.\n미분 # import sympy as sym from sympy.abc import x sym.diff(sym.poly(x**2 + 2*x + 3), x) Gradient ascent(경사상승법) # f(x) 미분값을 x에 더하며 함수의 극대값의 위치를 구할 때 사용. 즉, 목적함수를 최대화해야 할 때 사용.\nGradient descent(경사하강법) # f(x) 미분값을 x에 빼면서 함수의 극소값의 위치를 구할 때 사용. 즉, 목적ㅎ마수를 최소화해야 할 때 사용.\nAlgorithm # # gradient: 미분을 계산하는 함수 # init: 시작점 # lr: 학습률 # eps: 입실론 var = init grad = graident(var) while (abs(grad) \u0026gt; eps): var = var - lr * grad grad = gradient(var) 미분값이 0이 되는 것이 목표지만, 컴퓨터로 이를 표현할 수 없다. 따라서 매우 작은 실수값인 입실론을 종료 조건으로 설정한다.\nPartial differentiation(편미분) # ML에서 다루는 변수는 보통 벡터이기에, 일반적인 미분이 아니라 편미분을 통해 방향성을 확보한다. 고등학교 때 했던대로 하면 된다.\nundefined\n이 때 ei는 i번째 값만 1이고 나머지는 0인 단위 벡터이다. 즉, 원하는 곳의 정보만 필터링해서 미분하게 해준다.\ngradient vector # nabla # 벡터를 변수로 사용하는 함수라면 편미분을 사용해서 미분을 해야하는데, 이 때 변수들이 매우 많아진다.\n따라서, 모든 변수에 대한 편미분을 시행한 결과를 다시 벡터로 모아서 이를 gradient descent에 사용한다. 이를 gradient 벡터라고 하는데 한꺼번에 모든 변수에 대한 업데이트가 가능하다는 장점이 있다.\nundefined\n해당 기호를 nabla라고 한다.\ngradient vecotr 시각화 # ![](/assets/images/Gradient descent 기본/8e486de8-6882-48bb-89bb-0778f81aa965-image.png) ![](/assets/images/Gradient descent 기본/7c31e669-58b0-43d4-bf3a-85f7b0118c64-image.png)\n등고선으로 표시하면 이해가 쉽다. 등고선을 기준으로 벡터의 방향은 원점으로 가장 빨리 감소하는 방향으로 표시된다.\nAlgorithm using gradient vector # # gradient: gradient vector를 계산하는 함수 # init: 시작점 # lr: 학습률 # eps: 입실론 var = init grad = graident(var) while (norm(grad) \u0026gt; eps): var = var - lr * grad grad = gradient(var) 달라진 점은 gradient의 정의와 종료조건 계산시 abs 대신 norm을 사용하는 것이다.\n","date":"3 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-03-gradient-descent-%EA%B8%B0%EB%B3%B8/","section":"Posts","summary":"","title":"Gradient descent 기본","type":"posts"},{"content":"","date":"3 August 2021","externalUrl":null,"permalink":"/tags/matrix/","section":"Tags","summary":"","title":"Matrix","type":"tags"},{"content":" Matrix # 1학년 공수, 선대 이후로 수학 지식들이 삭제됐다.. 되새기는 겸으로 numpy 기호들과 기록.\nAnnotation # scalar calculation # numpy에서 +, - 가능.\nscalar product # Hadmard Product : 은 모양의 vector끼리 성분곱하는 것 X · Y\nX * Y Norm # ![](/assets/images/Matrix, Vector/46f4a9f2-c27e-4533-b96e-0181628262f0-image.png) 원점에서부터 벡터까지의 거리. L1 norm = 변화량의 절대값의 합 L2 norm = 유클리드 거리\nVector 사이의 각도 # ![](/assets/images/Matrix, Vector/afa1e8c8-b904-4223-8665-5982f81cfca0-image.png) 제2 코사인 법칙을 사용해, 두 벡터 사이의 각도 계산 가능.\ndef angle(x, y): v = np.inner(x, y) / (l2_norm(x) * l2_norm(y)) theta = np.arccos(v) return theta multiplication # XY\nX @ Y 행렬곱을 통해 Matrix를 벡터 공간에서의 operator로 이해할 수도 있다. 행렬곱으로 벡터를 다른 차원의 벡터로 보낼 수 있기 때문. 즉, 패턴 추출, 데이터 압축에 사용 가능.\ninner product # ![](/assets/images/Matrix, Vector/3209ccf9-99e0-4aac-b54c-46c608898d11-image.png)\n![](/assets/images/Matrix, Vector/ff574778-710e-4f6a-8638-29dc845ce06f-image.png)\ninner in numpy # np.inner는 벡터간의 내적이다. 벡터간의 내적을 행렬에서 표현하고자 하면 보통 Transpose를 활용해서 수식으로 표현한다. ![](/assets/images/Matrix, Vector/1fda92b4-f9d3-4339-9e37-c458e06b078d-image.png)\nnp.inner(X, Y) Inverse matrix # np.linalg.inv(X) Pseudo-inverse(유사역행렬), Moore-Penrose 행렬 # 역행렬과 다르게 행과 열의 수가 반드시 고정되지 않는다. 그럼에도 역행렬과 유사한 역할을 한다. ![](/assets/images/Matrix, Vector/b237ed5e-2263-4a08-b3ca-0aed843b9101-image.png) n = 행, m = 열 np.linalg.pinv(X) 연릭방정식 풀이 # ![](/assets/images/Matrix, Vector/5a0334a4-a0f5-4342-878b-184006fac8b9-image.png)\n선형회귀분석 # ![](/assets/images/Matrix, Vector/d0ffd305-292e-4c76-8493-70bc84974d75-image.png)\n데이터의 분포를 고려하면 연립방정식처럼 선형회귀분석을 하는 것은 불가능하다. 따라서, y의 L2 norm을 최소화하는 방향으로 해를 찾는 것이 일반적이다.\n# using sklearn for linear regression from sklearn.linear_model import LinearRegression model = LinearRegression() model.fit(X, y) y_test = model.predict(x_test) # Moore-Penrose inverse matrix X_ = np.array([np.append(x, [1]) for x in X]) # y절편(intercept) 추가 beta = np.linalg.pinv(X_) @ y y_test = np.append(x_test) @ beta sklearn에서 linear regression시에 y 절편을 자동으로 추정해서 계산한다. Moore-Penrose 역행렬을 통해서 선형회귀를 할 때, 직접 y 절편을 추가해서 X를 구성해야 한다.\n","date":"3 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-03-matrix-vector/","section":"Posts","summary":"","title":"Matrix, Vector","type":"posts"},{"content":" Nested function by decorator # Nested function # 함수를 중첩하여 사용가능하다. 일차원적으로 함수를 나열하는 것이 아니라, 복잡한 구조의 함수 결합체를 생성 가능.\nVia decorator # 기존의 방식으로는 매우 복잡하게 서술되는 함수 구조를, decorator를 통해 단순하게 표현 가능.\ndef start(func): def inner_func(*args, **kwargs): print(\u0026#34;*\u0026#34; * 30) func(*args, **kwargs) print(\u0026#34;*\u0026#34; * 30) return inner_func def percent(func): def inner_func(*args, **kwargs): print(\u0026#34;%\u0026#34; * 30) func(*args, **kwargs) print(\u0026#34;%\u0026#34; * 30) return inner_func @start @percent def printer(msg): print(msg) printer(\u0026#39;haha\u0026#39;) ****************************** %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% haha %%%%%%%%%%%%%%%%%%%%%%%%%%%%%% ****************************** ref : https://velog.io/@inyong_pang/Python-Nested-Function-2wk42jt94r\n","date":"3 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-03-python-nested-function/","section":"Posts","summary":"","title":"Python nested function","type":"posts"},{"content":" @property decorator # class의 getter, setter를 쉽게 사용 가능하게 해준다. 일반적인 getter, setter와 동일하게 제한, 하위호환성 등을 고려할 수 있다. python의 class에 property라는 내장함수가 있는데, 이를 decorator 형태로 사용 가능하게 한 것.\n쉽게 getter, setter 사용 # decorator 없이는 private member에 대해서 아래와 같이 코딩할 수 밖에 없다.\nclass Person(): def __init__(self): self.__name = \u0026#39;jack\u0026#39; def set_name(self, name): self.__name = name def get_name(self): return self.__name 이러한 getter, setter를 @property decorator를 통해 간단하게 표현할 수 있다.\nclass Person(): def __init__(self): self.__name = \u0026#39;jack\u0026#39; @property def name(self): return self.__name @name.setter def name(self, name): self.__name = name someone = Person() someone.name = \u0026#39;JACK\u0026#39; print(someone.name) --\u0026gt; JACK 하위호환성 # 추후에 Person이라는 class를 확장한다고 할 때, class member의 제약을 미리 걸어둬야할 경우가 있을 수 있다. 가령, 나이 제한을 둬야하는 경우가 있다고 하자.\nclass Person(): def __init__(self): self.age = 10 def set_age(self, age): if age \u0026lt; 0: print(\u0026#39;error\u0026#39;) return self.age = age def get_age(self): return self.age 하지만 외부에서 age에 바로 접근할 수 있는 경우이기에, age에 대한 제약이 효력이 없는 경우가 존재한다.\n이럴 때, property decorator를 사용한다.\nclass Person(): def __init__(self): self.age = 10 @property def age(self): return self.age @age.setter def age(self, age): if age \u0026lt; 0: print(\u0026#39;error\u0026#39;) return self.age = age 외부에서 어떤식으로 접근해도, age에 대한 제한이 유효하다.\n","date":"3 August 2021","externalUrl":null,"permalink":"/posts/ml/2021-08-03-python-decorator-property/","section":"Posts","summary":"","title":"Python decorator, property","type":"posts"},{"content":"","externalUrl":null,"permalink":"/about/","section":"","summary":"","title":"","type":"page"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]